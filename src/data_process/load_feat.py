
import numpy as np
import pandas as pd
from tqdm import tqdm
import pickle
import warnings
warnings.filterwarnings("ignore")
import argparse
import sys
sys.path.append("/content/drive/My Drive/Msc Project")  # if run in colab
from src.utils.data_utils import get_user_item_time_dict
from src.data_process.load_data import get_all_click_data
from src.utils.data_utils import get_hist_and_last_click

def get_item_feat_df(feat_dir):
    """
    :param feat_dir: String, the folder contain item feature file
    :return: Dataframe, the item txt embedding and img embedding
    """
    feat_path = feat_dir + "item_feat.csv"
    list_item_id = []
    list_txt_vec = []
    list_img_vec = []

    def string2float(str_embedding):
        embedding_split = str_embedding.strip().split(",")
        embedding = []
        for i in embedding_split:
            embedding.append(float(i))
        return embedding
    with open(feat_path, encoding='utf-8') as f:
        for line in tqdm(f):
            line_split = line.strip().split(',[')
            list_item_id.append(line_split[0])
            list_txt_vec.append(line_split[1].strip(']'))
            list_img_vec.append(line_split[2].strip(']'))
    item_feat_df = pd.DataFrame({"item_id": list_item_id, "txt_vec": list_txt_vec, "img_vec": list_img_vec})
    item_feat_df = item_feat_df[["item_id", "txt_vec", "img_vec"]]
    item_feat_df["txt_vec"] = item_feat_df["txt_vec"].apply(lambda x: string2float(x))
    item_feat_df["img_vec"] = item_feat_df["img_vec"].apply(lambda x: string2float(x))
    return item_feat_df


def process_item_feat(item_feat_df):
    """

    :param item_feat_df: Dataframe, contain item feature embedding generated by get_item_feat_df()
    :return: Dataframe, normalized item feature embedding
    """
    processed_item_feat_df = item_feat_df.copy()
    processed_item_feat_df['item_id'] = processed_item_feat_df['item_id'].apply(lambda x: int(x))
    # norm
    _scaler = lambda x: x / np.linalg.norm(x)
    processed_item_feat_df["txt_vec"] = processed_item_feat_df["txt_vec"].apply(_scaler)
    processed_item_feat_df["img_vec"] = processed_item_feat_df["img_vec"].apply(_scaler)

    item_content_vec_dict = dict(
        zip(processed_item_feat_df['item_id'], np.hstack([np.vstack(processed_item_feat_df["txt_vec"].values),
                                                          np.vstack(processed_item_feat_df["img_vec"].values)])))
    processed_item_feat_df = pd.DataFrame()
    processed_item_feat_df["feat_vec"] = item_content_vec_dict.values()
    processed_item_feat_df['item_id'] = list(item_content_vec_dict.keys())
    processed_item_feat_df = processed_item_feat_df[['item_id', "feat_vec"]]
    return processed_item_feat_df, item_content_vec_dict


def fill_item_feat(processed_item_feat_df, item_content_vec_dict, mode):
    all_click, test_click = get_all_click_data(mode)
    # all items and items have feature vector
    all_click_item = set(all_click["item_id"])
    feat_item = set(processed_item_feat_df["item_id"])
    # missing item
    missed_items = all_click_item - feat_item
    user_item_time_hist_dict = get_user_item_time_dict(all_click)

    # co-occurance
    co_occur_dict = {}
    window = 5

    def cal_occ(sentence):
        for i, word in enumerate(sentence):
            hist_len = len(sentence)
            co_occur_dict.setdefault(word, {})
            for j in range(max(i - window, 0), min(i + window, hist_len)):
                if j == i or word == sentence[j]: continue
                loc_weight = (0.9 ** abs(i - j))
                co_occur_dict[word].setdefault(sentence[j], 0)
                co_occur_dict[word][sentence[j]] += loc_weight

    for u, hist_item_times in user_item_time_hist_dict.items():
        hist_items = [i for i, t in hist_item_times]
        cal_occ(hist_items)

    # fill
    miss_item_content_vec_dict = {}
    for miss_item in missed_items:
        co_occur_item_dict = co_occur_dict[miss_item]
        weighted_vec = np.zeros(256)
        sum_weight = 0.0
        for co_item, weight in co_occur_item_dict.items():
            if co_item in item_content_vec_dict.keys():
                sum_weight += weight
                co_item_vec = item_content_vec_dict[co_item]
                weighted_vec += weight * co_item_vec
        if sum_weight != 0.0: 
            weighted_vec /= sum_weight
            txt_item_feat_np = weighted_vec[0:128] / np.linalg.norm(weighted_vec[0:128])
            img_item_feat_np = weighted_vec[128:] / np.linalg.norm(weighted_vec[128:])
            cnt_vec = np.concatenate([txt_item_feat_np, img_item_feat_np])
        else:
            cnt_vec = np.zeros(256)
        miss_item_content_vec_dict[miss_item] = cnt_vec

    miss_item_feat_df = pd.DataFrame()
    miss_item_feat_df["feat_vec"] = miss_item_content_vec_dict.values()
    miss_item_feat_df['item_id'] = list(miss_item_content_vec_dict.keys())
    miss_item_feat_df = miss_item_feat_df[['item_id', "feat_vec"]]

    return miss_item_feat_df, miss_item_content_vec_dict


def obtain_entire_item_feat_df(mode):
    item_feat_df = get_item_feat_df("Datasets/")
    processed_item_feat_df, item_content_vec_dict = process_item_feat(item_feat_df)
    
    miss_item_feat_df, miss_item_content_vec_dict = fill_item_feat(processed_item_feat_df, item_content_vec_dict, mode)
    print(miss_item_feat_df)
    processed_item_feat_df = processed_item_feat_df.append(miss_item_feat_df)
    processed_item_feat_df = processed_item_feat_df.reset_index(drop=True)

    item_content_vec_dict.update(miss_item_content_vec_dict)

    processed_item_feat_df.to_csv("Datasets/{}/processed_item_feat.csv".format(mode), sep=",")
    pickle.dump(item_content_vec_dict, open('Datasets/{}/item_content_vec_dict.pkl'.format(mode), 'wb'))
    print("Done")
    return processed_item_feat_df, item_content_vec_dict

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", default=None, required=True, type=str, help="online or offline")
    args = parser.parse_args()
    obtain_entire_item_feat_df(args.mode)
