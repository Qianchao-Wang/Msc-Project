{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_script.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNd6qo1sSoiJT8JXZLW1xro"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WWqTdhEKs7LM","executionInfo":{"status":"ok","timestamp":1639062217868,"user_tz":-480,"elapsed":20652,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"d4016a87-5de1-49fa-e145-741cb68f2f84"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/My Drive/Msc Project\")"],"metadata":{"id":"si6FEU-KtAVZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3WIKRZ4FtBql","executionInfo":{"status":"ok","timestamp":1639062219003,"user_tz":-480,"elapsed":1138,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"64bb0d79-dcc4-4b3c-9ef7-cc1ce5f09b01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["data_process.ipynb\t\t\t    outputrecall.npy\t  Recs_2.ipynb\n","Datasets\t\t\t\t    outputtest_loss.npy   scripts\n","evaluation.ipynb\t\t\t    outputtrain_loss.npy  src\n","faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2  rank.ipynb\t\t  srgnn.ipynb\n","output\t\t\t\t\t    README.md\t\t  vars\n","outputmrr.npy\t\t\t\t    Recs_1.ipynb\n"]}]},{"cell_type":"markdown","source":["Complete missing content vector"],"metadata":{"id":"nHNBe_2IdPI0"}},{"cell_type":"code","source":["!python src/data_process/load_feat.py --mode offline"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dEkk3s_1bxWS","executionInfo":{"status":"ok","timestamp":1638973962741,"user_tz":-480,"elapsed":89180,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"6f589eba-c4de-40b6-e9f0-2ba726dff16d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["108916it [00:04, 24618.45it/s]\n","      item_id                                           feat_vec\n","0       98308  (0.13940151844603407, 0.03236964743083462, 0.0...\n","1       65542  (0.10033042220648478, -0.07845207329315236, 0....\n","2       65543  (0.08800574114129903, -0.03308040854758099, 0....\n","3          13  (0.1076357517749375, 0.052943513931748454, 0.0...\n","4          17  (0.09589339143346551, -0.02418601180449125, 0....\n","...       ...                                                ...\n","8798    32741  (0.07806241946227811, -0.049406351787585345, 0...\n","8799    32748  (0.11452009269780347, -0.007705489757962259, 0...\n","8800    32750  (0.06562115446830015, -0.0019393055324254155, ...\n","8801    32756  (0.10608679904825634, -0.05510091789700992, 0....\n","8802    98300  (0.1182778553842628, -0.005801356231832416, 0....\n","\n","[8803 rows x 2 columns]\n","Done\n"]}]},{"cell_type":"code","source":["!python src/data_process/load_feat.py --mode online"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2OlJiSGHdHcO","executionInfo":{"status":"ok","timestamp":1638974050479,"user_tz":-480,"elapsed":87741,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"67c240b3-0e38-4426-88b0-70f6fb9e2a48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["108916it [00:01, 87472.02it/s]\n","      item_id                                           feat_vec\n","0       98308  (0.0910340637847681, -0.04240188520220434, 0.0...\n","1       65542  (0.11875883310394217, -0.0829754709736648, 0.0...\n","2       65543  (0.08800574114129903, -0.03308040854758099, 0....\n","3          13  (0.1076357517749375, 0.052943513931748454, 0.0...\n","4          17  (0.09589339143346551, -0.02418601180449125, 0....\n","...       ...                                                ...\n","8799    32741  (0.07983506509992742, -0.04914929670045257, 0....\n","8800    32748  (0.11719096602510044, -0.00484361478323152, 0....\n","8801    32750  (0.06232130356647397, 0.02034204624784919, 0.0...\n","8802    32756  (0.11502075912804352, -0.05420307781421694, 0....\n","8803    98300  (0.10895816812298921, -0.022282684971763594, 0...\n","\n","[8804 rows x 2 columns]\n","Done\n"]}]},{"cell_type":"markdown","source":["# Collaborative filtering algorithm"],"metadata":{"id":"3B_1vuZ9wYfM"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"CdRrJwfUwfXT"}},{"cell_type":"markdown","source":["1. Use the original method to calculate the similarity of items in the collaborative filtering algorithm"],"metadata":{"id":"HALHRdrYtyam"}},{"cell_type":"code","source":["!python src/recall/similarity/item_sim_baseline.py "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JYXiOfGmtHy-","executionInfo":{"status":"ok","timestamp":1638967591775,"user_tz":-480,"elapsed":104010,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"d4bc20f6-77a0-4e3a-c984-cf4b5d5d3451"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--------load data----------------------\n","---------------calculate sim------------\n","/content/drive/My Drive/Msc Project/src/utils/data_utils.py:23: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n","  user_item_time_df = user_behavior.groupby('user_id')['item_id', 'timestamp'].apply \\\n","100% 35444/35444 [00:42<00:00, 840.46it/s] \n","tcmalloc: large alloc 1181564928 bytes == 0x55d5079d8000 @  0x7f2483a462a4 0x55d3c1cf14cc 0x55d3c1c94ed0 0x55d3c1da682c 0x55d3c1da6738 0x55d3c1da6738 0x55d3c1da4f3c 0x55d3c1c96992 0x55d3c1cf446c 0x55d3c1cf4240 0x55d3c1d680f3 0x55d3c1cf5afa 0x55d3c1d63915 0x55d3c1d629ee 0x55d3c1d626f3 0x55d3c1e2c4c2 0x55d3c1e2c83d 0x55d3c1e2c6e6 0x55d3c1e04163 0x55d3c1e03e0c 0x7f248282fbf7 0x55d3c1e03cea\n"]}]},{"cell_type":"markdown","source":["2. Use the original method to calculate the similarity of users in the collaborative filtering algorithm"],"metadata":{"id":"mggxh1n-u7sO"}},{"cell_type":"code","source":["!python src/recall/similarity/user_sim_baseline.py "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qLKC9J3WuB0D","executionInfo":{"status":"ok","timestamp":1638967632656,"user_tz":-480,"elapsed":40888,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"2ad1c58b-d65e-405b-e8c1-33ba6333f6f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--------load data----------------------\n","---------------calculate sim------------\n","/content/drive/My Drive/Msc Project/src/utils/data_utils.py:41: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n","  item_user_time_df = user_behavior.groupby('item_id')['user_id', 'timestamp'].apply(\n","100% 117720/117720 [00:17<00:00, 6844.87it/s]\n"]}]},{"cell_type":"markdown","source":["3. Use the improved method to calculate the similarity of items in the training data set."],"metadata":{"id":"RaU4WBBwvEdd"}},{"cell_type":"code","source":["!python src/recall/similarity/item_similarity.py --mode offline"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zSnXrvPjuIw_","executionInfo":{"status":"ok","timestamp":1638968610916,"user_tz":-480,"elapsed":978265,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"5a1c08e3-895f-493d-c97f-1e1d840df6f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--------load data----------------------\n","---------------calculate sim------------\n","100% 29444/29444 [10:52<00:00, 45.11it/s]\n","tcmalloc: large alloc 1073741824 bytes == 0x559761a8c000 @  0x7f623587a1e7 0x55964bb7ba08 0x55964bbfdbbf 0x55964bbfa9ce 0x55964bbfc21b 0x55964bbfafda 0x55964bbfb6a1 0x55964bbfb781 0x55964bbf9f3c 0x55964baeb992 0x55964bb4946c 0x55964bb49240 0x55964bbbd0f3 0x55964bb4aafa 0x55964bbb8915 0x55964bbb79ee 0x55964bbb76f3 0x55964bc814c2 0x55964bc8183d 0x55964bc816e6 0x55964bc59163 0x55964bc58e0c 0x7f6234664bf7 0x55964bc58cea\n","tcmalloc: large alloc 2147483648 bytes == 0x5597a1a8c000 @  0x7f623587a1e7 0x55964bb7ba08 0x55964bbfdbbf 0x55964bbfb047 0x55964bbfa8a8 0x55964bbfc21b 0x55964bbfafda 0x55964bbfb781 0x55964bbfb6a1 0x55964bbf9f3c 0x55964baeb992 0x55964bb4946c 0x55964bb49240 0x55964bbbd0f3 0x55964bb4aafa 0x55964bbb8915 0x55964bbb79ee 0x55964bbb76f3 0x55964bc814c2 0x55964bc8183d 0x55964bc816e6 0x55964bc59163 0x55964bc58e0c 0x7f6234664bf7 0x55964bc58cea\n","tcmalloc: large alloc 4294967296 bytes == 0x559821a8c000 @  0x7f623587a1e7 0x55964bb7ba08 0x55964bbfdbbf 0x55964bbfb047 0x55964bbfa8a8 0x55964bbfc21b 0x55964bbfafda 0x55964bbfb738 0x55964bbfb6ee 0x55964bbf9f3c 0x55964baeb992 0x55964bb4946c 0x55964bb49240 0x55964bbbd0f3 0x55964bb4aafa 0x55964bbb8915 0x55964bbb79ee 0x55964bbb76f3 0x55964bc814c2 0x55964bc8183d 0x55964bc816e6 0x55964bc59163 0x55964bc58e0c 0x7f6234664bf7 0x55964bc58cea\n","tcmalloc: large alloc 1771053056 bytes == 0x559798d50000 @  0x7f623587b2a4 0x55964bb464cc 0x55964bbfdfd4 0x55964bbfc26b 0x55964bbfafda 0x55964bbfb781 0x55964bbfb6a1 0x55964bbf9f3c 0x55964baeb992 0x55964bb4946c 0x55964bb49240 0x55964bbbd0f3 0x55964bb4aafa 0x55964bbb8915 0x55964bbb79ee 0x55964bbb76f3 0x55964bc814c2 0x55964bc8183d 0x55964bc816e6 0x55964bc59163 0x55964bc58e0c 0x7f6234664bf7 0x55964bc58cea\n","tcmalloc: large alloc 2656575488 bytes == 0x55992228c000 @  0x7f623587b2a4 0x55964bb464cc 0x55964bbfdfd4 0x55964bbfc26b 0x55964bbfafda 0x55964bbfb6a1 0x55964bbfb6a1 0x55964bbf9f3c 0x55964baeb992 0x55964bb4946c 0x55964bb49240 0x55964bbbd0f3 0x55964bb4aafa 0x55964bbb8915 0x55964bbb79ee 0x55964bbb76f3 0x55964bc814c2 0x55964bc8183d 0x55964bc816e6 0x55964bc59163 0x55964bc58e0c 0x7f6234664bf7 0x55964bc58cea\n","tcmalloc: large alloc 8589934592 bytes == 0x5599c080e000 @  0x7f623587a1e7 0x55964bb7ba08 0x55964bbfdbbf 0x55964bbfc26b 0x55964bbfafda 0x55964bbfb6a1 0x55964bbfb6ee 0x55964bbf9f3c 0x55964baeb992 0x55964bb4946c 0x55964bb49240 0x55964bbbd0f3 0x55964bb4aafa 0x55964bbb8915 0x55964bbb79ee 0x55964bbb76f3 0x55964bc814c2 0x55964bc8183d 0x55964bc816e6 0x55964bc59163 0x55964bc58e0c 0x7f6234664bf7 0x55964bc58cea\n"]}]},{"cell_type":"markdown","source":["4. Use the improved method to calculate the similarity of items in the test data set."],"metadata":{"id":"6eubVNFMv3kP"}},{"cell_type":"code","source":["!python src/recall/similarity/item_similarity.py --mode online"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z735gSwDugWx","executionInfo":{"status":"ok","timestamp":1638969810501,"user_tz":-480,"elapsed":1199591,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"6ee2cd62-cf88-4854-dc17-4190da8029cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--------load data----------------------\n","---------------calculate sim------------\n","100% 35444/35444 [13:52<00:00, 42.55it/s]\n","tcmalloc: large alloc 1073741824 bytes == 0x559ed52d2000 @  0x7f07005aa1e7 0x559daf979a08 0x559daf9fbbbf 0x559daf9f9047 0x559daf9f88a8 0x559daf9fa21b 0x559daf9f8fda 0x559daf9f96a1 0x559daf9f9781 0x559daf9f7f3c 0x559daf8e9992 0x559daf94746c 0x559daf947240 0x559daf9bb0f3 0x559daf948afa 0x559daf9b6915 0x559daf9b59ee 0x559daf9b56f3 0x559dafa7f4c2 0x559dafa7f83d 0x559dafa7f6e6 0x559dafa57163 0x559dafa56e0c 0x7f06ff394bf7 0x559dafa56cea\n","tcmalloc: large alloc 2147483648 bytes == 0x559f152d2000 @  0x7f07005aa1e7 0x559daf979a08 0x559daf9fbbbf 0x559daf9f89ce 0x559daf9fa21b 0x559daf9f8fda 0x559daf9f96ee 0x559daf9f96ee 0x559daf9f7f3c 0x559daf8e9992 0x559daf94746c 0x559daf947240 0x559daf9bb0f3 0x559daf948afa 0x559daf9b6915 0x559daf9b59ee 0x559daf9b56f3 0x559dafa7f4c2 0x559dafa7f83d 0x559dafa7f6e6 0x559dafa57163 0x559dafa56e0c 0x7f06ff394bf7 0x559dafa56cea\n","tcmalloc: large alloc 4294967296 bytes == 0x559fdb862000 @  0x7f07005aa1e7 0x559daf979a08 0x559daf9fbbbf 0x559daf9f89ce 0x559daf9fa21b 0x559daf9f8fda 0x559daf9f96ee 0x559daf9f9781 0x559daf9f7f3c 0x559daf8e9992 0x559daf94746c 0x559daf947240 0x559daf9bb0f3 0x559daf948afa 0x559daf9b6915 0x559daf9b59ee 0x559daf9b56f3 0x559dafa7f4c2 0x559dafa7f83d 0x559dafa7f6e6 0x559dafa57163 0x559dafa56e0c 0x7f06ff394bf7 0x559dafa56cea\n","tcmalloc: large alloc 1770356736 bytes == 0x559ecc61a000 @  0x7f07005ab2a4 0x559daf9444cc 0x559daf8e7ed0 0x559daf9fa235 0x559daf9f8fda 0x559daf9f9738 0x559daf9f96a1 0x559daf9f7f3c 0x559daf8e9992 0x559daf94746c 0x559daf947240 0x559daf9bb0f3 0x559daf948afa 0x559daf9b6915 0x559daf9b59ee 0x559daf9b56f3 0x559dafa7f4c2 0x559dafa7f83d 0x559dafa7f6e6 0x559dafa57163 0x559dafa56e0c 0x7f06ff394bf7 0x559dafa56cea\n","tcmalloc: large alloc 2655526912 bytes == 0x559f35e72000 @  0x7f07005ab2a4 0x559daf9444cc 0x559dafa001a2 0x559daf9f9034 0x559daf9f88a8 0x559daf9fa21b 0x559daf9f8fda 0x559daf9f9781 0x559daf9f96a1 0x559daf9f7f3c 0x559daf8e9992 0x559daf94746c 0x559daf947240 0x559daf9bb0f3 0x559daf948afa 0x559daf9b6915 0x559daf9b59ee 0x559daf9b56f3 0x559dafa7f4c2 0x559dafa7f83d 0x559dafa7f6e6 0x559dafa57163 0x559dafa56e0c 0x7f06ff394bf7 0x559dafa56cea\n","tcmalloc: large alloc 8589934592 bytes == 0x55a0dc08a000 @  0x7f07005aa1e7 0x559daf979a08 0x559daf9fbbbf 0x559daf9fa26b 0x559daf9f8fda 0x559daf9f96ee 0x559daf9f9738 0x559daf9f7f3c 0x559daf8e9992 0x559daf94746c 0x559daf947240 0x559daf9bb0f3 0x559daf948afa 0x559daf9b6915 0x559daf9b59ee 0x559daf9b56f3 0x559dafa7f4c2 0x559dafa7f83d 0x559dafa7f6e6 0x559dafa57163 0x559dafa56e0c 0x7f06ff394bf7 0x559dafa56cea\n","tcmalloc: large alloc 3983294464 bytes == 0x559fd42f4000 @  0x7f07005ab2a4 0x559daf9444cc 0x559dafa002a1 0x559daf9f9034 0x559daf9f88a8 0x559daf9fa21b 0x559daf9f8fda 0x559daf9f96ee 0x559daf9f9781 0x559daf9f7f3c 0x559daf8e9992 0x559daf94746c 0x559daf947240 0x559daf9bb0f3 0x559daf948afa 0x559daf9b6915 0x559daf9b59ee 0x559daf9b56f3 0x559dafa7f4c2 0x559dafa7f83d 0x559dafa7f6e6 0x559dafa57163 0x559dafa56e0c 0x7f06ff394bf7 0x559dafa56cea\n"]}]},{"cell_type":"markdown","source":["5. Use the improved method to calculate the similarity of users in the training data set."],"metadata":{"id":"2u4mp0rfv_h4"}},{"cell_type":"code","source":["!python src/recall/similarity/user_similarity.py --mode offline"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J9Jny6ovuLCK","executionInfo":{"status":"ok","timestamp":1638969885935,"user_tz":-480,"elapsed":75442,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"6e0a82d3-0274-465a-bd1a-735f21ca6495"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--------load data----------------------\n","---------------calculate sim------------\n","100% 117625/117625 [00:27<00:00, 4309.72it/s]\n"]}]},{"cell_type":"markdown","source":["6. Use the improved method to calculate the similarity of users in the testing data set."],"metadata":{"id":"HDHgqKB_wHCa"}},{"cell_type":"code","source":["!python src/recall/similarity/user_similarity.py --mode online"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sf_-5gqQumgh","executionInfo":{"status":"ok","timestamp":1638969950852,"user_tz":-480,"elapsed":64925,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"e7321169-7cf5-46ee-b0d5-6d66f04a9f33"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--------load data----------------------\n","---------------calculate sim------------\n","100% 117720/117720 [00:40<00:00, 2886.70it/s]\n"]}]},{"cell_type":"markdown","source":["7. Baseline generation recommendation results based on item-based collaborative filtering algorithm"],"metadata":{"id":"O32nXJY4-NEi"}},{"cell_type":"code","source":["!python src/recall/main.py --task itemcf_baseline --mode online --data_dir Datasets/ --i2i_sim_dir output/online/similarity/ \\\\\n","--sim_item_topk 30 --sim_user_topk 30 --recall_item_num 200 --metric_recall --save_dir output/online/Candidate/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1aMvXN0c9VGO","executionInfo":{"status":"ok","timestamp":1638970080050,"user_tz":-480,"elapsed":129204,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"7a6c0d48-6116-4206-ef43-3c1af566d93d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 6000/6000 [01:41<00:00, 59.34it/s] \n"," topk:  10  :  hit_num:  232 hit_rate:  0.03867 user_num :  6000\n"," topk:  20  :  hit_num:  302 hit_rate:  0.05033 user_num :  6000\n"," topk:  30  :  hit_num:  355 hit_rate:  0.05917 user_num :  6000\n"," topk:  40  :  hit_num:  398 hit_rate:  0.06633 user_num :  6000\n"," topk:  50  :  hit_num:  434 hit_rate:  0.07233 user_num :  6000\n"," topk:  60  :  hit_num:  451 hit_rate:  0.07517 user_num :  6000\n"," topk:  70  :  hit_num:  477 hit_rate:  0.0795 user_num :  6000\n"," topk:  80  :  hit_num:  510 hit_rate:  0.085 user_num :  6000\n"," topk:  90  :  hit_num:  538 hit_rate:  0.08967 user_num :  6000\n"," topk:  100  :  hit_num:  548 hit_rate:  0.09133 user_num :  6000\n"," topk:  110  :  hit_num:  560 hit_rate:  0.09333 user_num :  6000\n"," topk:  120  :  hit_num:  575 hit_rate:  0.09583 user_num :  6000\n"," topk:  130  :  hit_num:  585 hit_rate:  0.0975 user_num :  6000\n"," topk:  140  :  hit_num:  592 hit_rate:  0.09867 user_num :  6000\n"," topk:  150  :  hit_num:  601 hit_rate:  0.10017 user_num :  6000\n"," topk:  160  :  hit_num:  608 hit_rate:  0.10133 user_num :  6000\n"," topk:  170  :  hit_num:  616 hit_rate:  0.10267 user_num :  6000\n"," topk:  180  :  hit_num:  619 hit_rate:  0.10317 user_num :  6000\n"," topk:  190  :  hit_num:  627 hit_rate:  0.1045 user_num :  6000\n"," topk:  200  :  hit_num:  635 hit_rate:  0.10583 user_num :  6000\n"]}]},{"cell_type":"markdown","source":["8. Baseline generation recommendation results based on user-based collaborative filtering algorithm"],"metadata":{"id":"C8EwLykn-Pnt"}},{"cell_type":"code","source":["!python src/recall/main.py --task usercf_baseline --mode online --data_dir Datasets/ --u2u_sim_dir output/online/similarity/ \\\\\n","--sim_item_topk 30 --sim_user_topk 30 --recall_item_num 200 --metric_recall --save_dir output/online/Candidate/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e0uXh-Bx9VSS","executionInfo":{"status":"ok","timestamp":1638970101421,"user_tz":-480,"elapsed":21374,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"f51f3ab5-68cf-4554-8721-3a874159eabe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 6000/6000 [00:08<00:00, 729.89it/s]\n"," topk:  10  :  hit_num:  28 hit_rate:  0.00467 user_num :  6000\n"," topk:  20  :  hit_num:  43 hit_rate:  0.00717 user_num :  6000\n"," topk:  30  :  hit_num:  58 hit_rate:  0.00967 user_num :  6000\n"," topk:  40  :  hit_num:  68 hit_rate:  0.01133 user_num :  6000\n"," topk:  50  :  hit_num:  84 hit_rate:  0.014 user_num :  6000\n"," topk:  60  :  hit_num:  93 hit_rate:  0.0155 user_num :  6000\n"," topk:  70  :  hit_num:  114 hit_rate:  0.019 user_num :  6000\n"," topk:  80  :  hit_num:  125 hit_rate:  0.02083 user_num :  6000\n"," topk:  90  :  hit_num:  135 hit_rate:  0.0225 user_num :  6000\n"," topk:  100  :  hit_num:  149 hit_rate:  0.02483 user_num :  6000\n"," topk:  110  :  hit_num:  168 hit_rate:  0.028 user_num :  6000\n"," topk:  120  :  hit_num:  180 hit_rate:  0.03 user_num :  6000\n"," topk:  130  :  hit_num:  195 hit_rate:  0.0325 user_num :  6000\n"," topk:  140  :  hit_num:  210 hit_rate:  0.035 user_num :  6000\n"," topk:  150  :  hit_num:  219 hit_rate:  0.0365 user_num :  6000\n"," topk:  160  :  hit_num:  230 hit_rate:  0.03833 user_num :  6000\n"," topk:  170  :  hit_num:  246 hit_rate:  0.041 user_num :  6000\n"," topk:  180  :  hit_num:  256 hit_rate:  0.04267 user_num :  6000\n"," topk:  190  :  hit_num:  265 hit_rate:  0.04417 user_num :  6000\n"," topk:  200  :  hit_num:  278 hit_rate:  0.04633 user_num :  6000\n"]}]},{"cell_type":"markdown","source":["9. Use item-based collaborative filtering to generate recommendations for each training set user"],"metadata":{"id":"LzlLrpyq9Fyd"}},{"cell_type":"code","source":["!python src/recall/main.py --task itemcf --mode offline --data_dir Datasets/ --i2i_sim_dir output/offline/similarity/ \\\\\n","--sim_item_topk 30 --sim_user_topk 30 --recall_item_num 200 --metric_recall --save_dir output/offline/Candidate/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LTn9_PpZwMZv","executionInfo":{"status":"ok","timestamp":1638970847130,"user_tz":-480,"elapsed":745713,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"b0909f97-7171-44dc-e157-ad764d0229ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tcmalloc: large alloc 1073741824 bytes == 0x5593d4682000 @  0x7f7219f792a4 0x5593757d47d2 0x5593756bfae0 0x55937571d46c 0x55937571d240 0x5593757910f3 0x55937571eafa 0x55937578c915 0x55937578b9ee 0x55937578b6f3 0x5593758554c2 0x55937585583d 0x5593758556e6 0x55937582d163 0x55937582ce0c 0x7f7218d62bf7 0x55937582ccea\n","tcmalloc: large alloc 2147483648 bytes == 0x55942e938000 @  0x7f7219f792a4 0x5593757d47d2 0x5593756bfae0 0x55937571d46c 0x55937571d240 0x5593757910f3 0x55937571eafa 0x55937578c915 0x55937578b9ee 0x55937578b6f3 0x5593758554c2 0x55937585583d 0x5593758556e6 0x55937582d163 0x55937582ce0c 0x7f7218d62bf7 0x55937582ccea\n","100% 29444/29444 [10:29<00:00, 46.75it/s]\n"," topk:  10  :  hit_num:  1086 hit_rate:  0.03688 user_num :  29444\n"," topk:  20  :  hit_num:  1519 hit_rate:  0.05159 user_num :  29444\n"," topk:  30  :  hit_num:  1799 hit_rate:  0.0611 user_num :  29444\n"," topk:  40  :  hit_num:  1982 hit_rate:  0.06731 user_num :  29444\n"," topk:  50  :  hit_num:  2150 hit_rate:  0.07302 user_num :  29444\n"," topk:  60  :  hit_num:  2292 hit_rate:  0.07784 user_num :  29444\n"," topk:  70  :  hit_num:  2409 hit_rate:  0.08182 user_num :  29444\n"," topk:  80  :  hit_num:  2532 hit_rate:  0.08599 user_num :  29444\n"," topk:  90  :  hit_num:  2623 hit_rate:  0.08908 user_num :  29444\n"," topk:  100  :  hit_num:  2711 hit_rate:  0.09207 user_num :  29444\n"," topk:  110  :  hit_num:  2775 hit_rate:  0.09425 user_num :  29444\n"," topk:  120  :  hit_num:  2847 hit_rate:  0.09669 user_num :  29444\n"," topk:  130  :  hit_num:  2912 hit_rate:  0.0989 user_num :  29444\n"," topk:  140  :  hit_num:  2967 hit_rate:  0.10077 user_num :  29444\n"," topk:  150  :  hit_num:  3023 hit_rate:  0.10267 user_num :  29444\n"," topk:  160  :  hit_num:  3072 hit_rate:  0.10433 user_num :  29444\n"," topk:  170  :  hit_num:  3118 hit_rate:  0.1059 user_num :  29444\n"," topk:  180  :  hit_num:  3158 hit_rate:  0.10725 user_num :  29444\n"," topk:  190  :  hit_num:  3201 hit_rate:  0.10871 user_num :  29444\n"," topk:  200  :  hit_num:  3241 hit_rate:  0.11007 user_num :  29444\n"]}]},{"cell_type":"markdown","source":["10. Use item-based collaborative filtering to generate recommendations for each test set user"],"metadata":{"id":"pBJDRx4E9I4M"}},{"cell_type":"code","source":["!python src/recall/main.py --task itemcf --mode online --data_dir Datasets/ --i2i_sim_dir output/online/similarity/ \\\\\n","--sim_item_topk 30 --sim_user_topk 30 --recall_item_num 200 --metric_recall --save_dir output/online/Candidate/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FvvhxEXX807f","executionInfo":{"status":"ok","timestamp":1638971072852,"user_tz":-480,"elapsed":225729,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"a6c82648-f424-4bda-c065-83c3ffb2f340"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tcmalloc: large alloc 1073741824 bytes == 0x559625468000 @  0x7fb16a2992a4 0x5595c5bd67d2 0x5595c5ac1ae0 0x5595c5b1f46c 0x5595c5b1f240 0x5595c5b930f3 0x5595c5b20afa 0x5595c5b8e915 0x5595c5b8d9ee 0x5595c5b8d6f3 0x5595c5c574c2 0x5595c5c5783d 0x5595c5c576e6 0x5595c5c2f163 0x5595c5c2ee0c 0x7fb169082bf7 0x5595c5c2ecea\n","tcmalloc: large alloc 2147483648 bytes == 0x55967eed4000 @  0x7fb16a2992a4 0x5595c5bd67d2 0x5595c5ac1ae0 0x5595c5b1f46c 0x5595c5b1f240 0x5595c5b930f3 0x5595c5b20afa 0x5595c5b8e915 0x5595c5b8d9ee 0x5595c5b8d6f3 0x5595c5c574c2 0x5595c5c5783d 0x5595c5c576e6 0x5595c5c2f163 0x5595c5c2ee0c 0x7fb169082bf7 0x5595c5c2ecea\n","100% 6000/6000 [02:31<00:00, 39.62it/s]\n"," topk:  10  :  hit_num:  268 hit_rate:  0.04467 user_num :  6000\n"," topk:  20  :  hit_num:  355 hit_rate:  0.05917 user_num :  6000\n"," topk:  30  :  hit_num:  430 hit_rate:  0.07167 user_num :  6000\n"," topk:  40  :  hit_num:  479 hit_rate:  0.07983 user_num :  6000\n"," topk:  50  :  hit_num:  514 hit_rate:  0.08567 user_num :  6000\n"," topk:  60  :  hit_num:  546 hit_rate:  0.091 user_num :  6000\n"," topk:  70  :  hit_num:  575 hit_rate:  0.09583 user_num :  6000\n"," topk:  80  :  hit_num:  595 hit_rate:  0.09917 user_num :  6000\n"," topk:  90  :  hit_num:  613 hit_rate:  0.10217 user_num :  6000\n"," topk:  100  :  hit_num:  626 hit_rate:  0.10433 user_num :  6000\n"," topk:  110  :  hit_num:  641 hit_rate:  0.10683 user_num :  6000\n"," topk:  120  :  hit_num:  652 hit_rate:  0.10867 user_num :  6000\n"," topk:  130  :  hit_num:  667 hit_rate:  0.11117 user_num :  6000\n"," topk:  140  :  hit_num:  676 hit_rate:  0.11267 user_num :  6000\n"," topk:  150  :  hit_num:  684 hit_rate:  0.114 user_num :  6000\n"," topk:  160  :  hit_num:  694 hit_rate:  0.11567 user_num :  6000\n"," topk:  170  :  hit_num:  698 hit_rate:  0.11633 user_num :  6000\n"," topk:  180  :  hit_num:  706 hit_rate:  0.11767 user_num :  6000\n"," topk:  190  :  hit_num:  714 hit_rate:  0.119 user_num :  6000\n"," topk:  200  :  hit_num:  721 hit_rate:  0.12017 user_num :  6000\n"]}]},{"cell_type":"markdown","source":["11. Use user-based collaborative filtering to generate recommendations for each training set user"],"metadata":{"id":"4UcpZMIY-lvL"}},{"cell_type":"code","source":["!python src/recall/main.py --task usercf --mode offline --data_dir Datasets/ --u2u_sim_dir output/offline/similarity/ \\\\\n","--sim_item_topk 30 --sim_user_topk 30 --recall_item_num 200 --metric_recall --save_dir output/offline/Candidate/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F6mxgARb-lBr","executionInfo":{"status":"ok","timestamp":1638971144620,"user_tz":-480,"elapsed":71782,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"929c8253-37e9-44ad-82ab-52295d9b695b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 29444/29444 [00:24<00:00, 1209.78it/s]\n"," topk:  10  :  hit_num:  140 hit_rate:  0.00475 user_num :  29444\n"," topk:  20  :  hit_num:  219 hit_rate:  0.00744 user_num :  29444\n"," topk:  30  :  hit_num:  306 hit_rate:  0.01039 user_num :  29444\n"," topk:  40  :  hit_num:  404 hit_rate:  0.01372 user_num :  29444\n"," topk:  50  :  hit_num:  484 hit_rate:  0.01644 user_num :  29444\n"," topk:  60  :  hit_num:  548 hit_rate:  0.01861 user_num :  29444\n"," topk:  70  :  hit_num:  613 hit_rate:  0.02082 user_num :  29444\n"," topk:  80  :  hit_num:  690 hit_rate:  0.02343 user_num :  29444\n"," topk:  90  :  hit_num:  767 hit_rate:  0.02605 user_num :  29444\n"," topk:  100  :  hit_num:  848 hit_rate:  0.0288 user_num :  29444\n"," topk:  110  :  hit_num:  919 hit_rate:  0.03121 user_num :  29444\n"," topk:  120  :  hit_num:  979 hit_rate:  0.03325 user_num :  29444\n"," topk:  130  :  hit_num:  1042 hit_rate:  0.03539 user_num :  29444\n"," topk:  140  :  hit_num:  1105 hit_rate:  0.03753 user_num :  29444\n"," topk:  150  :  hit_num:  1175 hit_rate:  0.03991 user_num :  29444\n"," topk:  160  :  hit_num:  1237 hit_rate:  0.04201 user_num :  29444\n"," topk:  170  :  hit_num:  1297 hit_rate:  0.04405 user_num :  29444\n"," topk:  180  :  hit_num:  1371 hit_rate:  0.04656 user_num :  29444\n"," topk:  190  :  hit_num:  1439 hit_rate:  0.04887 user_num :  29444\n"," topk:  200  :  hit_num:  1500 hit_rate:  0.05094 user_num :  29444\n"]}]},{"cell_type":"markdown","source":["12. Use user-based collaborative filtering to generate recommendations for each test set user"],"metadata":{"id":"a7NenZ9Y-sU8"}},{"cell_type":"code","source":["!python src/recall/main.py --task usercf --mode online --data_dir Datasets/ --u2u_sim_dir output/online/similarity/ \\\\\n","--sim_item_topk 30 --sim_user_topk 30 --recall_item_num 200 --metric_recall --save_dir output/online/Candidate/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1EjXXPfz-w4f","executionInfo":{"status":"ok","timestamp":1638971162444,"user_tz":-480,"elapsed":17829,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"1ec6862c-2588-4ce1-badd-f583b6564d70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 6000/6000 [00:05<00:00, 1136.39it/s]\n"," topk:  10  :  hit_num:  28 hit_rate:  0.00467 user_num :  6000\n"," topk:  20  :  hit_num:  45 hit_rate:  0.0075 user_num :  6000\n"," topk:  30  :  hit_num:  64 hit_rate:  0.01067 user_num :  6000\n"," topk:  40  :  hit_num:  77 hit_rate:  0.01283 user_num :  6000\n"," topk:  50  :  hit_num:  93 hit_rate:  0.0155 user_num :  6000\n"," topk:  60  :  hit_num:  102 hit_rate:  0.017 user_num :  6000\n"," topk:  70  :  hit_num:  116 hit_rate:  0.01933 user_num :  6000\n"," topk:  80  :  hit_num:  137 hit_rate:  0.02283 user_num :  6000\n"," topk:  90  :  hit_num:  155 hit_rate:  0.02583 user_num :  6000\n"," topk:  100  :  hit_num:  176 hit_rate:  0.02933 user_num :  6000\n"," topk:  110  :  hit_num:  194 hit_rate:  0.03233 user_num :  6000\n"," topk:  120  :  hit_num:  211 hit_rate:  0.03517 user_num :  6000\n"," topk:  130  :  hit_num:  226 hit_rate:  0.03767 user_num :  6000\n"," topk:  140  :  hit_num:  246 hit_rate:  0.041 user_num :  6000\n"," topk:  150  :  hit_num:  262 hit_rate:  0.04367 user_num :  6000\n"," topk:  160  :  hit_num:  287 hit_rate:  0.04783 user_num :  6000\n"," topk:  170  :  hit_num:  304 hit_rate:  0.05067 user_num :  6000\n"," topk:  180  :  hit_num:  318 hit_rate:  0.053 user_num :  6000\n"," topk:  190  :  hit_num:  334 hit_rate:  0.05567 user_num :  6000\n"," topk:  200  :  hit_num:  356 hit_rate:  0.05933 user_num :  6000\n"]}]},{"cell_type":"markdown","source":["# SR-GNN"],"metadata":{"id":"LnuhgU2IaDkj"}},{"cell_type":"markdown","source":["1. Generate input data for SR-GNN model"],"metadata":{"id":"sA8N9kzZdgxc"}},{"cell_type":"code","source":["!python src/data_process/generate_sr_gnn_input.py --mode offline"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2H693fOcaCK2","executionInfo":{"status":"ok","timestamp":1638974132613,"user_tz":-480,"elapsed":82137,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"d9f853db-e6a0-4e3a-a82d-5dd866608aa1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["117682\n","29444\n","29444\n","29332\n","28344\n","29332\n","29444\n","Done, Output Lines: 906228\n","10\n"]}]},{"cell_type":"code","source":["!python src/data_process/generate_sr_gnn_input.py --mode online"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cXIXhpPcdx3H","executionInfo":{"status":"ok","timestamp":1638974186533,"user_tz":-480,"elapsed":53926,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"e5e448b8-7968-463c-c79e-0e8ab967ec72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["117720\n","35444\n","35444\n","35420\n","35221\n","5976\n","6000\n","Done, Output Lines: 1145704\n","10\n"]}]},{"cell_type":"markdown","source":["2. Train the SR-GNN model"],"metadata":{"id":"MYpsPpxZd0Tj"}},{"cell_type":"code","source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","tf.__version__"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"iiHdaEjbed6x","executionInfo":{"status":"ok","timestamp":1638974223448,"user_tz":-480,"elapsed":3488,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"b22b176a-a13d-41cc-c4ec-6b47356602e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'1.15.2'"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["!python src/recall/sr_gnn/train/main2.py --task train --node_count 117682 --checkpoint_path output/srgnn/offline/srgnn_model.ckpt \\\n","--train_input Datasets/offline/srgnn/train_item_seq_enhanced.txt --test_input Datasets/offline/srgnn/test_item_seq.txt \\\n","--gru_step 2 --epochs 20 --lr 0.001 --lr_dc 2 --dc_rate 0.1 --early_stop_epoch 3 --hidden_size 256 --batch_size 256 \\\n","--max_len 20 --has_uid True --sigma 10 --sq_max_len 5 --batch_logging_step 200"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89j_0VjLdz6E","executionInfo":{"status":"ok","timestamp":1638979500637,"user_tz":-480,"elapsed":212297,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"a6556d76-b490-4d1d-84b9-edfe51cb23ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:10: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n","\n","2021-12-08 14:36:59,049 root:INFO:Data Loaded, Length: 906228ï¼Œ Max Length: 9\n","2021-12-08 14:36:59,368 root:INFO:Data Loaded, Length: 29332ï¼Œ Max Length: 19\n","2021-12-08 14:36:59,368 src.recall.sr_gnn.train.trainer:INFO:Train: {'task': 'train', 'node_count': 117682, 'checkpoint_path': 'output/srgnn/offline/srgnn_model.ckpt', 'l2': None, 'lr': 0.001, 'gru_step': 2, 'batch_size': 256, 'hidden_size': 256, 'epochs': 20, 'batch_logging_step': 200, 'save_step': None, 'max_test_batch': None, 'lr_dc': 7080.0, 'dc_rate': 0.1, 'early_stop_epochs': 3, 'sigma': 10.0, 'max_len': 20, 'has_uid': True, 'feature_init': None, 'node_weight': None, 'node_weight_trainable': None, 'sq_max_len': 5, 'train_input': 'Datasets/offline/srgnn/train_item_seq_enhanced.txt', 'test_input': 'Datasets/offline/srgnn/test_item_seq.txt', 'eval_input': None, 'session_input': None, 'item_lookup': None, 'item_feature': None, 'recommend_output': None, 'embedding_output': None, 'rec_extra_count': None, 'rec_count': None, 'remove_duplicates': None}\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:23: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","2021-12-08 14:36:59,368 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:23: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:24: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","2021-12-08 14:36:59,373 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:24: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:13: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","2021-12-08 14:36:59,389 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:13: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:18: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n","2021-12-08 14:36:59,412 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:18: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","2021-12-08 14:36:59,453 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","2021-12-08 14:36:59,460 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","2021-12-08 14:36:59,470 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:61: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","2021-12-08 14:36:59,541 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:61: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:62: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","2021-12-08 14:36:59,545 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:62: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","2021-12-08 14:36:59,545 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:72: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","2021-12-08 14:36:59,941 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:72: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","2021-12-08 14:36:59,948 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","2021-12-08 14:36:59,948 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","2021-12-08 14:37:00,029 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:79: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","2021-12-08 14:37:00,457 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:79: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:81: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","2021-12-08 14:37:00,458 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:81: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","2021-12-08 14:37:00.458388: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n","2021-12-08 14:37:00.467949: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz\n","2021-12-08 14:37:00.468809: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557136c72a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-12-08 14:37:00.468841: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2021-12-08 14:37:00.472814: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-12-08 14:37:00.651881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 14:37:00.652713: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557136c72d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2021-12-08 14:37:00.652755: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n","2021-12-08 14:37:00.654242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 14:37:00.654847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n","pciBusID: 0000:00:04.0\n","2021-12-08 14:37:00.674795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-12-08 14:37:00.850381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-12-08 14:37:00.958008: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2021-12-08 14:37:00.981796: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2021-12-08 14:37:01.178240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2021-12-08 14:37:01.253362: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2021-12-08 14:37:01.632215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2021-12-08 14:37:01.632405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 14:37:01.633106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 14:37:01.633704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2021-12-08 14:37:01.637539: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-12-08 14:37:01.638925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-12-08 14:37:01.638956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n","2021-12-08 14:37:01.638965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n","2021-12-08 14:37:01.639712: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 14:37:01.640398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 14:37:01.641046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15060 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:82: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n","\n","2021-12-08 14:37:01,642 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:82: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n","\n","2021-12-08 14:37:05,052 model:INFO:The passed save_path is not a valid checkpoint: output/srgnn/offline/srgnn_model.ckpt\n","Epoch:   0% 0/20 [00:00<?, ?it/s]2021-12-08 14:37:05,293 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 3540\n","2021-12-08 14:37:05.526619: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-12-08 14:37:06,372 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 12.17328\n","2021-12-08 14:37:13,986 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 11.82511\n","2021-12-08 14:37:21,659 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 11.70537\n","2021-12-08 14:37:29,285 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 11.58858\n","2021-12-08 14:37:36,978 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 11.46803\n","2021-12-08 14:37:44,672 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 11.36226\n","2021-12-08 14:37:52,356 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 11.26702\n","2021-12-08 14:38:00,094 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 11.17773\n","2021-12-08 14:38:08,119 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 11.09788\n","2021-12-08 14:38:15,913 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 11.02515\n","2021-12-08 14:38:23,654 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 10.95807\n","2021-12-08 14:38:31,365 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 10.89531\n","2021-12-08 14:38:39,051 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 10.83795\n","2021-12-08 14:38:46,737 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 10.78506\n","2021-12-08 14:38:54,423 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 10.73627\n","2021-12-08 14:39:02,016 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 10.69092\n","2021-12-08 14:39:09,604 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 10.64827\n","2021-12-08 14:39:17,221 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 10.60875\n","2021-12-08 14:39:25,584 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9590  @50, Recall: 0.1641  MRR: 0.0378\n","2021-12-08 14:39:28,540 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0661  @50, Recall: 0.1367  MRR: 0.0314\n","2021-12-08 14:39:31,473 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.4590  @50, Recall: 0.0781  MRR: 0.0150\n","2021-12-08 14:39:34,358 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0572  @50, Recall: 0.1641  MRR: 0.0357\n","2021-12-08 14:39:37,217 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1444  @50, Recall: 0.1406  MRR: 0.0275\n","2021-12-08 14:39:40,118 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7779  @50, Recall: 0.1797  MRR: 0.0440\n","2021-12-08 14:39:42,979 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8878  @50, Recall: 0.1602  MRR: 0.0347\n","2021-12-08 14:39:45,972 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0529  @50, Recall: 0.1484  MRR: 0.0283\n","2021-12-08 14:39:48,933 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1444  @50, Recall: 0.1094  MRR: 0.0258\n","2021-12-08 14:39:51,879 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0923  @50, Recall: 0.1250  MRR: 0.0261\n","2021-12-08 14:39:54,899 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9315  @50, Recall: 0.1484  MRR: 0.0320\n","2021-12-08 14:39:57,960 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9137  @50, Recall: 0.1641  MRR: 0.0224\n","2021-12-08 14:40:01,015 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0028  @50, Recall: 0.1211  MRR: 0.0188\n","2021-12-08 14:40:04,033 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.2590  @50, Recall: 0.0977  MRR: 0.0194\n","2021-12-08 14:40:06,999 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1894  @50, Recall: 0.1289  MRR: 0.0261\n","2021-12-08 14:40:09,985 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8233  @50, Recall: 0.1523  MRR: 0.0353\n","2021-12-08 14:40:12,905 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8734  @50, Recall: 0.1562  MRR: 0.0428\n","2021-12-08 14:40:15,867 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9513  @50, Recall: 0.1523  MRR: 0.0242\n","2021-12-08 14:40:18,866 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.3263  @50, Recall: 0.1016  MRR: 0.0136\n","2021-12-08 14:40:21,865 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8351  @50, Recall: 0.1641  MRR: 0.0317\n","2021-12-08 14:40:24,858 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1481  @50, Recall: 0.1250  MRR: 0.0176\n","2021-12-08 14:40:27,781 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5613  @50, Recall: 0.2305  MRR: 0.0462\n","2021-12-08 14:40:30,707 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7746  @50, Recall: 0.1797  MRR: 0.0420\n","2021-12-08 14:40:33,613 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6309  @50, Recall: 0.1953  MRR: 0.0539\n","2021-12-08 14:40:36,560 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1614  @50, Recall: 0.1562  MRR: 0.0352\n","2021-12-08 14:40:39,497 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0492  @50, Recall: 0.1445  MRR: 0.0336\n","2021-12-08 14:40:42,389 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9660  @50, Recall: 0.1211  MRR: 0.0165\n","2021-12-08 14:40:45,335 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0898  @50, Recall: 0.1328  MRR: 0.0308\n","2021-12-08 14:40:48,237 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.2073  @50, Recall: 0.1289  MRR: 0.0261\n","2021-12-08 14:40:51,139 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8172  @50, Recall: 0.1367  MRR: 0.0312\n","2021-12-08 14:40:54,019 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1515  @50, Recall: 0.1211  MRR: 0.0223\n","2021-12-08 14:40:56,946 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7972  @50, Recall: 0.1523  MRR: 0.0403\n","2021-12-08 14:40:59,834 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9608  @50, Recall: 0.1484  MRR: 0.0313\n","2021-12-08 14:41:02,711 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0119  @50, Recall: 0.1523  MRR: 0.0318\n","2021-12-08 14:41:05,627 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7558  @50, Recall: 0.1836  MRR: 0.0350\n","2021-12-08 14:41:08,497 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9624  @50, Recall: 0.1562  MRR: 0.0370\n","2021-12-08 14:41:11,400 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9119  @50, Recall: 0.1719  MRR: 0.0308\n","2021-12-08 14:41:14,314 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7363  @50, Recall: 0.1797  MRR: 0.0316\n","2021-12-08 14:41:17,214 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1369  @50, Recall: 0.1172  MRR: 0.0241\n","2021-12-08 14:41:20,121 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9626  @50, Recall: 0.1719  MRR: 0.0358\n","2021-12-08 14:41:23,018 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8703  @50, Recall: 0.1758  MRR: 0.0292\n","2021-12-08 14:41:25,972 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9391  @50, Recall: 0.1680  MRR: 0.0244\n","2021-12-08 14:41:28,854 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7911  @50, Recall: 0.1797  MRR: 0.0443\n","2021-12-08 14:41:31,787 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9940  @50, Recall: 0.1328  MRR: 0.0280\n","2021-12-08 14:41:34,728 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0186  @50, Recall: 0.1406  MRR: 0.0316\n","2021-12-08 14:41:37,605 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7585  @50, Recall: 0.1914  MRR: 0.0316\n","2021-12-08 14:41:40,473 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8188  @50, Recall: 0.1680  MRR: 0.0350\n","2021-12-08 14:41:43,335 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0622  @50, Recall: 0.1406  MRR: 0.0281\n","2021-12-08 14:41:46,238 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1164  @50, Recall: 0.1367  MRR: 0.0313\n","2021-12-08 14:41:49,139 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0082  @50, Recall: 0.1406  MRR: 0.0299\n","2021-12-08 14:41:52,019 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9349  @50, Recall: 0.1328  MRR: 0.0294\n","2021-12-08 14:41:55,042 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7890  @50, Recall: 0.1836  MRR: 0.0316\n","2021-12-08 14:41:57,975 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1699  @50, Recall: 0.1172  MRR: 0.0295\n","2021-12-08 14:42:00,961 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0889  @50, Recall: 0.1289  MRR: 0.0284\n","2021-12-08 14:42:03,927 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9420  @50, Recall: 0.1484  MRR: 0.0286\n","2021-12-08 14:42:06,879 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8060  @50, Recall: 0.1758  MRR: 0.0392\n","2021-12-08 14:42:09,836 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.2603  @50, Recall: 0.1055  MRR: 0.0196\n","2021-12-08 14:42:12,745 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9727  @50, Recall: 0.1758  MRR: 0.0445\n","2021-12-08 14:42:15,694 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9904  @50, Recall: 0.1289  MRR: 0.0198\n","2021-12-08 14:42:18,618 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0981  @50, Recall: 0.1211  MRR: 0.0224\n","2021-12-08 14:42:21,535 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9507  @50, Recall: 0.1250  MRR: 0.0332\n","2021-12-08 14:42:24,450 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9969  @50, Recall: 0.1719  MRR: 0.0360\n","2021-12-08 14:42:27,366 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9427  @50, Recall: 0.1484  MRR: 0.0338\n","2021-12-08 14:42:30,337 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9439  @50, Recall: 0.1758  MRR: 0.0338\n","2021-12-08 14:42:33,294 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1497  @50, Recall: 0.1172  MRR: 0.0277\n","2021-12-08 14:42:36,242 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8707  @50, Recall: 0.1836  MRR: 0.0438\n","2021-12-08 14:42:39,164 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9225  @50, Recall: 0.1484  MRR: 0.0245\n","2021-12-08 14:42:42,062 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1597  @50, Recall: 0.1289  MRR: 0.0260\n","2021-12-08 14:42:45,018 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9527  @50, Recall: 0.1445  MRR: 0.0310\n","2021-12-08 14:42:47,987 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1961  @50, Recall: 0.1328  MRR: 0.0223\n","2021-12-08 14:42:50,941 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8514  @50, Recall: 0.1445  MRR: 0.0309\n","2021-12-08 14:42:53,908 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1463  @50, Recall: 0.1328  MRR: 0.0209\n","2021-12-08 14:42:56,793 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1728  @50, Recall: 0.1211  MRR: 0.0223\n","2021-12-08 14:42:59,735 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9448  @50, Recall: 0.1289  MRR: 0.0236\n","2021-12-08 14:43:02,622 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0373  @50, Recall: 0.1484  MRR: 0.0235\n","2021-12-08 14:43:05,614 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1831  @50, Recall: 0.1094  MRR: 0.0216\n","2021-12-08 14:43:08,523 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0649  @50, Recall: 0.1211  MRR: 0.0212\n","2021-12-08 14:43:11,462 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.2327  @50, Recall: 0.1289  MRR: 0.0257\n","2021-12-08 14:43:14,380 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.3073  @50, Recall: 0.1211  MRR: 0.0210\n","2021-12-08 14:43:17,254 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1159  @50, Recall: 0.1328  MRR: 0.0256\n","2021-12-08 14:43:20,191 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9459  @50, Recall: 0.1328  MRR: 0.0332\n","2021-12-08 14:43:23,069 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9625  @50, Recall: 0.1094  MRR: 0.0172\n","2021-12-08 14:43:25,978 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9598  @50, Recall: 0.1602  MRR: 0.0469\n","2021-12-08 14:43:28,885 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0314  @50, Recall: 0.1211  MRR: 0.0201\n","2021-12-08 14:43:31,801 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1732  @50, Recall: 0.1094  MRR: 0.0132\n","2021-12-08 14:43:34,731 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8884  @50, Recall: 0.1523  MRR: 0.0251\n","2021-12-08 14:43:37,716 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9105  @50, Recall: 0.1406  MRR: 0.0261\n","2021-12-08 14:43:40,681 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0056  @50, Recall: 0.1250  MRR: 0.0336\n","2021-12-08 14:43:43,649 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1381  @50, Recall: 0.0938  MRR: 0.0141\n","2021-12-08 14:43:46,627 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9594  @50, Recall: 0.1211  MRR: 0.0253\n","2021-12-08 14:43:49,597 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.2132  @50, Recall: 0.1133  MRR: 0.0174\n","2021-12-08 14:43:52,553 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0710  @50, Recall: 0.1445  MRR: 0.0241\n","2021-12-08 14:43:55,488 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8710  @50, Recall: 0.1523  MRR: 0.0332\n","2021-12-08 14:43:58,436 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6436  @50, Recall: 0.1758  MRR: 0.0372\n","2021-12-08 14:44:01,394 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0287  @50, Recall: 0.1367  MRR: 0.0234\n","2021-12-08 14:44:04,365 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1596  @50, Recall: 0.1016  MRR: 0.0213\n","2021-12-08 14:44:07,323 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9473  @50, Recall: 0.1445  MRR: 0.0384\n","2021-12-08 14:44:10,321 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8702  @50, Recall: 0.1289  MRR: 0.0266\n","2021-12-08 14:44:13,297 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.2227  @50, Recall: 0.1016  MRR: 0.0164\n","2021-12-08 14:44:16,255 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9528  @50, Recall: 0.1133  MRR: 0.0187\n","2021-12-08 14:44:19,264 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1375  @50, Recall: 0.1094  MRR: 0.0297\n","2021-12-08 14:44:22,219 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.2698  @50, Recall: 0.0781  MRR: 0.0183\n","2021-12-08 14:44:25,150 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0339  @50, Recall: 0.0977  MRR: 0.0221\n","2021-12-08 14:44:28,085 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1425  @50, Recall: 0.1172  MRR: 0.0126\n","2021-12-08 14:44:31,088 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8404  @50, Recall: 0.1445  MRR: 0.0314\n","2021-12-08 14:44:34,046 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9687  @50, Recall: 0.1406  MRR: 0.0305\n","2021-12-08 14:44:37,049 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0901  @50, Recall: 0.1289  MRR: 0.0272\n","2021-12-08 14:44:40,021 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1896  @50, Recall: 0.1094  MRR: 0.0188\n","2021-12-08 14:44:43,006 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7624  @50, Recall: 0.1719  MRR: 0.0354\n","2021-12-08 14:44:45,996 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0727  @50, Recall: 0.1289  MRR: 0.0177\n","2021-12-08 14:44:48,957 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1148  @50, Recall: 0.1094  MRR: 0.0309\n","2021-12-08 14:44:51,961 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9784  @50, Recall: 0.1406  MRR: 0.0181\n","2021-12-08 14:44:54,937 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9117  @50, Recall: 0.1367  MRR: 0.0247\n","2021-12-08 14:44:57,892 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0730  @50, Recall: 0.1172  MRR: 0.0278\n","2021-12-08 14:44:59,647 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0116  @50, Recall: 0.0811  MRR: 0.0126\n","2021-12-08 14:44:59,653 src.recall.sr_gnn.train.trainer:INFO:Epoch: 0 Train Loss: 10.5823 Test Loss: 10.0058 Recall: 0.1397 MRR: 0.0283\n","2021-12-08 14:44:59,654 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1397,  0.0283  Epoch: 0,  0\n","Epoch:   5% 1/20 [07:55<2:30:25, 475.04s/it]2021-12-08 14:45:00,344 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 3540\n","2021-12-08 14:45:00,385 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 8.27153\n","2021-12-08 14:45:08,159 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 8.34246\n","2021-12-08 14:45:16,009 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 8.41490\n","2021-12-08 14:45:23,892 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 8.47686\n","2021-12-08 14:45:31,673 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 8.53775\n","2021-12-08 14:45:39,367 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 8.59247\n","2021-12-08 14:45:47,102 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.63908\n","2021-12-08 14:45:55,095 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.68509\n","2021-12-08 14:46:03,047 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.72547\n","2021-12-08 14:46:10,961 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.76359\n","2021-12-08 14:46:18,759 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.80041\n","2021-12-08 14:46:26,619 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.83373\n","2021-12-08 14:46:34,457 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.86387\n","2021-12-08 14:46:42,143 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.89252\n","2021-12-08 14:46:49,750 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.91932\n","2021-12-08 14:46:57,543 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.94269\n","2021-12-08 14:47:05,115 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.96461\n","2021-12-08 14:47:12,688 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.98510\n","2021-12-08 14:47:20,864 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6297  @50, Recall: 0.1914  MRR: 0.0409\n","2021-12-08 14:47:23,770 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8271  @50, Recall: 0.1680  MRR: 0.0330\n","2021-12-08 14:47:26,652 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1245  @50, Recall: 0.1172  MRR: 0.0191\n","2021-12-08 14:47:29,648 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7024  @50, Recall: 0.1953  MRR: 0.0481\n","2021-12-08 14:47:32,566 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8609  @50, Recall: 0.1758  MRR: 0.0282\n","2021-12-08 14:47:35,483 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5609  @50, Recall: 0.2305  MRR: 0.0459\n","2021-12-08 14:47:38,379 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6111  @50, Recall: 0.1875  MRR: 0.0379\n","2021-12-08 14:47:41,268 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8278  @50, Recall: 0.1680  MRR: 0.0297\n","2021-12-08 14:47:44,152 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9017  @50, Recall: 0.1328  MRR: 0.0298\n","2021-12-08 14:47:47,044 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9033  @50, Recall: 0.1484  MRR: 0.0265\n","2021-12-08 14:47:49,957 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6741  @50, Recall: 0.1914  MRR: 0.0374\n","2021-12-08 14:47:52,892 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6878  @50, Recall: 0.2031  MRR: 0.0259\n","2021-12-08 14:47:55,833 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7714  @50, Recall: 0.1445  MRR: 0.0215\n","2021-12-08 14:47:58,764 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9762  @50, Recall: 0.1094  MRR: 0.0234\n","2021-12-08 14:48:01,767 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9047  @50, Recall: 0.1641  MRR: 0.0301\n","2021-12-08 14:48:04,797 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5693  @50, Recall: 0.1875  MRR: 0.0399\n","2021-12-08 14:48:07,755 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7102  @50, Recall: 0.1641  MRR: 0.0415\n","2021-12-08 14:48:10,693 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7490  @50, Recall: 0.1875  MRR: 0.0300\n","2021-12-08 14:48:13,628 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.1241  @50, Recall: 0.1211  MRR: 0.0187\n","2021-12-08 14:48:16,607 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6164  @50, Recall: 0.2031  MRR: 0.0325\n","2021-12-08 14:48:19,599 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9331  @50, Recall: 0.1367  MRR: 0.0210\n","2021-12-08 14:48:22,546 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3555  @50, Recall: 0.2422  MRR: 0.0541\n","2021-12-08 14:48:25,489 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4945  @50, Recall: 0.1914  MRR: 0.0461\n","2021-12-08 14:48:28,498 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4346  @50, Recall: 0.2266  MRR: 0.0591\n","2021-12-08 14:48:31,590 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9026  @50, Recall: 0.1836  MRR: 0.0405\n","2021-12-08 14:48:34,693 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7206  @50, Recall: 0.1992  MRR: 0.0323\n","2021-12-08 14:48:37,705 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7064  @50, Recall: 0.1758  MRR: 0.0218\n","2021-12-08 14:48:40,718 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8616  @50, Recall: 0.1758  MRR: 0.0394\n","2021-12-08 14:48:43,699 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9653  @50, Recall: 0.1602  MRR: 0.0291\n","2021-12-08 14:48:46,692 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6642  @50, Recall: 0.1680  MRR: 0.0345\n","2021-12-08 14:48:49,690 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9174  @50, Recall: 0.1250  MRR: 0.0205\n","2021-12-08 14:48:52,665 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5520  @50, Recall: 0.1758  MRR: 0.0395\n","2021-12-08 14:48:55,648 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7384  @50, Recall: 0.1836  MRR: 0.0347\n","2021-12-08 14:48:58,608 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7187  @50, Recall: 0.1719  MRR: 0.0358\n","2021-12-08 14:49:01,587 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5576  @50, Recall: 0.1953  MRR: 0.0419\n","2021-12-08 14:49:04,549 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6770  @50, Recall: 0.1914  MRR: 0.0319\n","2021-12-08 14:49:07,531 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6464  @50, Recall: 0.2266  MRR: 0.0420\n","2021-12-08 14:49:10,533 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4454  @50, Recall: 0.2070  MRR: 0.0348\n","2021-12-08 14:49:13,533 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9053  @50, Recall: 0.1562  MRR: 0.0263\n","2021-12-08 14:49:16,517 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7424  @50, Recall: 0.1836  MRR: 0.0333\n","2021-12-08 14:49:19,510 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6775  @50, Recall: 0.1875  MRR: 0.0321\n","2021-12-08 14:49:22,475 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6560  @50, Recall: 0.1992  MRR: 0.0262\n","2021-12-08 14:49:25,474 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5483  @50, Recall: 0.1992  MRR: 0.0454\n","2021-12-08 14:49:28,444 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7429  @50, Recall: 0.1523  MRR: 0.0299\n","2021-12-08 14:49:31,495 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7104  @50, Recall: 0.1641  MRR: 0.0329\n","2021-12-08 14:49:34,526 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4572  @50, Recall: 0.2109  MRR: 0.0370\n","2021-12-08 14:49:37,507 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6072  @50, Recall: 0.1719  MRR: 0.0339\n","2021-12-08 14:49:40,555 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7767  @50, Recall: 0.1602  MRR: 0.0313\n","2021-12-08 14:49:43,590 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8845  @50, Recall: 0.1602  MRR: 0.0293\n","2021-12-08 14:49:46,676 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8135  @50, Recall: 0.1484  MRR: 0.0273\n","2021-12-08 14:49:49,722 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6946  @50, Recall: 0.1641  MRR: 0.0320\n","2021-12-08 14:49:52,769 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6339  @50, Recall: 0.2109  MRR: 0.0372\n","2021-12-08 14:49:55,832 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9266  @50, Recall: 0.1484  MRR: 0.0277\n","2021-12-08 14:49:58,927 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8228  @50, Recall: 0.1602  MRR: 0.0361\n","2021-12-08 14:50:02,084 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6431  @50, Recall: 0.1680  MRR: 0.0296\n","2021-12-08 14:50:05,125 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6213  @50, Recall: 0.1680  MRR: 0.0388\n","2021-12-08 14:50:08,134 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9436  @50, Recall: 0.1328  MRR: 0.0213\n","2021-12-08 14:50:11,165 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6754  @50, Recall: 0.2031  MRR: 0.0488\n","2021-12-08 14:50:14,288 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7364  @50, Recall: 0.1523  MRR: 0.0191\n","2021-12-08 14:50:17,393 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8314  @50, Recall: 0.1602  MRR: 0.0261\n","2021-12-08 14:50:20,485 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6640  @50, Recall: 0.1836  MRR: 0.0324\n","2021-12-08 14:50:23,522 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7833  @50, Recall: 0.1836  MRR: 0.0350\n","2021-12-08 14:50:26,541 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7754  @50, Recall: 0.1719  MRR: 0.0298\n","2021-12-08 14:50:29,582 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6867  @50, Recall: 0.2109  MRR: 0.0404\n","2021-12-08 14:50:32,612 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9010  @50, Recall: 0.1406  MRR: 0.0300\n","2021-12-08 14:50:35,651 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5635  @50, Recall: 0.2109  MRR: 0.0479\n","2021-12-08 14:50:38,695 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6800  @50, Recall: 0.1875  MRR: 0.0330\n","2021-12-08 14:50:41,728 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8643  @50, Recall: 0.1367  MRR: 0.0274\n","2021-12-08 14:50:44,743 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6994  @50, Recall: 0.1797  MRR: 0.0392\n","2021-12-08 14:50:47,752 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9144  @50, Recall: 0.1641  MRR: 0.0270\n","2021-12-08 14:50:50,738 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5860  @50, Recall: 0.1797  MRR: 0.0359\n","2021-12-08 14:50:53,706 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8887  @50, Recall: 0.1484  MRR: 0.0167\n","2021-12-08 14:50:56,684 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0163  @50, Recall: 0.1367  MRR: 0.0217\n","2021-12-08 14:50:59,673 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7760  @50, Recall: 0.1445  MRR: 0.0165\n","2021-12-08 14:51:02,671 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7588  @50, Recall: 0.1719  MRR: 0.0258\n","2021-12-08 14:51:05,666 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8910  @50, Recall: 0.1484  MRR: 0.0280\n","2021-12-08 14:51:08,679 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8917  @50, Recall: 0.1367  MRR: 0.0269\n","2021-12-08 14:51:11,707 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9261  @50, Recall: 0.1602  MRR: 0.0256\n","2021-12-08 14:51:14,720 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0717  @50, Recall: 0.1367  MRR: 0.0262\n","2021-12-08 14:51:17,751 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8318  @50, Recall: 0.1602  MRR: 0.0296\n","2021-12-08 14:51:20,842 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7363  @50, Recall: 0.1602  MRR: 0.0326\n","2021-12-08 14:51:23,853 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7166  @50, Recall: 0.1367  MRR: 0.0185\n","2021-12-08 14:51:26,867 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7106  @50, Recall: 0.1914  MRR: 0.0431\n","2021-12-08 14:51:29,901 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6831  @50, Recall: 0.1719  MRR: 0.0279\n","2021-12-08 14:51:32,910 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9460  @50, Recall: 0.1250  MRR: 0.0143\n","2021-12-08 14:51:35,918 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6573  @50, Recall: 0.1719  MRR: 0.0275\n","2021-12-08 14:51:38,934 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5536  @50, Recall: 0.1758  MRR: 0.0320\n","2021-12-08 14:51:41,950 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7840  @50, Recall: 0.1484  MRR: 0.0280\n","2021-12-08 14:51:44,960 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8562  @50, Recall: 0.1289  MRR: 0.0133\n","2021-12-08 14:51:47,976 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6644  @50, Recall: 0.1641  MRR: 0.0298\n","2021-12-08 14:51:50,978 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9633  @50, Recall: 0.1211  MRR: 0.0233\n","2021-12-08 14:51:54,101 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8217  @50, Recall: 0.1680  MRR: 0.0277\n","2021-12-08 14:51:57,101 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6766  @50, Recall: 0.1875  MRR: 0.0363\n","2021-12-08 14:52:00,110 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4517  @50, Recall: 0.1914  MRR: 0.0433\n","2021-12-08 14:52:03,126 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7534  @50, Recall: 0.1406  MRR: 0.0291\n","2021-12-08 14:52:06,118 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8967  @50, Recall: 0.1328  MRR: 0.0228\n","2021-12-08 14:52:09,086 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6673  @50, Recall: 0.1680  MRR: 0.0371\n","2021-12-08 14:52:12,059 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6763  @50, Recall: 0.1523  MRR: 0.0302\n","2021-12-08 14:52:15,044 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9382  @50, Recall: 0.1406  MRR: 0.0198\n","2021-12-08 14:52:18,037 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6617  @50, Recall: 0.1641  MRR: 0.0212\n","2021-12-08 14:52:21,030 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9310  @50, Recall: 0.1367  MRR: 0.0308\n","2021-12-08 14:52:23,987 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0140  @50, Recall: 0.1094  MRR: 0.0190\n","2021-12-08 14:52:26,983 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7669  @50, Recall: 0.1289  MRR: 0.0222\n","2021-12-08 14:52:29,970 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8211  @50, Recall: 0.1602  MRR: 0.0212\n","2021-12-08 14:52:32,946 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5321  @50, Recall: 0.1602  MRR: 0.0341\n","2021-12-08 14:52:35,889 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7053  @50, Recall: 0.1523  MRR: 0.0312\n","2021-12-08 14:52:38,838 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7930  @50, Recall: 0.1445  MRR: 0.0292\n","2021-12-08 14:52:41,771 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9342  @50, Recall: 0.1406  MRR: 0.0256\n","2021-12-08 14:52:44,737 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5090  @50, Recall: 0.1836  MRR: 0.0440\n","2021-12-08 14:52:47,662 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7701  @50, Recall: 0.1680  MRR: 0.0258\n","2021-12-08 14:52:50,558 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8039  @50, Recall: 0.1406  MRR: 0.0267\n","2021-12-08 14:52:53,441 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6484  @50, Recall: 0.1914  MRR: 0.0252\n","2021-12-08 14:52:56,307 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6887  @50, Recall: 0.1602  MRR: 0.0267\n","2021-12-08 14:52:59,222 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8269  @50, Recall: 0.1445  MRR: 0.0288\n","2021-12-08 14:53:00,913 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7405  @50, Recall: 0.1014  MRR: 0.0218\n","2021-12-08 14:53:00,921 src.recall.sr_gnn.train.trainer:INFO:Epoch: 1 Train Loss: 8.9982 Test Loss: 9.7526 Recall: 0.1667 MRR: 0.0310\n","2021-12-08 14:53:00,921 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1667,  0.0310  Epoch: 1,  1\n","Epoch:  10% 2/20 [15:56<2:23:35, 478.64s/it]2021-12-08 14:53:01,506 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 3540\n","2021-12-08 14:53:01,549 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.76985\n","2021-12-08 14:53:09,369 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.84621\n","2021-12-08 14:53:17,214 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.92661\n","2021-12-08 14:53:24,993 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 8.00207\n","2021-12-08 14:53:32,817 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 8.07838\n","2021-12-08 14:53:40,689 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 8.14540\n","2021-12-08 14:53:48,513 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.20966\n","2021-12-08 14:53:56,337 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.26774\n","2021-12-08 14:54:04,285 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.32111\n","2021-12-08 14:54:12,317 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.37169\n","2021-12-08 14:54:20,326 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.41942\n","2021-12-08 14:54:28,249 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.46337\n","2021-12-08 14:54:36,145 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.50405\n","2021-12-08 14:54:44,069 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.54161\n","2021-12-08 14:54:51,886 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.57714\n","2021-12-08 14:54:59,693 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.61087\n","2021-12-08 14:55:07,523 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.64133\n","2021-12-08 14:55:15,417 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.66954\n","2021-12-08 14:55:23,829 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5593  @50, Recall: 0.2070  MRR: 0.0462\n","2021-12-08 14:55:26,713 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8068  @50, Recall: 0.1719  MRR: 0.0288\n","2021-12-08 14:55:29,568 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0709  @50, Recall: 0.1094  MRR: 0.0170\n","2021-12-08 14:55:32,453 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6829  @50, Recall: 0.2031  MRR: 0.0435\n","2021-12-08 14:55:35,344 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8028  @50, Recall: 0.1602  MRR: 0.0275\n","2021-12-08 14:55:38,251 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4793  @50, Recall: 0.2227  MRR: 0.0447\n","2021-12-08 14:55:41,150 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5689  @50, Recall: 0.2070  MRR: 0.0399\n","2021-12-08 14:55:44,123 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7703  @50, Recall: 0.1914  MRR: 0.0308\n","2021-12-08 14:55:47,063 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8386  @50, Recall: 0.1367  MRR: 0.0291\n","2021-12-08 14:55:50,005 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8294  @50, Recall: 0.1719  MRR: 0.0321\n","2021-12-08 14:55:52,943 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6123  @50, Recall: 0.1797  MRR: 0.0357\n","2021-12-08 14:55:55,884 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6432  @50, Recall: 0.1914  MRR: 0.0261\n","2021-12-08 14:55:58,811 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7497  @50, Recall: 0.1562  MRR: 0.0234\n","2021-12-08 14:56:01,755 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9545  @50, Recall: 0.1289  MRR: 0.0229\n","2021-12-08 14:56:04,715 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8525  @50, Recall: 0.1758  MRR: 0.0301\n","2021-12-08 14:56:07,649 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5412  @50, Recall: 0.2031  MRR: 0.0359\n","2021-12-08 14:56:10,577 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6907  @50, Recall: 0.1562  MRR: 0.0386\n","2021-12-08 14:56:13,533 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6884  @50, Recall: 0.1875  MRR: 0.0333\n","2021-12-08 14:56:16,495 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0790  @50, Recall: 0.1172  MRR: 0.0191\n","2021-12-08 14:56:19,535 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5953  @50, Recall: 0.1992  MRR: 0.0325\n","2021-12-08 14:56:22,479 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9034  @50, Recall: 0.1523  MRR: 0.0192\n","2021-12-08 14:56:25,479 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3565  @50, Recall: 0.2305  MRR: 0.0445\n","2021-12-08 14:56:28,421 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4508  @50, Recall: 0.1953  MRR: 0.0453\n","2021-12-08 14:56:31,353 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4198  @50, Recall: 0.2383  MRR: 0.0596\n","2021-12-08 14:56:34,261 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8216  @50, Recall: 0.1914  MRR: 0.0412\n","2021-12-08 14:56:37,179 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7089  @50, Recall: 0.1914  MRR: 0.0340\n","2021-12-08 14:56:40,076 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6366  @50, Recall: 0.1875  MRR: 0.0224\n","2021-12-08 14:56:42,972 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8106  @50, Recall: 0.1914  MRR: 0.0399\n","2021-12-08 14:56:45,887 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8666  @50, Recall: 0.1680  MRR: 0.0310\n","2021-12-08 14:56:48,770 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5859  @50, Recall: 0.1758  MRR: 0.0390\n","2021-12-08 14:56:51,707 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8837  @50, Recall: 0.1328  MRR: 0.0211\n","2021-12-08 14:56:54,764 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5582  @50, Recall: 0.1836  MRR: 0.0430\n","2021-12-08 14:56:57,627 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7101  @50, Recall: 0.1875  MRR: 0.0355\n","2021-12-08 14:57:00,483 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6411  @50, Recall: 0.1797  MRR: 0.0422\n","2021-12-08 14:57:03,352 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4758  @50, Recall: 0.1914  MRR: 0.0360\n","2021-12-08 14:57:06,219 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6395  @50, Recall: 0.1914  MRR: 0.0326\n","2021-12-08 14:57:09,071 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5597  @50, Recall: 0.2188  MRR: 0.0410\n","2021-12-08 14:57:11,928 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4126  @50, Recall: 0.2109  MRR: 0.0358\n","2021-12-08 14:57:14,869 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8561  @50, Recall: 0.1758  MRR: 0.0269\n","2021-12-08 14:57:17,763 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7164  @50, Recall: 0.2031  MRR: 0.0364\n","2021-12-08 14:57:20,658 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6371  @50, Recall: 0.2070  MRR: 0.0282\n","2021-12-08 14:57:23,657 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6085  @50, Recall: 0.1992  MRR: 0.0243\n","2021-12-08 14:57:26,598 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4907  @50, Recall: 0.2070  MRR: 0.0465\n","2021-12-08 14:57:29,495 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6841  @50, Recall: 0.1680  MRR: 0.0321\n","2021-12-08 14:57:32,385 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6945  @50, Recall: 0.1719  MRR: 0.0362\n","2021-12-08 14:57:35,277 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3995  @50, Recall: 0.2070  MRR: 0.0375\n","2021-12-08 14:57:38,193 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5699  @50, Recall: 0.1875  MRR: 0.0373\n","2021-12-08 14:57:41,075 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7193  @50, Recall: 0.1836  MRR: 0.0319\n","2021-12-08 14:57:43,940 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7977  @50, Recall: 0.1484  MRR: 0.0307\n","2021-12-08 14:57:46,859 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7034  @50, Recall: 0.1758  MRR: 0.0309\n","2021-12-08 14:57:49,756 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6441  @50, Recall: 0.1680  MRR: 0.0316\n","2021-12-08 14:57:52,640 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5771  @50, Recall: 0.2266  MRR: 0.0371\n","2021-12-08 14:57:55,561 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8792  @50, Recall: 0.1445  MRR: 0.0293\n","2021-12-08 14:57:58,565 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7306  @50, Recall: 0.1641  MRR: 0.0383\n","2021-12-08 14:58:01,526 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5398  @50, Recall: 0.1875  MRR: 0.0316\n","2021-12-08 14:58:04,466 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5086  @50, Recall: 0.1914  MRR: 0.0389\n","2021-12-08 14:58:07,406 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9169  @50, Recall: 0.1367  MRR: 0.0227\n","2021-12-08 14:58:10,333 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6768  @50, Recall: 0.1992  MRR: 0.0473\n","2021-12-08 14:58:13,254 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6723  @50, Recall: 0.1641  MRR: 0.0260\n","2021-12-08 14:58:16,188 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7384  @50, Recall: 0.1641  MRR: 0.0296\n","2021-12-08 14:58:19,145 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6046  @50, Recall: 0.1992  MRR: 0.0327\n","2021-12-08 14:58:22,104 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6666  @50, Recall: 0.2227  MRR: 0.0393\n","2021-12-08 14:58:25,022 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7055  @50, Recall: 0.1797  MRR: 0.0299\n","2021-12-08 14:58:27,946 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6003  @50, Recall: 0.2422  MRR: 0.0414\n","2021-12-08 14:58:30,936 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8779  @50, Recall: 0.1367  MRR: 0.0267\n","2021-12-08 14:58:33,874 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5722  @50, Recall: 0.2305  MRR: 0.0484\n","2021-12-08 14:58:36,845 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6432  @50, Recall: 0.1914  MRR: 0.0299\n","2021-12-08 14:58:39,794 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7767  @50, Recall: 0.1602  MRR: 0.0288\n","2021-12-08 14:58:42,719 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6180  @50, Recall: 0.1992  MRR: 0.0400\n","2021-12-08 14:58:45,657 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8737  @50, Recall: 0.1641  MRR: 0.0318\n","2021-12-08 14:58:48,581 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4685  @50, Recall: 0.2070  MRR: 0.0346\n","2021-12-08 14:58:51,448 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8263  @50, Recall: 0.1758  MRR: 0.0231\n","2021-12-08 14:58:54,320 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9395  @50, Recall: 0.1289  MRR: 0.0213\n","2021-12-08 14:58:57,180 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7472  @50, Recall: 0.1602  MRR: 0.0244\n","2021-12-08 14:59:00,043 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6890  @50, Recall: 0.1719  MRR: 0.0278\n","2021-12-08 14:59:03,014 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7895  @50, Recall: 0.1523  MRR: 0.0290\n","2021-12-08 14:59:05,896 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7973  @50, Recall: 0.1680  MRR: 0.0272\n","2021-12-08 14:59:08,788 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9289  @50, Recall: 0.1484  MRR: 0.0236\n","2021-12-08 14:59:11,723 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9714  @50, Recall: 0.1406  MRR: 0.0274\n","2021-12-08 14:59:14,627 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7463  @50, Recall: 0.1367  MRR: 0.0296\n","2021-12-08 14:59:17,495 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7051  @50, Recall: 0.1719  MRR: 0.0346\n","2021-12-08 14:59:20,374 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6202  @50, Recall: 0.1445  MRR: 0.0197\n","2021-12-08 14:59:23,225 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6650  @50, Recall: 0.2031  MRR: 0.0362\n","2021-12-08 14:59:26,083 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6741  @50, Recall: 0.1797  MRR: 0.0283\n","2021-12-08 14:59:28,948 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8588  @50, Recall: 0.1484  MRR: 0.0153\n","2021-12-08 14:59:31,792 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7154  @50, Recall: 0.1641  MRR: 0.0276\n","2021-12-08 14:59:34,759 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4684  @50, Recall: 0.1797  MRR: 0.0312\n","2021-12-08 14:59:37,673 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6865  @50, Recall: 0.1562  MRR: 0.0288\n","2021-12-08 14:59:40,575 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8058  @50, Recall: 0.1523  MRR: 0.0157\n","2021-12-08 14:59:43,469 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6793  @50, Recall: 0.1484  MRR: 0.0324\n","2021-12-08 14:59:46,365 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9866  @50, Recall: 0.1406  MRR: 0.0269\n","2021-12-08 14:59:49,271 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8083  @50, Recall: 0.1641  MRR: 0.0245\n","2021-12-08 14:59:52,198 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6369  @50, Recall: 0.1836  MRR: 0.0399\n","2021-12-08 14:59:55,168 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4249  @50, Recall: 0.2109  MRR: 0.0446\n","2021-12-08 14:59:58,127 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6750  @50, Recall: 0.1484  MRR: 0.0309\n","2021-12-08 15:00:01,095 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8591  @50, Recall: 0.1289  MRR: 0.0239\n","2021-12-08 15:00:04,026 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6138  @50, Recall: 0.1797  MRR: 0.0382\n","2021-12-08 15:00:06,972 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6277  @50, Recall: 0.1797  MRR: 0.0284\n","2021-12-08 15:00:09,955 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9061  @50, Recall: 0.1406  MRR: 0.0198\n","2021-12-08 15:00:12,915 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6050  @50, Recall: 0.1680  MRR: 0.0196\n","2021-12-08 15:00:15,889 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8487  @50, Recall: 0.1484  MRR: 0.0264\n","2021-12-08 15:00:18,905 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9577  @50, Recall: 0.1172  MRR: 0.0175\n","2021-12-08 15:00:21,869 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7385  @50, Recall: 0.1367  MRR: 0.0260\n","2021-12-08 15:00:24,826 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7935  @50, Recall: 0.1680  MRR: 0.0219\n","2021-12-08 15:00:27,806 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5703  @50, Recall: 0.1641  MRR: 0.0303\n","2021-12-08 15:00:30,774 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7013  @50, Recall: 0.1953  MRR: 0.0302\n","2021-12-08 15:00:33,759 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7790  @50, Recall: 0.1406  MRR: 0.0305\n","2021-12-08 15:00:36,727 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8975  @50, Recall: 0.1367  MRR: 0.0192\n","2021-12-08 15:00:39,664 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4802  @50, Recall: 0.1953  MRR: 0.0419\n","2021-12-08 15:00:42,615 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7193  @50, Recall: 0.1641  MRR: 0.0272\n","2021-12-08 15:00:45,571 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7916  @50, Recall: 0.1523  MRR: 0.0268\n","2021-12-08 15:00:48,514 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6275  @50, Recall: 0.1680  MRR: 0.0215\n","2021-12-08 15:00:51,475 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6141  @50, Recall: 0.1641  MRR: 0.0261\n","2021-12-08 15:00:54,376 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7122  @50, Recall: 0.1445  MRR: 0.0270\n","2021-12-08 15:00:56,073 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7794  @50, Recall: 0.1216  MRR: 0.0189\n","2021-12-08 15:00:56,080 src.recall.sr_gnn.train.trainer:INFO:Epoch: 2 Train Loss: 8.6889 Test Loss: 9.7034 Recall: 0.1741 MRR: 0.0314\n","2021-12-08 15:00:56,080 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1741,  0.0314  Epoch: 2,  2\n","Epoch:  15% 3/20 [23:51<2:15:09, 477.06s/it]2021-12-08 15:00:56,676 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 3540\n","2021-12-08 15:00:56,718 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.67851\n","2021-12-08 15:01:04,476 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.71470\n","2021-12-08 15:01:12,263 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.79194\n","2021-12-08 15:01:20,068 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 7.87309\n","2021-12-08 15:01:27,844 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 7.95005\n","2021-12-08 15:01:35,548 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 8.02156\n","2021-12-08 15:01:43,284 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.08777\n","2021-12-08 15:01:51,189 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.14690\n","2021-12-08 15:01:59,312 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.20286\n","2021-12-08 15:02:07,292 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.25460\n","2021-12-08 15:02:15,207 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.30457\n","2021-12-08 15:02:23,168 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.35069\n","2021-12-08 15:02:31,057 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.39488\n","2021-12-08 15:02:38,892 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.43278\n","2021-12-08 15:02:46,687 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.46985\n","2021-12-08 15:02:54,414 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.50415\n","2021-12-08 15:03:02,143 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.53619\n","2021-12-08 15:03:09,831 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.56694\n","2021-12-08 15:03:18,025 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5530  @50, Recall: 0.1914  MRR: 0.0432\n","2021-12-08 15:03:20,925 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7231  @50, Recall: 0.1680  MRR: 0.0293\n","2021-12-08 15:03:23,802 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0103  @50, Recall: 0.1172  MRR: 0.0166\n","2021-12-08 15:03:26,709 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6939  @50, Recall: 0.2031  MRR: 0.0446\n","2021-12-08 15:03:29,637 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7828  @50, Recall: 0.1680  MRR: 0.0305\n","2021-12-08 15:03:32,573 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4475  @50, Recall: 0.2188  MRR: 0.0473\n","2021-12-08 15:03:35,516 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5467  @50, Recall: 0.1992  MRR: 0.0400\n","2021-12-08 15:03:38,464 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7269  @50, Recall: 0.1836  MRR: 0.0348\n","2021-12-08 15:03:41,382 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8646  @50, Recall: 0.1484  MRR: 0.0287\n","2021-12-08 15:03:44,310 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8633  @50, Recall: 0.1641  MRR: 0.0292\n","2021-12-08 15:03:47,343 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5360  @50, Recall: 0.1875  MRR: 0.0315\n","2021-12-08 15:03:50,364 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6307  @50, Recall: 0.2070  MRR: 0.0267\n","2021-12-08 15:03:53,402 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7453  @50, Recall: 0.1641  MRR: 0.0241\n","2021-12-08 15:03:56,426 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9579  @50, Recall: 0.1367  MRR: 0.0222\n","2021-12-08 15:03:59,417 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8550  @50, Recall: 0.1719  MRR: 0.0371\n","2021-12-08 15:04:02,390 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4378  @50, Recall: 0.2109  MRR: 0.0387\n","2021-12-08 15:04:05,412 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6566  @50, Recall: 0.1562  MRR: 0.0398\n","2021-12-08 15:04:08,358 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7039  @50, Recall: 0.1758  MRR: 0.0314\n","2021-12-08 15:04:11,310 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0235  @50, Recall: 0.1289  MRR: 0.0237\n","2021-12-08 15:04:14,233 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5554  @50, Recall: 0.1914  MRR: 0.0357\n","2021-12-08 15:04:17,179 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8273  @50, Recall: 0.1602  MRR: 0.0183\n","2021-12-08 15:04:20,123 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3228  @50, Recall: 0.2266  MRR: 0.0520\n","2021-12-08 15:04:23,031 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4267  @50, Recall: 0.1992  MRR: 0.0450\n","2021-12-08 15:04:25,959 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4112  @50, Recall: 0.2266  MRR: 0.0577\n","2021-12-08 15:04:28,956 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8622  @50, Recall: 0.1719  MRR: 0.0402\n","2021-12-08 15:04:31,919 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7121  @50, Recall: 0.1797  MRR: 0.0317\n","2021-12-08 15:04:34,883 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5711  @50, Recall: 0.1953  MRR: 0.0231\n","2021-12-08 15:04:37,839 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7754  @50, Recall: 0.1719  MRR: 0.0396\n","2021-12-08 15:04:40,718 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8797  @50, Recall: 0.1602  MRR: 0.0299\n","2021-12-08 15:04:43,585 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5830  @50, Recall: 0.1719  MRR: 0.0352\n","2021-12-08 15:04:46,506 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8499  @50, Recall: 0.1484  MRR: 0.0202\n","2021-12-08 15:04:49,399 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4676  @50, Recall: 0.1875  MRR: 0.0425\n","2021-12-08 15:04:52,310 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7056  @50, Recall: 0.1836  MRR: 0.0345\n","2021-12-08 15:04:55,231 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6321  @50, Recall: 0.1875  MRR: 0.0463\n","2021-12-08 15:04:58,155 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4388  @50, Recall: 0.2031  MRR: 0.0403\n","2021-12-08 15:05:01,055 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6314  @50, Recall: 0.1836  MRR: 0.0323\n","2021-12-08 15:05:03,945 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5307  @50, Recall: 0.2305  MRR: 0.0436\n","2021-12-08 15:05:06,899 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3666  @50, Recall: 0.2266  MRR: 0.0384\n","2021-12-08 15:05:09,792 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8071  @50, Recall: 0.1523  MRR: 0.0284\n","2021-12-08 15:05:12,686 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6601  @50, Recall: 0.2031  MRR: 0.0371\n","2021-12-08 15:05:15,577 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6683  @50, Recall: 0.1953  MRR: 0.0304\n","2021-12-08 15:05:18,483 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6252  @50, Recall: 0.1797  MRR: 0.0247\n","2021-12-08 15:05:21,428 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4575  @50, Recall: 0.2070  MRR: 0.0467\n","2021-12-08 15:05:24,320 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6896  @50, Recall: 0.1680  MRR: 0.0305\n","2021-12-08 15:05:27,232 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6790  @50, Recall: 0.1641  MRR: 0.0349\n","2021-12-08 15:05:30,121 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4107  @50, Recall: 0.2148  MRR: 0.0387\n","2021-12-08 15:05:33,037 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5246  @50, Recall: 0.1953  MRR: 0.0342\n","2021-12-08 15:05:35,970 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6937  @50, Recall: 0.1680  MRR: 0.0318\n","2021-12-08 15:05:38,915 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8269  @50, Recall: 0.1719  MRR: 0.0325\n","2021-12-08 15:05:41,852 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7708  @50, Recall: 0.1562  MRR: 0.0260\n","2021-12-08 15:05:44,786 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5828  @50, Recall: 0.1719  MRR: 0.0354\n","2021-12-08 15:05:47,756 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5383  @50, Recall: 0.2227  MRR: 0.0358\n","2021-12-08 15:05:50,705 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8725  @50, Recall: 0.1523  MRR: 0.0258\n","2021-12-08 15:05:53,639 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6935  @50, Recall: 0.1562  MRR: 0.0340\n","2021-12-08 15:05:56,562 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5923  @50, Recall: 0.1758  MRR: 0.0286\n","2021-12-08 15:05:59,498 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4936  @50, Recall: 0.1836  MRR: 0.0345\n","2021-12-08 15:06:02,420 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9368  @50, Recall: 0.1367  MRR: 0.0193\n","2021-12-08 15:06:05,346 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6583  @50, Recall: 0.2031  MRR: 0.0468\n","2021-12-08 15:06:08,271 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6576  @50, Recall: 0.1641  MRR: 0.0242\n","2021-12-08 15:06:11,222 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7045  @50, Recall: 0.1836  MRR: 0.0302\n","2021-12-08 15:06:14,215 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5152  @50, Recall: 0.1875  MRR: 0.0337\n","2021-12-08 15:06:17,149 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6337  @50, Recall: 0.2031  MRR: 0.0412\n","2021-12-08 15:06:20,123 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7340  @50, Recall: 0.1797  MRR: 0.0316\n","2021-12-08 15:06:23,027 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5727  @50, Recall: 0.2227  MRR: 0.0414\n","2021-12-08 15:06:25,946 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8764  @50, Recall: 0.1484  MRR: 0.0282\n","2021-12-08 15:06:28,866 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5681  @50, Recall: 0.2227  MRR: 0.0507\n","2021-12-08 15:06:31,769 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5871  @50, Recall: 0.1914  MRR: 0.0339\n","2021-12-08 15:06:34,625 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7747  @50, Recall: 0.1562  MRR: 0.0296\n","2021-12-08 15:06:37,515 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6168  @50, Recall: 0.2070  MRR: 0.0410\n","2021-12-08 15:06:40,432 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8427  @50, Recall: 0.1797  MRR: 0.0287\n","2021-12-08 15:06:43,339 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4951  @50, Recall: 0.1953  MRR: 0.0303\n","2021-12-08 15:06:46,303 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8153  @50, Recall: 0.1680  MRR: 0.0264\n","2021-12-08 15:06:49,206 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8985  @50, Recall: 0.1328  MRR: 0.0195\n","2021-12-08 15:06:52,105 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7225  @50, Recall: 0.1602  MRR: 0.0205\n","2021-12-08 15:06:55,154 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6637  @50, Recall: 0.1797  MRR: 0.0290\n","2021-12-08 15:06:58,032 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7202  @50, Recall: 0.1602  MRR: 0.0310\n","2021-12-08 15:07:00,920 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7519  @50, Recall: 0.1602  MRR: 0.0258\n","2021-12-08 15:07:03,848 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8961  @50, Recall: 0.1523  MRR: 0.0285\n","2021-12-08 15:07:06,770 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9743  @50, Recall: 0.1484  MRR: 0.0260\n","2021-12-08 15:07:09,641 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7523  @50, Recall: 0.1562  MRR: 0.0302\n","2021-12-08 15:07:12,512 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6234  @50, Recall: 0.1797  MRR: 0.0368\n","2021-12-08 15:07:15,377 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5386  @50, Recall: 0.1758  MRR: 0.0223\n","2021-12-08 15:07:18,358 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6227  @50, Recall: 0.1992  MRR: 0.0376\n","2021-12-08 15:07:21,274 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6278  @50, Recall: 0.1758  MRR: 0.0231\n","2021-12-08 15:07:24,207 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8355  @50, Recall: 0.1406  MRR: 0.0153\n","2021-12-08 15:07:27,111 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6239  @50, Recall: 0.1797  MRR: 0.0307\n","2021-12-08 15:07:29,993 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4541  @50, Recall: 0.1914  MRR: 0.0369\n","2021-12-08 15:07:32,889 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6639  @50, Recall: 0.1562  MRR: 0.0278\n","2021-12-08 15:07:35,840 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7549  @50, Recall: 0.1562  MRR: 0.0195\n","2021-12-08 15:07:38,843 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6420  @50, Recall: 0.1641  MRR: 0.0294\n","2021-12-08 15:07:41,787 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9380  @50, Recall: 0.1406  MRR: 0.0271\n","2021-12-08 15:07:44,756 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7780  @50, Recall: 0.1641  MRR: 0.0271\n","2021-12-08 15:07:47,736 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6756  @50, Recall: 0.1875  MRR: 0.0418\n","2021-12-08 15:07:50,759 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4287  @50, Recall: 0.2031  MRR: 0.0458\n","2021-12-08 15:07:53,752 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6591  @50, Recall: 0.1523  MRR: 0.0292\n","2021-12-08 15:07:56,773 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8289  @50, Recall: 0.1289  MRR: 0.0221\n","2021-12-08 15:07:59,744 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5960  @50, Recall: 0.1836  MRR: 0.0406\n","2021-12-08 15:08:02,708 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6151  @50, Recall: 0.1719  MRR: 0.0276\n","2021-12-08 15:08:05,645 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8778  @50, Recall: 0.1484  MRR: 0.0233\n","2021-12-08 15:08:08,665 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5638  @50, Recall: 0.1797  MRR: 0.0230\n","2021-12-08 15:08:11,590 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8712  @50, Recall: 0.1211  MRR: 0.0266\n","2021-12-08 15:08:14,523 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9089  @50, Recall: 0.1094  MRR: 0.0210\n","2021-12-08 15:08:17,472 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7682  @50, Recall: 0.1367  MRR: 0.0234\n","2021-12-08 15:08:20,497 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7476  @50, Recall: 0.1602  MRR: 0.0220\n","2021-12-08 15:08:23,526 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5337  @50, Recall: 0.1719  MRR: 0.0372\n","2021-12-08 15:08:26,566 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6630  @50, Recall: 0.1875  MRR: 0.0284\n","2021-12-08 15:08:29,568 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7786  @50, Recall: 0.1406  MRR: 0.0374\n","2021-12-08 15:08:32,565 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8715  @50, Recall: 0.1523  MRR: 0.0245\n","2021-12-08 15:08:35,585 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4035  @50, Recall: 0.1992  MRR: 0.0429\n","2021-12-08 15:08:38,606 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7339  @50, Recall: 0.1562  MRR: 0.0290\n","2021-12-08 15:08:41,590 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7745  @50, Recall: 0.1602  MRR: 0.0323\n","2021-12-08 15:08:44,595 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5989  @50, Recall: 0.1797  MRR: 0.0257\n","2021-12-08 15:08:47,602 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6081  @50, Recall: 0.1680  MRR: 0.0293\n","2021-12-08 15:08:50,590 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6932  @50, Recall: 0.1484  MRR: 0.0281\n","2021-12-08 15:08:52,380 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7888  @50, Recall: 0.1216  MRR: 0.0138\n","2021-12-08 15:08:52,388 src.recall.sr_gnn.train.trainer:INFO:Epoch: 3 Train Loss: 8.5866 Test Loss: 9.6813 Recall: 0.1744 MRR: 0.0320\n","2021-12-08 15:08:52,388 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1744,  0.0320  Epoch: 3,  3\n","Epoch:  20% 4/20 [31:47<2:07:08, 476.76s/it]2021-12-08 15:08:52,931 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 3540\n","2021-12-08 15:08:52,973 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.34695\n","2021-12-08 15:09:01,043 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.65221\n","2021-12-08 15:09:09,079 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.73387\n","2021-12-08 15:09:16,961 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 7.81551\n","2021-12-08 15:09:24,920 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 7.89083\n","2021-12-08 15:09:32,963 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 7.96000\n","2021-12-08 15:09:40,971 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.02435\n","2021-12-08 15:09:48,933 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.08540\n","2021-12-08 15:09:56,990 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.14328\n","2021-12-08 15:10:05,077 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.19693\n","2021-12-08 15:10:13,114 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.24649\n","2021-12-08 15:10:21,205 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.29323\n","2021-12-08 15:10:29,170 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.33730\n","2021-12-08 15:10:37,148 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.37757\n","2021-12-08 15:10:45,051 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.41441\n","2021-12-08 15:10:52,856 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.44933\n","2021-12-08 15:11:00,680 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.48180\n","2021-12-08 15:11:08,465 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.51196\n","2021-12-08 15:11:16,918 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5292  @50, Recall: 0.2031  MRR: 0.0478\n","2021-12-08 15:11:19,918 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7922  @50, Recall: 0.1719  MRR: 0.0316\n","2021-12-08 15:11:22,878 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0054  @50, Recall: 0.1133  MRR: 0.0178\n","2021-12-08 15:11:25,869 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6511  @50, Recall: 0.1992  MRR: 0.0442\n","2021-12-08 15:11:28,827 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8012  @50, Recall: 0.1758  MRR: 0.0275\n","2021-12-08 15:11:31,807 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4719  @50, Recall: 0.2148  MRR: 0.0473\n","2021-12-08 15:11:34,768 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5348  @50, Recall: 0.2188  MRR: 0.0469\n","2021-12-08 15:11:37,785 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7239  @50, Recall: 0.1836  MRR: 0.0312\n","2021-12-08 15:11:40,779 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8394  @50, Recall: 0.1445  MRR: 0.0302\n","2021-12-08 15:11:43,885 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8302  @50, Recall: 0.1719  MRR: 0.0295\n","2021-12-08 15:11:46,931 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5726  @50, Recall: 0.1797  MRR: 0.0357\n","2021-12-08 15:11:49,942 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5937  @50, Recall: 0.2031  MRR: 0.0275\n","2021-12-08 15:11:53,034 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7079  @50, Recall: 0.1602  MRR: 0.0235\n","2021-12-08 15:11:56,112 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9424  @50, Recall: 0.1328  MRR: 0.0235\n","2021-12-08 15:11:59,126 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8373  @50, Recall: 0.1836  MRR: 0.0312\n","2021-12-08 15:12:02,154 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5243  @50, Recall: 0.1953  MRR: 0.0361\n","2021-12-08 15:12:05,174 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6319  @50, Recall: 0.1641  MRR: 0.0392\n","2021-12-08 15:12:08,175 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6541  @50, Recall: 0.1875  MRR: 0.0367\n","2021-12-08 15:12:11,216 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0450  @50, Recall: 0.1211  MRR: 0.0214\n","2021-12-08 15:12:14,227 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5855  @50, Recall: 0.1953  MRR: 0.0346\n","2021-12-08 15:12:17,302 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8087  @50, Recall: 0.1523  MRR: 0.0218\n","2021-12-08 15:12:20,322 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3153  @50, Recall: 0.2383  MRR: 0.0482\n","2021-12-08 15:12:23,348 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4305  @50, Recall: 0.1953  MRR: 0.0461\n","2021-12-08 15:12:26,380 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4175  @50, Recall: 0.2383  MRR: 0.0565\n","2021-12-08 15:12:29,354 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8271  @50, Recall: 0.1914  MRR: 0.0408\n","2021-12-08 15:12:32,330 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6912  @50, Recall: 0.1836  MRR: 0.0332\n","2021-12-08 15:12:35,313 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5773  @50, Recall: 0.1836  MRR: 0.0257\n","2021-12-08 15:12:38,285 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7666  @50, Recall: 0.1875  MRR: 0.0419\n","2021-12-08 15:12:41,241 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8686  @50, Recall: 0.1797  MRR: 0.0338\n","2021-12-08 15:12:44,183 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6044  @50, Recall: 0.1719  MRR: 0.0328\n","2021-12-08 15:12:47,161 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8645  @50, Recall: 0.1484  MRR: 0.0186\n","2021-12-08 15:12:50,100 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4701  @50, Recall: 0.1914  MRR: 0.0447\n","2021-12-08 15:12:53,033 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6803  @50, Recall: 0.1797  MRR: 0.0370\n","2021-12-08 15:12:55,982 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6443  @50, Recall: 0.2031  MRR: 0.0392\n","2021-12-08 15:12:58,926 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4371  @50, Recall: 0.1914  MRR: 0.0408\n","2021-12-08 15:13:01,845 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6471  @50, Recall: 0.1914  MRR: 0.0261\n","2021-12-08 15:13:04,847 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5697  @50, Recall: 0.2188  MRR: 0.0455\n","2021-12-08 15:13:07,899 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3997  @50, Recall: 0.2109  MRR: 0.0395\n","2021-12-08 15:13:10,883 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8339  @50, Recall: 0.1523  MRR: 0.0282\n","2021-12-08 15:13:13,845 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6825  @50, Recall: 0.2031  MRR: 0.0357\n","2021-12-08 15:13:16,814 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6508  @50, Recall: 0.1953  MRR: 0.0337\n","2021-12-08 15:13:19,779 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5967  @50, Recall: 0.1914  MRR: 0.0248\n","2021-12-08 15:13:22,828 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4458  @50, Recall: 0.2188  MRR: 0.0434\n","2021-12-08 15:13:25,795 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6978  @50, Recall: 0.1641  MRR: 0.0316\n","2021-12-08 15:13:28,743 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6428  @50, Recall: 0.1680  MRR: 0.0374\n","2021-12-08 15:13:31,667 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4158  @50, Recall: 0.2070  MRR: 0.0373\n","2021-12-08 15:13:34,605 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5393  @50, Recall: 0.1914  MRR: 0.0382\n","2021-12-08 15:13:37,552 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6710  @50, Recall: 0.1953  MRR: 0.0341\n","2021-12-08 15:13:40,464 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8080  @50, Recall: 0.1680  MRR: 0.0307\n","2021-12-08 15:13:43,376 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7186  @50, Recall: 0.1562  MRR: 0.0302\n","2021-12-08 15:13:46,318 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5994  @50, Recall: 0.1719  MRR: 0.0344\n","2021-12-08 15:13:49,284 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5499  @50, Recall: 0.2070  MRR: 0.0356\n","2021-12-08 15:13:52,248 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8660  @50, Recall: 0.1602  MRR: 0.0282\n","2021-12-08 15:13:55,305 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6974  @50, Recall: 0.1719  MRR: 0.0376\n","2021-12-08 15:13:58,302 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5569  @50, Recall: 0.1875  MRR: 0.0296\n","2021-12-08 15:14:01,318 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5025  @50, Recall: 0.1914  MRR: 0.0373\n","2021-12-08 15:14:04,302 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8638  @50, Recall: 0.1523  MRR: 0.0231\n","2021-12-08 15:14:07,281 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6044  @50, Recall: 0.1992  MRR: 0.0504\n","2021-12-08 15:14:10,257 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6367  @50, Recall: 0.1836  MRR: 0.0220\n","2021-12-08 15:14:13,215 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7290  @50, Recall: 0.1641  MRR: 0.0298\n","2021-12-08 15:14:16,182 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5738  @50, Recall: 0.2031  MRR: 0.0366\n","2021-12-08 15:14:19,173 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6787  @50, Recall: 0.1953  MRR: 0.0398\n","2021-12-08 15:14:22,151 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7185  @50, Recall: 0.1758  MRR: 0.0300\n","2021-12-08 15:14:25,139 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5533  @50, Recall: 0.2266  MRR: 0.0384\n","2021-12-08 15:14:28,176 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8915  @50, Recall: 0.1367  MRR: 0.0272\n","2021-12-08 15:14:31,166 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5429  @50, Recall: 0.2109  MRR: 0.0525\n","2021-12-08 15:14:34,149 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5958  @50, Recall: 0.1914  MRR: 0.0315\n","2021-12-08 15:14:37,135 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7261  @50, Recall: 0.1680  MRR: 0.0333\n","2021-12-08 15:14:40,095 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6022  @50, Recall: 0.1953  MRR: 0.0398\n","2021-12-08 15:14:43,036 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7934  @50, Recall: 0.1719  MRR: 0.0311\n","2021-12-08 15:14:46,006 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4483  @50, Recall: 0.1914  MRR: 0.0305\n","2021-12-08 15:14:48,948 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7936  @50, Recall: 0.1602  MRR: 0.0217\n","2021-12-08 15:14:51,887 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9081  @50, Recall: 0.1562  MRR: 0.0207\n","2021-12-08 15:14:54,797 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7305  @50, Recall: 0.1719  MRR: 0.0240\n","2021-12-08 15:14:57,734 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6476  @50, Recall: 0.1836  MRR: 0.0265\n","2021-12-08 15:15:00,728 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7316  @50, Recall: 0.1445  MRR: 0.0313\n","2021-12-08 15:15:03,720 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7463  @50, Recall: 0.1562  MRR: 0.0318\n","2021-12-08 15:15:06,655 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8978  @50, Recall: 0.1523  MRR: 0.0255\n","2021-12-08 15:15:09,575 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9355  @50, Recall: 0.1406  MRR: 0.0290\n","2021-12-08 15:15:12,488 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7532  @50, Recall: 0.1562  MRR: 0.0296\n","2021-12-08 15:15:15,400 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6456  @50, Recall: 0.1836  MRR: 0.0357\n","2021-12-08 15:15:18,294 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5845  @50, Recall: 0.1406  MRR: 0.0200\n","2021-12-08 15:15:21,265 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6830  @50, Recall: 0.1992  MRR: 0.0397\n","2021-12-08 15:15:24,202 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5785  @50, Recall: 0.1836  MRR: 0.0229\n","2021-12-08 15:15:27,100 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8233  @50, Recall: 0.1562  MRR: 0.0148\n","2021-12-08 15:15:29,989 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6321  @50, Recall: 0.1797  MRR: 0.0299\n","2021-12-08 15:15:32,909 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4225  @50, Recall: 0.1914  MRR: 0.0343\n","2021-12-08 15:15:35,818 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6439  @50, Recall: 0.1719  MRR: 0.0294\n","2021-12-08 15:15:38,754 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7584  @50, Recall: 0.1484  MRR: 0.0167\n","2021-12-08 15:15:41,683 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6352  @50, Recall: 0.1484  MRR: 0.0323\n","2021-12-08 15:15:44,631 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9281  @50, Recall: 0.1484  MRR: 0.0281\n","2021-12-08 15:15:47,575 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7989  @50, Recall: 0.1680  MRR: 0.0258\n","2021-12-08 15:15:50,509 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6442  @50, Recall: 0.1680  MRR: 0.0395\n","2021-12-08 15:15:53,463 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4152  @50, Recall: 0.2070  MRR: 0.0423\n","2021-12-08 15:15:56,410 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6624  @50, Recall: 0.1641  MRR: 0.0300\n","2021-12-08 15:15:59,373 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8267  @50, Recall: 0.1289  MRR: 0.0262\n","2021-12-08 15:16:02,338 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5805  @50, Recall: 0.1875  MRR: 0.0414\n","2021-12-08 15:16:05,319 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6002  @50, Recall: 0.1758  MRR: 0.0294\n","2021-12-08 15:16:08,349 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8934  @50, Recall: 0.1562  MRR: 0.0259\n","2021-12-08 15:16:11,299 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5984  @50, Recall: 0.1758  MRR: 0.0227\n","2021-12-08 15:16:14,283 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8550  @50, Recall: 0.1406  MRR: 0.0290\n","2021-12-08 15:16:17,242 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9085  @50, Recall: 0.1211  MRR: 0.0176\n","2021-12-08 15:16:20,225 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6904  @50, Recall: 0.1562  MRR: 0.0243\n","2021-12-08 15:16:23,167 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7457  @50, Recall: 0.1602  MRR: 0.0224\n","2021-12-08 15:16:26,125 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5063  @50, Recall: 0.1602  MRR: 0.0297\n","2021-12-08 15:16:29,086 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6118  @50, Recall: 0.1875  MRR: 0.0340\n","2021-12-08 15:16:32,011 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7535  @50, Recall: 0.1523  MRR: 0.0326\n","2021-12-08 15:16:34,964 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8986  @50, Recall: 0.1445  MRR: 0.0207\n","2021-12-08 15:16:37,897 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4275  @50, Recall: 0.1953  MRR: 0.0372\n","2021-12-08 15:16:40,886 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6958  @50, Recall: 0.1719  MRR: 0.0298\n","2021-12-08 15:16:43,770 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7527  @50, Recall: 0.1758  MRR: 0.0310\n","2021-12-08 15:16:46,698 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5588  @50, Recall: 0.1719  MRR: 0.0257\n","2021-12-08 15:16:49,626 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6172  @50, Recall: 0.1602  MRR: 0.0295\n","2021-12-08 15:16:52,536 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6832  @50, Recall: 0.1523  MRR: 0.0293\n","2021-12-08 15:16:54,330 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7363  @50, Recall: 0.1216  MRR: 0.0159\n","2021-12-08 15:16:54,337 src.recall.sr_gnn.train.trainer:INFO:Epoch: 4 Train Loss: 8.5322 Test Loss: 9.6754 Recall: 0.1760 MRR: 0.0322\n","2021-12-08 15:16:54,338 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1760,  0.0322  Epoch: 4,  4\n","Epoch:  25% 5/20 [39:49<1:59:39, 478.62s/it]2021-12-08 15:16:54,912 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 3540\n","2021-12-08 15:16:54,952 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.47245\n","2021-12-08 15:17:02,720 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.60581\n","2021-12-08 15:17:10,460 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.68691\n","2021-12-08 15:17:18,226 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 7.77083\n","2021-12-08 15:17:25,986 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 7.84588\n","2021-12-08 15:17:33,687 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 7.91563\n","2021-12-08 15:17:41,381 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 7.98238\n","2021-12-08 15:17:49,225 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.04387\n","2021-12-08 15:17:57,198 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.10184\n","2021-12-08 15:18:05,157 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.15625\n","2021-12-08 15:18:13,081 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.20599\n","2021-12-08 15:18:21,056 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.25183\n","2021-12-08 15:18:28,904 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.29572\n","2021-12-08 15:18:36,716 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.33538\n","2021-12-08 15:18:44,426 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.37257\n","2021-12-08 15:18:52,287 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.40791\n","2021-12-08 15:19:00,179 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.44133\n","2021-12-08 15:19:07,949 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.47258\n","2021-12-08 15:19:16,181 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5521  @50, Recall: 0.2031  MRR: 0.0394\n","2021-12-08 15:19:19,122 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7421  @50, Recall: 0.1914  MRR: 0.0295\n","2021-12-08 15:19:22,014 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0202  @50, Recall: 0.1211  MRR: 0.0204\n","2021-12-08 15:19:24,997 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6404  @50, Recall: 0.1875  MRR: 0.0432\n","2021-12-08 15:19:27,947 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7850  @50, Recall: 0.1758  MRR: 0.0275\n","2021-12-08 15:19:30,853 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5059  @50, Recall: 0.2148  MRR: 0.0429\n","2021-12-08 15:19:33,758 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5484  @50, Recall: 0.2070  MRR: 0.0424\n","2021-12-08 15:19:36,704 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6991  @50, Recall: 0.1836  MRR: 0.0316\n","2021-12-08 15:19:39,634 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8349  @50, Recall: 0.1406  MRR: 0.0292\n","2021-12-08 15:19:42,507 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8531  @50, Recall: 0.1680  MRR: 0.0309\n","2021-12-08 15:19:45,444 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5724  @50, Recall: 0.1875  MRR: 0.0325\n","2021-12-08 15:19:48,363 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6048  @50, Recall: 0.2031  MRR: 0.0306\n","2021-12-08 15:19:51,335 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7276  @50, Recall: 0.1719  MRR: 0.0243\n","2021-12-08 15:19:54,288 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9719  @50, Recall: 0.1445  MRR: 0.0252\n","2021-12-08 15:19:57,276 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8479  @50, Recall: 0.1562  MRR: 0.0345\n","2021-12-08 15:20:00,240 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4860  @50, Recall: 0.1914  MRR: 0.0372\n","2021-12-08 15:20:03,208 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6224  @50, Recall: 0.1719  MRR: 0.0408\n","2021-12-08 15:20:06,172 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7164  @50, Recall: 0.1797  MRR: 0.0339\n","2021-12-08 15:20:09,138 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0174  @50, Recall: 0.1328  MRR: 0.0217\n","2021-12-08 15:20:12,071 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6111  @50, Recall: 0.2031  MRR: 0.0317\n","2021-12-08 15:20:15,012 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8472  @50, Recall: 0.1562  MRR: 0.0199\n","2021-12-08 15:20:17,922 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3330  @50, Recall: 0.2383  MRR: 0.0532\n","2021-12-08 15:20:20,936 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4241  @50, Recall: 0.2109  MRR: 0.0530\n","2021-12-08 15:20:23,869 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4102  @50, Recall: 0.2305  MRR: 0.0555\n","2021-12-08 15:20:26,792 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8469  @50, Recall: 0.1797  MRR: 0.0396\n","2021-12-08 15:20:29,780 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7187  @50, Recall: 0.1758  MRR: 0.0328\n","2021-12-08 15:20:32,772 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6390  @50, Recall: 0.1797  MRR: 0.0214\n","2021-12-08 15:20:35,770 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7362  @50, Recall: 0.1836  MRR: 0.0389\n","2021-12-08 15:20:38,722 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8918  @50, Recall: 0.1680  MRR: 0.0347\n","2021-12-08 15:20:41,610 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6785  @50, Recall: 0.1680  MRR: 0.0348\n","2021-12-08 15:20:44,487 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8509  @50, Recall: 0.1602  MRR: 0.0204\n","2021-12-08 15:20:47,380 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4991  @50, Recall: 0.1758  MRR: 0.0414\n","2021-12-08 15:20:50,258 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6905  @50, Recall: 0.1875  MRR: 0.0375\n","2021-12-08 15:20:53,156 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6465  @50, Recall: 0.1992  MRR: 0.0431\n","2021-12-08 15:20:56,035 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4905  @50, Recall: 0.1914  MRR: 0.0386\n","2021-12-08 15:20:58,925 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5942  @50, Recall: 0.2148  MRR: 0.0353\n","2021-12-08 15:21:01,786 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5549  @50, Recall: 0.2188  MRR: 0.0445\n","2021-12-08 15:21:04,780 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4032  @50, Recall: 0.2227  MRR: 0.0406\n","2021-12-08 15:21:07,663 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8396  @50, Recall: 0.1484  MRR: 0.0324\n","2021-12-08 15:21:10,537 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6870  @50, Recall: 0.2266  MRR: 0.0375\n","2021-12-08 15:21:13,418 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6765  @50, Recall: 0.1875  MRR: 0.0301\n","2021-12-08 15:21:16,270 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5869  @50, Recall: 0.1836  MRR: 0.0248\n","2021-12-08 15:21:19,161 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4411  @50, Recall: 0.2188  MRR: 0.0472\n","2021-12-08 15:21:22,021 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6292  @50, Recall: 0.1641  MRR: 0.0329\n","2021-12-08 15:21:24,901 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6888  @50, Recall: 0.1680  MRR: 0.0341\n","2021-12-08 15:21:27,795 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3960  @50, Recall: 0.2188  MRR: 0.0382\n","2021-12-08 15:21:30,690 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5169  @50, Recall: 0.1875  MRR: 0.0407\n","2021-12-08 15:21:33,598 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6893  @50, Recall: 0.1953  MRR: 0.0308\n","2021-12-08 15:21:36,583 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8239  @50, Recall: 0.1602  MRR: 0.0301\n","2021-12-08 15:21:39,539 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7159  @50, Recall: 0.1523  MRR: 0.0271\n","2021-12-08 15:21:42,476 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5680  @50, Recall: 0.1797  MRR: 0.0349\n","2021-12-08 15:21:45,447 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5532  @50, Recall: 0.2109  MRR: 0.0377\n","2021-12-08 15:21:48,384 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8835  @50, Recall: 0.1523  MRR: 0.0250\n","2021-12-08 15:21:51,307 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7173  @50, Recall: 0.1680  MRR: 0.0370\n","2021-12-08 15:21:54,401 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5512  @50, Recall: 0.1797  MRR: 0.0277\n","2021-12-08 15:21:57,332 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5043  @50, Recall: 0.1836  MRR: 0.0390\n","2021-12-08 15:22:00,269 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8455  @50, Recall: 0.1484  MRR: 0.0214\n","2021-12-08 15:22:03,199 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5883  @50, Recall: 0.1992  MRR: 0.0476\n","2021-12-08 15:22:06,112 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6687  @50, Recall: 0.1641  MRR: 0.0221\n","2021-12-08 15:22:09,061 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7241  @50, Recall: 0.1758  MRR: 0.0314\n","2021-12-08 15:22:12,077 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5072  @50, Recall: 0.1992  MRR: 0.0382\n","2021-12-08 15:22:15,032 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6550  @50, Recall: 0.1992  MRR: 0.0460\n","2021-12-08 15:22:17,984 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6628  @50, Recall: 0.1797  MRR: 0.0325\n","2021-12-08 15:22:20,949 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5569  @50, Recall: 0.2109  MRR: 0.0363\n","2021-12-08 15:22:23,909 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9165  @50, Recall: 0.1250  MRR: 0.0262\n","2021-12-08 15:22:26,852 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5465  @50, Recall: 0.2305  MRR: 0.0492\n","2021-12-08 15:22:29,797 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6096  @50, Recall: 0.2109  MRR: 0.0344\n","2021-12-08 15:22:32,712 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7305  @50, Recall: 0.1641  MRR: 0.0314\n","2021-12-08 15:22:35,594 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5969  @50, Recall: 0.1953  MRR: 0.0421\n","2021-12-08 15:22:38,481 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8320  @50, Recall: 0.1641  MRR: 0.0272\n","2021-12-08 15:22:41,355 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4272  @50, Recall: 0.1953  MRR: 0.0349\n","2021-12-08 15:22:44,322 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7962  @50, Recall: 0.1836  MRR: 0.0223\n","2021-12-08 15:22:47,247 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9653  @50, Recall: 0.1406  MRR: 0.0187\n","2021-12-08 15:22:50,133 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7515  @50, Recall: 0.1562  MRR: 0.0202\n","2021-12-08 15:22:53,014 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6667  @50, Recall: 0.1836  MRR: 0.0272\n","2021-12-08 15:22:55,867 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6538  @50, Recall: 0.1758  MRR: 0.0329\n","2021-12-08 15:22:58,740 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7296  @50, Recall: 0.1602  MRR: 0.0273\n","2021-12-08 15:23:01,586 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9180  @50, Recall: 0.1602  MRR: 0.0281\n","2021-12-08 15:23:04,479 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9548  @50, Recall: 0.1445  MRR: 0.0266\n","2021-12-08 15:23:07,349 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8054  @50, Recall: 0.1484  MRR: 0.0258\n","2021-12-08 15:23:10,190 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6248  @50, Recall: 0.1836  MRR: 0.0352\n","2021-12-08 15:23:13,046 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5726  @50, Recall: 0.1523  MRR: 0.0221\n","2021-12-08 15:23:15,997 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6313  @50, Recall: 0.1953  MRR: 0.0383\n","2021-12-08 15:23:18,934 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6501  @50, Recall: 0.1758  MRR: 0.0232\n","2021-12-08 15:23:21,861 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8176  @50, Recall: 0.1602  MRR: 0.0158\n","2021-12-08 15:23:24,781 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6380  @50, Recall: 0.1797  MRR: 0.0281\n","2021-12-08 15:23:27,661 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4431  @50, Recall: 0.1953  MRR: 0.0424\n","2021-12-08 15:23:30,557 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6443  @50, Recall: 0.1680  MRR: 0.0284\n","2021-12-08 15:23:33,434 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7900  @50, Recall: 0.1445  MRR: 0.0155\n","2021-12-08 15:23:36,314 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5873  @50, Recall: 0.1484  MRR: 0.0309\n","2021-12-08 15:23:39,235 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9098  @50, Recall: 0.1562  MRR: 0.0295\n","2021-12-08 15:23:42,127 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8202  @50, Recall: 0.1562  MRR: 0.0285\n","2021-12-08 15:23:45,058 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6149  @50, Recall: 0.1953  MRR: 0.0359\n","2021-12-08 15:23:48,121 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3895  @50, Recall: 0.1992  MRR: 0.0423\n","2021-12-08 15:23:51,101 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6421  @50, Recall: 0.1719  MRR: 0.0300\n","2021-12-08 15:23:54,117 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8036  @50, Recall: 0.1484  MRR: 0.0263\n","2021-12-08 15:23:57,062 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5749  @50, Recall: 0.1797  MRR: 0.0407\n","2021-12-08 15:24:00,001 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5808  @50, Recall: 0.1914  MRR: 0.0270\n","2021-12-08 15:24:02,943 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8578  @50, Recall: 0.1680  MRR: 0.0229\n","2021-12-08 15:24:05,879 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5727  @50, Recall: 0.1758  MRR: 0.0224\n","2021-12-08 15:24:08,808 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8579  @50, Recall: 0.1289  MRR: 0.0255\n","2021-12-08 15:24:11,742 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9390  @50, Recall: 0.1328  MRR: 0.0192\n","2021-12-08 15:24:14,653 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7251  @50, Recall: 0.1484  MRR: 0.0253\n","2021-12-08 15:24:17,571 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7012  @50, Recall: 0.1719  MRR: 0.0237\n","2021-12-08 15:24:20,498 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5416  @50, Recall: 0.1602  MRR: 0.0325\n","2021-12-08 15:24:23,462 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6488  @50, Recall: 0.1914  MRR: 0.0320\n","2021-12-08 15:24:26,392 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7560  @50, Recall: 0.1562  MRR: 0.0276\n","2021-12-08 15:24:29,333 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8596  @50, Recall: 0.1367  MRR: 0.0212\n","2021-12-08 15:24:32,259 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4308  @50, Recall: 0.1797  MRR: 0.0425\n","2021-12-08 15:24:35,132 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7191  @50, Recall: 0.1758  MRR: 0.0272\n","2021-12-08 15:24:37,997 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7907  @50, Recall: 0.1680  MRR: 0.0286\n","2021-12-08 15:24:40,854 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5947  @50, Recall: 0.1680  MRR: 0.0257\n","2021-12-08 15:24:43,684 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5925  @50, Recall: 0.1797  MRR: 0.0345\n","2021-12-08 15:24:46,566 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6657  @50, Recall: 0.1562  MRR: 0.0264\n","2021-12-08 15:24:48,225 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7385  @50, Recall: 0.1014  MRR: 0.0210\n","2021-12-08 15:24:48,232 src.recall.sr_gnn.train.trainer:INFO:Epoch: 5 Train Loss: 8.4934 Test Loss: 9.6776 Recall: 0.1768 MRR: 0.0321\n","2021-12-08 15:24:48,232 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1768,  0.0322  Epoch: 5,  4\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","2021-12-08 15:24:48,536 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","Epoch:  30% 6/20 [47:43<1:51:18, 477.03s/it]2021-12-08 15:24:48,753 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 3540\n","2021-12-08 15:24:48,793 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.52108\n","2021-12-08 15:24:56,585 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.56757\n","2021-12-08 15:25:04,340 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.64958\n","2021-12-08 15:25:12,011 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 7.73423\n","2021-12-08 15:25:19,748 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 7.81181\n","2021-12-08 15:25:27,469 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 7.88309\n","2021-12-08 15:25:35,291 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 7.95033\n","2021-12-08 15:25:43,037 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.01232\n","2021-12-08 15:25:50,886 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.06923\n","2021-12-08 15:25:58,795 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.12378\n","2021-12-08 15:26:06,853 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.17421\n","2021-12-08 15:26:14,756 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.22077\n","2021-12-08 15:26:22,553 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.26324\n","2021-12-08 15:26:30,363 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.30429\n","2021-12-08 15:26:38,143 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.34210\n","2021-12-08 15:26:45,777 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.37848\n","2021-12-08 15:26:53,506 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.41112\n","2021-12-08 15:27:01,181 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.44278\n","2021-12-08 15:27:09,528 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5240  @50, Recall: 0.1992  MRR: 0.0414\n","2021-12-08 15:27:12,413 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8016  @50, Recall: 0.1836  MRR: 0.0284\n","2021-12-08 15:27:15,342 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0198  @50, Recall: 0.1328  MRR: 0.0175\n","2021-12-08 15:27:18,210 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7023  @50, Recall: 0.1875  MRR: 0.0410\n","2021-12-08 15:27:21,124 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7558  @50, Recall: 0.1719  MRR: 0.0276\n","2021-12-08 15:27:24,007 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5211  @50, Recall: 0.2305  MRR: 0.0437\n","2021-12-08 15:27:26,870 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5150  @50, Recall: 0.1992  MRR: 0.0451\n","2021-12-08 15:27:29,772 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7370  @50, Recall: 0.1875  MRR: 0.0328\n","2021-12-08 15:27:32,669 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8818  @50, Recall: 0.1523  MRR: 0.0297\n","2021-12-08 15:27:35,547 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8430  @50, Recall: 0.1602  MRR: 0.0302\n","2021-12-08 15:27:38,460 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5596  @50, Recall: 0.1992  MRR: 0.0344\n","2021-12-08 15:27:41,424 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5792  @50, Recall: 0.2109  MRR: 0.0285\n","2021-12-08 15:27:44,345 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7278  @50, Recall: 0.1992  MRR: 0.0254\n","2021-12-08 15:27:47,307 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9627  @50, Recall: 0.1406  MRR: 0.0230\n","2021-12-08 15:27:50,262 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9152  @50, Recall: 0.1680  MRR: 0.0248\n","2021-12-08 15:27:53,224 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5111  @50, Recall: 0.1953  MRR: 0.0389\n","2021-12-08 15:27:56,142 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6409  @50, Recall: 0.1758  MRR: 0.0385\n","2021-12-08 15:27:59,047 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7120  @50, Recall: 0.1875  MRR: 0.0362\n","2021-12-08 15:28:01,962 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9906  @50, Recall: 0.1250  MRR: 0.0230\n","2021-12-08 15:28:04,881 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5581  @50, Recall: 0.2070  MRR: 0.0350\n","2021-12-08 15:28:07,802 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8306  @50, Recall: 0.1641  MRR: 0.0200\n","2021-12-08 15:28:10,704 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3369  @50, Recall: 0.2305  MRR: 0.0519\n","2021-12-08 15:28:13,713 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4427  @50, Recall: 0.1875  MRR: 0.0463\n","2021-12-08 15:28:16,664 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3844  @50, Recall: 0.2188  MRR: 0.0548\n","2021-12-08 15:28:19,656 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7780  @50, Recall: 0.1836  MRR: 0.0451\n","2021-12-08 15:28:22,603 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7503  @50, Recall: 0.1875  MRR: 0.0299\n","2021-12-08 15:28:25,545 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6543  @50, Recall: 0.1836  MRR: 0.0246\n","2021-12-08 15:28:28,457 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7270  @50, Recall: 0.1797  MRR: 0.0348\n","2021-12-08 15:28:31,357 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9054  @50, Recall: 0.1523  MRR: 0.0319\n","2021-12-08 15:28:34,205 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6580  @50, Recall: 0.1562  MRR: 0.0340\n","2021-12-08 15:28:37,082 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8623  @50, Recall: 0.1562  MRR: 0.0198\n","2021-12-08 15:28:39,977 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4444  @50, Recall: 0.1836  MRR: 0.0465\n","2021-12-08 15:28:42,870 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7297  @50, Recall: 0.1797  MRR: 0.0381\n","2021-12-08 15:28:45,821 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6201  @50, Recall: 0.1875  MRR: 0.0459\n","2021-12-08 15:28:48,720 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4586  @50, Recall: 0.1992  MRR: 0.0371\n","2021-12-08 15:28:51,598 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6183  @50, Recall: 0.1758  MRR: 0.0283\n","2021-12-08 15:28:54,591 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5729  @50, Recall: 0.2188  MRR: 0.0450\n","2021-12-08 15:28:57,486 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3679  @50, Recall: 0.2031  MRR: 0.0355\n","2021-12-08 15:29:00,360 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8309  @50, Recall: 0.1797  MRR: 0.0304\n","2021-12-08 15:29:03,216 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6629  @50, Recall: 0.2070  MRR: 0.0363\n","2021-12-08 15:29:06,128 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6523  @50, Recall: 0.1953  MRR: 0.0314\n","2021-12-08 15:29:09,006 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6185  @50, Recall: 0.1680  MRR: 0.0241\n","2021-12-08 15:29:11,886 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4055  @50, Recall: 0.2227  MRR: 0.0508\n","2021-12-08 15:29:14,764 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6462  @50, Recall: 0.1680  MRR: 0.0338\n","2021-12-08 15:29:17,697 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6716  @50, Recall: 0.1602  MRR: 0.0343\n","2021-12-08 15:29:20,724 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3654  @50, Recall: 0.2070  MRR: 0.0382\n","2021-12-08 15:29:23,610 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4973  @50, Recall: 0.1875  MRR: 0.0388\n","2021-12-08 15:29:26,487 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7084  @50, Recall: 0.1797  MRR: 0.0316\n","2021-12-08 15:29:29,363 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8016  @50, Recall: 0.1562  MRR: 0.0351\n","2021-12-08 15:29:32,290 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7606  @50, Recall: 0.1445  MRR: 0.0277\n","2021-12-08 15:29:35,213 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5897  @50, Recall: 0.1797  MRR: 0.0341\n","2021-12-08 15:29:38,144 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5716  @50, Recall: 0.2188  MRR: 0.0359\n","2021-12-08 15:29:41,068 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8350  @50, Recall: 0.1719  MRR: 0.0262\n","2021-12-08 15:29:43,990 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6835  @50, Recall: 0.1719  MRR: 0.0386\n","2021-12-08 15:29:47,059 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5647  @50, Recall: 0.1875  MRR: 0.0282\n","2021-12-08 15:29:50,004 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4938  @50, Recall: 0.1914  MRR: 0.0385\n","2021-12-08 15:29:52,996 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8614  @50, Recall: 0.1406  MRR: 0.0213\n","2021-12-08 15:29:55,949 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5998  @50, Recall: 0.2070  MRR: 0.0483\n","2021-12-08 15:29:58,898 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6261  @50, Recall: 0.1641  MRR: 0.0249\n","2021-12-08 15:30:01,838 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7752  @50, Recall: 0.1719  MRR: 0.0296\n","2021-12-08 15:30:04,783 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5093  @50, Recall: 0.2266  MRR: 0.0336\n","2021-12-08 15:30:07,747 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6974  @50, Recall: 0.1953  MRR: 0.0416\n","2021-12-08 15:30:10,747 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6522  @50, Recall: 0.1719  MRR: 0.0301\n","2021-12-08 15:30:13,665 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5810  @50, Recall: 0.2148  MRR: 0.0380\n","2021-12-08 15:30:16,627 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8684  @50, Recall: 0.1406  MRR: 0.0298\n","2021-12-08 15:30:19,665 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5408  @50, Recall: 0.2070  MRR: 0.0511\n","2021-12-08 15:30:22,572 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5656  @50, Recall: 0.2031  MRR: 0.0347\n","2021-12-08 15:30:25,610 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7660  @50, Recall: 0.1758  MRR: 0.0291\n","2021-12-08 15:30:28,596 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5642  @50, Recall: 0.2070  MRR: 0.0397\n","2021-12-08 15:30:31,556 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8159  @50, Recall: 0.1602  MRR: 0.0330\n","2021-12-08 15:30:34,522 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4538  @50, Recall: 0.2109  MRR: 0.0320\n","2021-12-08 15:30:37,421 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7680  @50, Recall: 0.1836  MRR: 0.0223\n","2021-12-08 15:30:40,291 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9216  @50, Recall: 0.1289  MRR: 0.0209\n","2021-12-08 15:30:43,157 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7580  @50, Recall: 0.1484  MRR: 0.0207\n","2021-12-08 15:30:46,020 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6739  @50, Recall: 0.1719  MRR: 0.0237\n","2021-12-08 15:30:48,886 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7079  @50, Recall: 0.1562  MRR: 0.0333\n","2021-12-08 15:30:51,754 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7295  @50, Recall: 0.1641  MRR: 0.0306\n","2021-12-08 15:30:54,623 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9004  @50, Recall: 0.1523  MRR: 0.0257\n","2021-12-08 15:30:57,521 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9650  @50, Recall: 0.1523  MRR: 0.0259\n","2021-12-08 15:31:00,456 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8191  @50, Recall: 0.1406  MRR: 0.0224\n","2021-12-08 15:31:03,357 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6039  @50, Recall: 0.1836  MRR: 0.0398\n","2021-12-08 15:31:06,258 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5642  @50, Recall: 0.1680  MRR: 0.0203\n","2021-12-08 15:31:09,121 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6474  @50, Recall: 0.2148  MRR: 0.0363\n","2021-12-08 15:31:12,002 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6211  @50, Recall: 0.1797  MRR: 0.0224\n","2021-12-08 15:31:14,909 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8502  @50, Recall: 0.1328  MRR: 0.0176\n","2021-12-08 15:31:17,829 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5956  @50, Recall: 0.1719  MRR: 0.0301\n","2021-12-08 15:31:20,745 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4249  @50, Recall: 0.2109  MRR: 0.0346\n","2021-12-08 15:31:23,657 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6717  @50, Recall: 0.1758  MRR: 0.0304\n","2021-12-08 15:31:26,623 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7617  @50, Recall: 0.1406  MRR: 0.0161\n","2021-12-08 15:31:29,649 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6344  @50, Recall: 0.1562  MRR: 0.0319\n","2021-12-08 15:31:32,713 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9674  @50, Recall: 0.1484  MRR: 0.0282\n","2021-12-08 15:31:35,729 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8417  @50, Recall: 0.1523  MRR: 0.0282\n","2021-12-08 15:31:38,784 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6564  @50, Recall: 0.1914  MRR: 0.0374\n","2021-12-08 15:31:41,782 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3944  @50, Recall: 0.2109  MRR: 0.0436\n","2021-12-08 15:31:44,796 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6917  @50, Recall: 0.1445  MRR: 0.0281\n","2021-12-08 15:31:47,787 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7808  @50, Recall: 0.1367  MRR: 0.0209\n","2021-12-08 15:31:50,723 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5876  @50, Recall: 0.1797  MRR: 0.0403\n","2021-12-08 15:31:53,835 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5827  @50, Recall: 0.1797  MRR: 0.0297\n","2021-12-08 15:31:56,846 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8728  @50, Recall: 0.1523  MRR: 0.0209\n","2021-12-08 15:31:59,819 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5993  @50, Recall: 0.1641  MRR: 0.0179\n","2021-12-08 15:32:02,750 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8712  @50, Recall: 0.1445  MRR: 0.0274\n","2021-12-08 15:32:05,812 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8938  @50, Recall: 0.1367  MRR: 0.0213\n","2021-12-08 15:32:08,764 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7088  @50, Recall: 0.1523  MRR: 0.0291\n","2021-12-08 15:32:11,743 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7402  @50, Recall: 0.1562  MRR: 0.0206\n","2021-12-08 15:32:14,699 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5082  @50, Recall: 0.1680  MRR: 0.0319\n","2021-12-08 15:32:17,655 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6742  @50, Recall: 0.1836  MRR: 0.0313\n","2021-12-08 15:32:20,597 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7813  @50, Recall: 0.1367  MRR: 0.0278\n","2021-12-08 15:32:23,526 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8779  @50, Recall: 0.1406  MRR: 0.0202\n","2021-12-08 15:32:26,413 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3965  @50, Recall: 0.2031  MRR: 0.0421\n","2021-12-08 15:32:29,319 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7323  @50, Recall: 0.1758  MRR: 0.0310\n","2021-12-08 15:32:32,196 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8387  @50, Recall: 0.1758  MRR: 0.0274\n","2021-12-08 15:32:35,104 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5784  @50, Recall: 0.1758  MRR: 0.0243\n","2021-12-08 15:32:38,111 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6231  @50, Recall: 0.1562  MRR: 0.0281\n","2021-12-08 15:32:41,053 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6785  @50, Recall: 0.1484  MRR: 0.0275\n","2021-12-08 15:32:42,716 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7710  @50, Recall: 0.1081  MRR: 0.0209\n","2021-12-08 15:32:42,723 src.recall.sr_gnn.train.trainer:INFO:Epoch: 6 Train Loss: 8.4631 Test Loss: 9.6792 Recall: 0.1760 MRR: 0.0318\n","2021-12-08 15:32:42,723 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1768,  0.0322  Epoch: 5,  4\n","Epoch:  35% 7/20 [55:37<1:43:09, 476.08s/it]2021-12-08 15:32:42,894 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 3540\n","2021-12-08 15:32:42,935 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.66002\n","2021-12-08 15:32:50,714 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.53871\n","2021-12-08 15:32:58,499 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.62438\n","2021-12-08 15:33:06,280 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 7.70612\n","2021-12-08 15:33:14,167 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 7.78151\n","2021-12-08 15:33:22,025 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 7.85469\n","2021-12-08 15:33:29,888 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 7.92223\n","2021-12-08 15:33:37,907 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 7.98345\n","2021-12-08 15:33:45,975 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.04034\n","2021-12-08 15:33:53,984 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.09598\n","2021-12-08 15:34:01,934 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.14520\n","2021-12-08 15:34:09,845 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.19282\n","2021-12-08 15:34:17,801 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.23649\n","2021-12-08 15:34:25,625 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.27773\n","2021-12-08 15:34:33,371 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.31564\n","2021-12-08 15:34:41,125 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.35176\n","2021-12-08 15:34:48,928 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.38557\n","2021-12-08 15:34:56,719 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.41675\n","2021-12-08 15:35:05,123 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5564  @50, Recall: 0.1953  MRR: 0.0437\n","2021-12-08 15:35:08,069 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8244  @50, Recall: 0.1758  MRR: 0.0258\n","2021-12-08 15:35:10,970 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0079  @50, Recall: 0.1289  MRR: 0.0175\n","2021-12-08 15:35:13,880 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6617  @50, Recall: 0.1992  MRR: 0.0430\n","2021-12-08 15:35:16,800 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7710  @50, Recall: 0.1680  MRR: 0.0296\n","2021-12-08 15:35:19,825 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5386  @50, Recall: 0.2422  MRR: 0.0425\n","2021-12-08 15:35:22,809 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5389  @50, Recall: 0.2109  MRR: 0.0395\n","2021-12-08 15:35:25,823 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7137  @50, Recall: 0.1836  MRR: 0.0319\n","2021-12-08 15:35:28,812 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8550  @50, Recall: 0.1562  MRR: 0.0343\n","2021-12-08 15:35:31,790 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8479  @50, Recall: 0.1523  MRR: 0.0302\n","2021-12-08 15:35:34,799 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5396  @50, Recall: 0.1914  MRR: 0.0362\n","2021-12-08 15:35:37,777 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6041  @50, Recall: 0.1992  MRR: 0.0244\n","2021-12-08 15:35:40,828 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7147  @50, Recall: 0.1797  MRR: 0.0255\n","2021-12-08 15:35:43,845 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9427  @50, Recall: 0.1172  MRR: 0.0231\n","2021-12-08 15:35:46,857 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9160  @50, Recall: 0.1641  MRR: 0.0305\n","2021-12-08 15:35:49,852 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4920  @50, Recall: 0.2188  MRR: 0.0349\n","2021-12-08 15:35:52,822 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6363  @50, Recall: 0.1641  MRR: 0.0372\n","2021-12-08 15:35:55,838 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7207  @50, Recall: 0.1992  MRR: 0.0353\n","2021-12-08 15:35:58,864 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0232  @50, Recall: 0.1250  MRR: 0.0226\n","2021-12-08 15:36:01,852 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6043  @50, Recall: 0.1992  MRR: 0.0360\n","2021-12-08 15:36:04,843 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8063  @50, Recall: 0.1562  MRR: 0.0193\n","2021-12-08 15:36:07,778 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3427  @50, Recall: 0.2422  MRR: 0.0489\n","2021-12-08 15:36:10,698 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4373  @50, Recall: 0.2070  MRR: 0.0483\n","2021-12-08 15:36:13,624 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4266  @50, Recall: 0.2266  MRR: 0.0538\n","2021-12-08 15:36:16,584 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8297  @50, Recall: 0.1758  MRR: 0.0453\n","2021-12-08 15:36:19,525 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7208  @50, Recall: 0.1914  MRR: 0.0316\n","2021-12-08 15:36:22,413 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6289  @50, Recall: 0.1953  MRR: 0.0252\n","2021-12-08 15:36:25,309 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7734  @50, Recall: 0.1875  MRR: 0.0390\n","2021-12-08 15:36:28,313 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8870  @50, Recall: 0.1641  MRR: 0.0316\n","2021-12-08 15:36:31,235 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6924  @50, Recall: 0.1602  MRR: 0.0335\n","2021-12-08 15:36:34,187 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8602  @50, Recall: 0.1367  MRR: 0.0207\n","2021-12-08 15:36:37,115 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4739  @50, Recall: 0.1875  MRR: 0.0464\n","2021-12-08 15:36:40,094 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6784  @50, Recall: 0.1953  MRR: 0.0395\n","2021-12-08 15:36:43,010 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6553  @50, Recall: 0.1914  MRR: 0.0444\n","2021-12-08 15:36:45,970 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4268  @50, Recall: 0.2070  MRR: 0.0382\n","2021-12-08 15:36:48,935 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6155  @50, Recall: 0.1680  MRR: 0.0299\n","2021-12-08 15:36:51,972 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5853  @50, Recall: 0.2188  MRR: 0.0459\n","2021-12-08 15:36:55,071 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3923  @50, Recall: 0.2109  MRR: 0.0362\n","2021-12-08 15:36:58,002 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8230  @50, Recall: 0.1680  MRR: 0.0300\n","2021-12-08 15:37:00,957 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7022  @50, Recall: 0.1992  MRR: 0.0367\n","2021-12-08 15:37:04,002 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6918  @50, Recall: 0.1836  MRR: 0.0282\n","2021-12-08 15:37:06,999 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5620  @50, Recall: 0.1992  MRR: 0.0265\n","2021-12-08 15:37:09,994 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4077  @50, Recall: 0.2383  MRR: 0.0487\n","2021-12-08 15:37:12,906 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6877  @50, Recall: 0.1641  MRR: 0.0342\n","2021-12-08 15:37:15,828 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6374  @50, Recall: 0.1758  MRR: 0.0368\n","2021-12-08 15:37:18,756 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3958  @50, Recall: 0.2227  MRR: 0.0368\n","2021-12-08 15:37:21,630 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5374  @50, Recall: 0.1875  MRR: 0.0353\n","2021-12-08 15:37:24,549 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7119  @50, Recall: 0.1797  MRR: 0.0294\n","2021-12-08 15:37:27,441 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8130  @50, Recall: 0.1758  MRR: 0.0310\n","2021-12-08 15:37:30,348 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7348  @50, Recall: 0.1719  MRR: 0.0298\n","2021-12-08 15:37:33,290 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5852  @50, Recall: 0.1875  MRR: 0.0357\n","2021-12-08 15:37:36,321 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5615  @50, Recall: 0.2109  MRR: 0.0371\n","2021-12-08 15:37:39,400 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8660  @50, Recall: 0.1523  MRR: 0.0304\n","2021-12-08 15:37:42,403 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7523  @50, Recall: 0.1602  MRR: 0.0342\n","2021-12-08 15:37:45,429 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5690  @50, Recall: 0.1758  MRR: 0.0278\n","2021-12-08 15:37:48,457 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5147  @50, Recall: 0.1797  MRR: 0.0385\n","2021-12-08 15:37:51,433 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8822  @50, Recall: 0.1562  MRR: 0.0198\n","2021-12-08 15:37:54,402 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5895  @50, Recall: 0.2070  MRR: 0.0481\n","2021-12-08 15:37:57,369 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6602  @50, Recall: 0.1797  MRR: 0.0275\n","2021-12-08 15:38:00,395 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7296  @50, Recall: 0.1680  MRR: 0.0280\n","2021-12-08 15:38:03,422 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4885  @50, Recall: 0.2031  MRR: 0.0352\n","2021-12-08 15:38:06,424 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6946  @50, Recall: 0.2070  MRR: 0.0412\n","2021-12-08 15:38:09,481 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7052  @50, Recall: 0.1875  MRR: 0.0284\n","2021-12-08 15:38:12,492 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5760  @50, Recall: 0.2227  MRR: 0.0374\n","2021-12-08 15:38:15,486 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8690  @50, Recall: 0.1328  MRR: 0.0311\n","2021-12-08 15:38:18,521 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5633  @50, Recall: 0.2148  MRR: 0.0510\n","2021-12-08 15:38:21,502 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5560  @50, Recall: 0.2109  MRR: 0.0353\n","2021-12-08 15:38:24,472 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7680  @50, Recall: 0.1680  MRR: 0.0334\n","2021-12-08 15:38:27,417 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5789  @50, Recall: 0.1953  MRR: 0.0390\n","2021-12-08 15:38:30,391 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8330  @50, Recall: 0.1602  MRR: 0.0317\n","2021-12-08 15:38:33,369 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4689  @50, Recall: 0.1914  MRR: 0.0306\n","2021-12-08 15:38:36,317 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7057  @50, Recall: 0.1875  MRR: 0.0216\n","2021-12-08 15:38:39,291 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9342  @50, Recall: 0.1289  MRR: 0.0208\n","2021-12-08 15:38:42,300 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7150  @50, Recall: 0.1641  MRR: 0.0257\n","2021-12-08 15:38:45,287 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6777  @50, Recall: 0.1758  MRR: 0.0244\n","2021-12-08 15:38:48,289 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6967  @50, Recall: 0.1562  MRR: 0.0310\n","2021-12-08 15:38:51,239 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7309  @50, Recall: 0.1562  MRR: 0.0312\n","2021-12-08 15:38:54,178 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8776  @50, Recall: 0.1523  MRR: 0.0291\n","2021-12-08 15:38:57,127 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9663  @50, Recall: 0.1367  MRR: 0.0253\n","2021-12-08 15:39:00,057 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8014  @50, Recall: 0.1523  MRR: 0.0267\n","2021-12-08 15:39:03,023 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5947  @50, Recall: 0.1836  MRR: 0.0365\n","2021-12-08 15:39:05,932 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5270  @50, Recall: 0.1602  MRR: 0.0208\n","2021-12-08 15:39:08,808 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6263  @50, Recall: 0.2109  MRR: 0.0363\n","2021-12-08 15:39:11,678 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6432  @50, Recall: 0.1914  MRR: 0.0250\n","2021-12-08 15:39:14,701 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8502  @50, Recall: 0.1562  MRR: 0.0161\n","2021-12-08 15:39:17,632 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6300  @50, Recall: 0.1758  MRR: 0.0310\n","2021-12-08 15:39:20,584 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3887  @50, Recall: 0.2031  MRR: 0.0403\n","2021-12-08 15:39:23,476 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6155  @50, Recall: 0.1836  MRR: 0.0306\n","2021-12-08 15:39:26,416 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7777  @50, Recall: 0.1445  MRR: 0.0165\n","2021-12-08 15:39:29,376 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6145  @50, Recall: 0.1562  MRR: 0.0320\n","2021-12-08 15:39:32,321 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9241  @50, Recall: 0.1289  MRR: 0.0264\n","2021-12-08 15:39:35,283 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8365  @50, Recall: 0.1484  MRR: 0.0256\n","2021-12-08 15:39:38,223 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6051  @50, Recall: 0.1836  MRR: 0.0380\n","2021-12-08 15:39:41,185 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4082  @50, Recall: 0.2070  MRR: 0.0443\n","2021-12-08 15:39:44,130 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6810  @50, Recall: 0.1641  MRR: 0.0274\n","2021-12-08 15:39:47,180 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7622  @50, Recall: 0.1602  MRR: 0.0257\n","2021-12-08 15:39:50,161 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5609  @50, Recall: 0.1914  MRR: 0.0408\n","2021-12-08 15:39:53,176 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5787  @50, Recall: 0.1875  MRR: 0.0314\n","2021-12-08 15:39:56,158 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8328  @50, Recall: 0.1641  MRR: 0.0219\n","2021-12-08 15:39:59,178 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5973  @50, Recall: 0.1836  MRR: 0.0211\n","2021-12-08 15:40:02,175 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8866  @50, Recall: 0.1367  MRR: 0.0263\n","2021-12-08 15:40:05,148 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9038  @50, Recall: 0.1328  MRR: 0.0202\n","2021-12-08 15:40:08,100 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7477  @50, Recall: 0.1406  MRR: 0.0225\n","2021-12-08 15:40:11,055 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7149  @50, Recall: 0.1797  MRR: 0.0197\n","2021-12-08 15:40:14,029 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4976  @50, Recall: 0.1484  MRR: 0.0291\n","2021-12-08 15:40:16,995 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6719  @50, Recall: 0.1797  MRR: 0.0319\n","2021-12-08 15:40:20,013 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7946  @50, Recall: 0.1484  MRR: 0.0311\n","2021-12-08 15:40:23,015 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9062  @50, Recall: 0.1484  MRR: 0.0210\n","2021-12-08 15:40:25,927 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4440  @50, Recall: 0.1719  MRR: 0.0471\n","2021-12-08 15:40:28,819 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7071  @50, Recall: 0.1797  MRR: 0.0300\n","2021-12-08 15:40:31,676 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7705  @50, Recall: 0.1875  MRR: 0.0296\n","2021-12-08 15:40:34,595 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5698  @50, Recall: 0.1797  MRR: 0.0218\n","2021-12-08 15:40:37,481 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6135  @50, Recall: 0.1797  MRR: 0.0270\n","2021-12-08 15:40:40,373 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6701  @50, Recall: 0.1680  MRR: 0.0316\n","2021-12-08 15:40:42,048 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7249  @50, Recall: 0.1284  MRR: 0.0258\n","2021-12-08 15:40:42,055 src.recall.sr_gnn.train.trainer:INFO:Epoch: 7 Train Loss: 8.4378 Test Loss: 9.6786 Recall: 0.1779 MRR: 0.0321\n","2021-12-08 15:40:42,055 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1779,  0.0322  Epoch: 7,  4\n","Epoch:  40% 8/20 [1:03:37<1:35:26, 477.22s/it]2021-12-08 15:40:42,528 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 3540\n","2021-12-08 15:40:42,569 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.50452\n","2021-12-08 15:40:50,260 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.51353\n","2021-12-08 15:40:58,054 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.59535\n","2021-12-08 15:41:05,754 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 7.67498\n","2021-12-08 15:41:13,426 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 7.75476\n","2021-12-08 15:41:21,078 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 7.82487\n","2021-12-08 15:41:28,884 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 7.89212\n","2021-12-08 15:41:36,800 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 7.95416\n","2021-12-08 15:41:44,749 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.01232\n","2021-12-08 15:41:52,689 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.06655\n","2021-12-08 15:42:00,709 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.11730\n","2021-12-08 15:42:08,597 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.16528\n","2021-12-08 15:42:16,348 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.20977\n","2021-12-08 15:42:24,105 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.25246\n","2021-12-08 15:42:31,758 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.29120\n","2021-12-08 15:42:39,459 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.32790\n","2021-12-08 15:42:47,106 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.36173\n","2021-12-08 15:42:54,674 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.39377\n","2021-12-08 15:43:02,841 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5258  @50, Recall: 0.2031  MRR: 0.0448\n","2021-12-08 15:43:05,869 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8337  @50, Recall: 0.1797  MRR: 0.0317\n","2021-12-08 15:43:08,798 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0153  @50, Recall: 0.1250  MRR: 0.0149\n","2021-12-08 15:43:11,702 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6927  @50, Recall: 0.2070  MRR: 0.0434\n","2021-12-08 15:43:14,667 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8036  @50, Recall: 0.1680  MRR: 0.0258\n","2021-12-08 15:43:17,618 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5278  @50, Recall: 0.2305  MRR: 0.0426\n","2021-12-08 15:43:20,562 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5519  @50, Recall: 0.2109  MRR: 0.0430\n","2021-12-08 15:43:23,547 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7289  @50, Recall: 0.1797  MRR: 0.0312\n","2021-12-08 15:43:26,552 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8808  @50, Recall: 0.1523  MRR: 0.0330\n","2021-12-08 15:43:29,556 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8583  @50, Recall: 0.1641  MRR: 0.0321\n","2021-12-08 15:43:32,535 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5696  @50, Recall: 0.1836  MRR: 0.0367\n","2021-12-08 15:43:35,480 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5907  @50, Recall: 0.1914  MRR: 0.0300\n","2021-12-08 15:43:38,503 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7127  @50, Recall: 0.1719  MRR: 0.0245\n","2021-12-08 15:43:41,504 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9429  @50, Recall: 0.1328  MRR: 0.0217\n","2021-12-08 15:43:44,492 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9036  @50, Recall: 0.1602  MRR: 0.0290\n","2021-12-08 15:43:47,428 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5591  @50, Recall: 0.1992  MRR: 0.0359\n","2021-12-08 15:43:50,366 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6271  @50, Recall: 0.1641  MRR: 0.0383\n","2021-12-08 15:43:53,352 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7319  @50, Recall: 0.1914  MRR: 0.0329\n","2021-12-08 15:43:56,264 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0215  @50, Recall: 0.1133  MRR: 0.0193\n","2021-12-08 15:43:59,256 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5707  @50, Recall: 0.2031  MRR: 0.0303\n","2021-12-08 15:44:02,181 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8124  @50, Recall: 0.1641  MRR: 0.0243\n","2021-12-08 15:44:05,138 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3188  @50, Recall: 0.2422  MRR: 0.0501\n","2021-12-08 15:44:08,084 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4092  @50, Recall: 0.1992  MRR: 0.0489\n","2021-12-08 15:44:11,073 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3885  @50, Recall: 0.2227  MRR: 0.0529\n","2021-12-08 15:44:14,110 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7822  @50, Recall: 0.1797  MRR: 0.0440\n","2021-12-08 15:44:17,046 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7391  @50, Recall: 0.1797  MRR: 0.0346\n","2021-12-08 15:44:20,056 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6354  @50, Recall: 0.1836  MRR: 0.0250\n","2021-12-08 15:44:22,959 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7502  @50, Recall: 0.1797  MRR: 0.0422\n","2021-12-08 15:44:25,877 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9078  @50, Recall: 0.1758  MRR: 0.0334\n","2021-12-08 15:44:28,790 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6917  @50, Recall: 0.1719  MRR: 0.0329\n","2021-12-08 15:44:31,668 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8642  @50, Recall: 0.1406  MRR: 0.0206\n","2021-12-08 15:44:34,572 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5104  @50, Recall: 0.1914  MRR: 0.0423\n","2021-12-08 15:44:37,503 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7429  @50, Recall: 0.1797  MRR: 0.0396\n","2021-12-08 15:44:40,398 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6242  @50, Recall: 0.1914  MRR: 0.0444\n","2021-12-08 15:44:43,338 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4433  @50, Recall: 0.2070  MRR: 0.0421\n","2021-12-08 15:44:46,273 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6511  @50, Recall: 0.1836  MRR: 0.0342\n","2021-12-08 15:44:49,175 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5471  @50, Recall: 0.2148  MRR: 0.0455\n","2021-12-08 15:44:52,077 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3501  @50, Recall: 0.2070  MRR: 0.0373\n","2021-12-08 15:44:54,972 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8402  @50, Recall: 0.1562  MRR: 0.0311\n","2021-12-08 15:44:57,853 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7170  @50, Recall: 0.1992  MRR: 0.0356\n","2021-12-08 15:45:00,798 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7086  @50, Recall: 0.1758  MRR: 0.0316\n","2021-12-08 15:45:03,757 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5790  @50, Recall: 0.1875  MRR: 0.0264\n","2021-12-08 15:45:06,727 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5053  @50, Recall: 0.2188  MRR: 0.0454\n","2021-12-08 15:45:09,647 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7164  @50, Recall: 0.1641  MRR: 0.0301\n","2021-12-08 15:45:12,552 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6541  @50, Recall: 0.1758  MRR: 0.0331\n","2021-12-08 15:45:15,536 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4583  @50, Recall: 0.2070  MRR: 0.0371\n","2021-12-08 15:45:18,607 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5337  @50, Recall: 0.1914  MRR: 0.0381\n","2021-12-08 15:45:21,638 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7149  @50, Recall: 0.1875  MRR: 0.0319\n","2021-12-08 15:45:24,655 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7867  @50, Recall: 0.1797  MRR: 0.0347\n","2021-12-08 15:45:27,635 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7723  @50, Recall: 0.1719  MRR: 0.0252\n","2021-12-08 15:45:30,611 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5866  @50, Recall: 0.1680  MRR: 0.0370\n","2021-12-08 15:45:33,534 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5468  @50, Recall: 0.2148  MRR: 0.0365\n","2021-12-08 15:45:36,447 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8010  @50, Recall: 0.1602  MRR: 0.0307\n","2021-12-08 15:45:39,385 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7479  @50, Recall: 0.1719  MRR: 0.0356\n","2021-12-08 15:45:42,303 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5586  @50, Recall: 0.1758  MRR: 0.0264\n","2021-12-08 15:45:45,272 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5239  @50, Recall: 0.1719  MRR: 0.0383\n","2021-12-08 15:45:48,267 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8822  @50, Recall: 0.1484  MRR: 0.0232\n","2021-12-08 15:45:51,309 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5787  @50, Recall: 0.2070  MRR: 0.0476\n","2021-12-08 15:45:54,305 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6662  @50, Recall: 0.1758  MRR: 0.0252\n","2021-12-08 15:45:57,337 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7200  @50, Recall: 0.1836  MRR: 0.0290\n","2021-12-08 15:46:00,357 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4970  @50, Recall: 0.2227  MRR: 0.0394\n","2021-12-08 15:46:03,297 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6646  @50, Recall: 0.2070  MRR: 0.0438\n","2021-12-08 15:46:06,236 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6924  @50, Recall: 0.1758  MRR: 0.0290\n","2021-12-08 15:46:09,164 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5357  @50, Recall: 0.2266  MRR: 0.0373\n","2021-12-08 15:46:12,118 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9030  @50, Recall: 0.1445  MRR: 0.0265\n","2021-12-08 15:46:15,055 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5461  @50, Recall: 0.2422  MRR: 0.0541\n","2021-12-08 15:46:17,964 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5899  @50, Recall: 0.1992  MRR: 0.0327\n","2021-12-08 15:46:20,883 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7619  @50, Recall: 0.1641  MRR: 0.0310\n","2021-12-08 15:46:23,878 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6178  @50, Recall: 0.2148  MRR: 0.0406\n","2021-12-08 15:46:26,844 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8282  @50, Recall: 0.1484  MRR: 0.0316\n","2021-12-08 15:46:29,782 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5127  @50, Recall: 0.1758  MRR: 0.0340\n","2021-12-08 15:46:32,698 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7590  @50, Recall: 0.1641  MRR: 0.0215\n","2021-12-08 15:46:35,608 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9388  @50, Recall: 0.1289  MRR: 0.0219\n","2021-12-08 15:46:38,495 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7709  @50, Recall: 0.1484  MRR: 0.0256\n","2021-12-08 15:46:41,402 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7144  @50, Recall: 0.1562  MRR: 0.0238\n","2021-12-08 15:46:44,267 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7503  @50, Recall: 0.1406  MRR: 0.0265\n","2021-12-08 15:46:47,188 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7553  @50, Recall: 0.1562  MRR: 0.0305\n","2021-12-08 15:46:50,130 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8904  @50, Recall: 0.1602  MRR: 0.0276\n","2021-12-08 15:46:53,124 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9508  @50, Recall: 0.1406  MRR: 0.0285\n","2021-12-08 15:46:56,116 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8179  @50, Recall: 0.1406  MRR: 0.0270\n","2021-12-08 15:46:59,029 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5919  @50, Recall: 0.1758  MRR: 0.0356\n","2021-12-08 15:47:01,979 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5442  @50, Recall: 0.1641  MRR: 0.0233\n","2021-12-08 15:47:04,908 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6316  @50, Recall: 0.2109  MRR: 0.0386\n","2021-12-08 15:47:07,829 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6161  @50, Recall: 0.1797  MRR: 0.0280\n","2021-12-08 15:47:10,789 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8500  @50, Recall: 0.1406  MRR: 0.0186\n","2021-12-08 15:47:13,719 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6153  @50, Recall: 0.1758  MRR: 0.0291\n","2021-12-08 15:47:16,687 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5022  @50, Recall: 0.1836  MRR: 0.0360\n","2021-12-08 15:47:19,664 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6326  @50, Recall: 0.1602  MRR: 0.0294\n","2021-12-08 15:47:22,610 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7934  @50, Recall: 0.1523  MRR: 0.0170\n","2021-12-08 15:47:25,583 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6316  @50, Recall: 0.1797  MRR: 0.0329\n","2021-12-08 15:47:28,597 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9805  @50, Recall: 0.1367  MRR: 0.0257\n","2021-12-08 15:47:31,605 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8391  @50, Recall: 0.1602  MRR: 0.0293\n","2021-12-08 15:47:34,562 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5941  @50, Recall: 0.1953  MRR: 0.0343\n","2021-12-08 15:47:37,514 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3595  @50, Recall: 0.2109  MRR: 0.0454\n","2021-12-08 15:47:40,486 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6291  @50, Recall: 0.1719  MRR: 0.0309\n","2021-12-08 15:47:43,451 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8050  @50, Recall: 0.1211  MRR: 0.0219\n","2021-12-08 15:47:46,408 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6235  @50, Recall: 0.1875  MRR: 0.0399\n","2021-12-08 15:47:49,381 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6014  @50, Recall: 0.1953  MRR: 0.0290\n","2021-12-08 15:47:52,325 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8416  @50, Recall: 0.1602  MRR: 0.0217\n","2021-12-08 15:47:55,274 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7045  @50, Recall: 0.1562  MRR: 0.0173\n","2021-12-08 15:47:58,206 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8649  @50, Recall: 0.1328  MRR: 0.0273\n","2021-12-08 15:48:01,163 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8960  @50, Recall: 0.1172  MRR: 0.0188\n","2021-12-08 15:48:04,209 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7428  @50, Recall: 0.1562  MRR: 0.0271\n","2021-12-08 15:48:07,184 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7220  @50, Recall: 0.1641  MRR: 0.0225\n","2021-12-08 15:48:10,153 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5517  @50, Recall: 0.1641  MRR: 0.0331\n","2021-12-08 15:48:13,071 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6808  @50, Recall: 0.1797  MRR: 0.0351\n","2021-12-08 15:48:15,968 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7883  @50, Recall: 0.1406  MRR: 0.0295\n","2021-12-08 15:48:18,887 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8716  @50, Recall: 0.1445  MRR: 0.0238\n","2021-12-08 15:48:21,793 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4402  @50, Recall: 0.1914  MRR: 0.0412\n","2021-12-08 15:48:24,688 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7452  @50, Recall: 0.1680  MRR: 0.0267\n","2021-12-08 15:48:27,597 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8092  @50, Recall: 0.1602  MRR: 0.0277\n","2021-12-08 15:48:30,485 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5824  @50, Recall: 0.1719  MRR: 0.0227\n","2021-12-08 15:48:33,416 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6349  @50, Recall: 0.1797  MRR: 0.0301\n","2021-12-08 15:48:36,387 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6672  @50, Recall: 0.1680  MRR: 0.0285\n","2021-12-08 15:48:38,113 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7055  @50, Recall: 0.1284  MRR: 0.0202\n","2021-12-08 15:48:38,121 src.recall.sr_gnn.train.trainer:INFO:Epoch: 8 Train Loss: 8.4147 Test Loss: 9.6870 Recall: 0.1756 MRR: 0.0322\n","2021-12-08 15:48:38,121 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1779,  0.0322  Epoch: 7,  4\n","Epoch:  45% 9/20 [1:11:33<1:27:24, 476.75s/it]2021-12-08 15:48:38,259 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 3540\n","2021-12-08 15:48:38,301 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.32013\n","2021-12-08 15:48:46,012 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.49103\n","2021-12-08 15:48:53,668 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.57082\n","2021-12-08 15:49:01,374 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 7.65742\n","2021-12-08 15:49:09,121 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 7.73881\n","2021-12-08 15:49:17,033 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 7.80922\n","2021-12-08 15:49:25,005 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 7.87668\n","2021-12-08 15:49:32,936 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 7.93782\n","2021-12-08 15:49:40,806 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 7.99601\n","2021-12-08 15:49:48,679 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.05051\n","2021-12-08 15:49:56,469 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.10006\n","2021-12-08 15:50:04,324 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.14764\n","2021-12-08 15:50:11,978 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.19176\n","2021-12-08 15:50:19,784 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.23210\n","2021-12-08 15:50:27,460 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.27130\n","2021-12-08 15:50:35,095 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.30731\n","2021-12-08 15:50:42,785 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.34092\n","2021-12-08 15:50:50,515 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.37290\n","2021-12-08 15:50:58,721 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5274  @50, Recall: 0.2031  MRR: 0.0473\n","2021-12-08 15:51:01,609 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8438  @50, Recall: 0.1797  MRR: 0.0302\n","2021-12-08 15:51:04,494 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0507  @50, Recall: 0.1328  MRR: 0.0145\n","2021-12-08 15:51:07,429 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6850  @50, Recall: 0.2070  MRR: 0.0421\n","2021-12-08 15:51:10,433 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7627  @50, Recall: 0.1602  MRR: 0.0260\n","2021-12-08 15:51:13,421 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5477  @50, Recall: 0.2188  MRR: 0.0419\n","2021-12-08 15:51:16,416 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5570  @50, Recall: 0.1992  MRR: 0.0444\n","2021-12-08 15:51:19,442 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7742  @50, Recall: 0.1836  MRR: 0.0300\n","2021-12-08 15:51:22,441 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8523  @50, Recall: 0.1523  MRR: 0.0325\n","2021-12-08 15:51:25,434 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8823  @50, Recall: 0.1719  MRR: 0.0294\n","2021-12-08 15:51:28,424 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5502  @50, Recall: 0.1914  MRR: 0.0319\n","2021-12-08 15:51:31,379 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6265  @50, Recall: 0.1758  MRR: 0.0271\n","2021-12-08 15:51:34,377 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7258  @50, Recall: 0.1758  MRR: 0.0254\n","2021-12-08 15:51:37,358 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9591  @50, Recall: 0.1250  MRR: 0.0209\n","2021-12-08 15:51:40,343 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9498  @50, Recall: 0.1680  MRR: 0.0305\n","2021-12-08 15:51:43,362 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5396  @50, Recall: 0.1953  MRR: 0.0348\n","2021-12-08 15:51:46,363 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6356  @50, Recall: 0.1641  MRR: 0.0393\n","2021-12-08 15:51:49,338 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7451  @50, Recall: 0.1758  MRR: 0.0335\n","2021-12-08 15:51:52,335 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0084  @50, Recall: 0.1055  MRR: 0.0225\n","2021-12-08 15:51:55,491 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6162  @50, Recall: 0.2148  MRR: 0.0296\n","2021-12-08 15:51:58,535 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8190  @50, Recall: 0.1602  MRR: 0.0224\n","2021-12-08 15:52:01,544 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2829  @50, Recall: 0.2266  MRR: 0.0606\n","2021-12-08 15:52:04,532 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3942  @50, Recall: 0.2070  MRR: 0.0505\n","2021-12-08 15:52:07,471 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3874  @50, Recall: 0.2070  MRR: 0.0578\n","2021-12-08 15:52:10,367 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7609  @50, Recall: 0.1914  MRR: 0.0455\n","2021-12-08 15:52:13,271 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7336  @50, Recall: 0.1992  MRR: 0.0302\n","2021-12-08 15:52:16,182 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6517  @50, Recall: 0.1680  MRR: 0.0245\n","2021-12-08 15:52:19,127 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7660  @50, Recall: 0.1836  MRR: 0.0413\n","2021-12-08 15:52:22,029 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9197  @50, Recall: 0.1641  MRR: 0.0271\n","2021-12-08 15:52:25,012 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7280  @50, Recall: 0.1641  MRR: 0.0343\n","2021-12-08 15:52:27,975 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8486  @50, Recall: 0.1445  MRR: 0.0209\n","2021-12-08 15:52:30,927 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4804  @50, Recall: 0.1914  MRR: 0.0426\n","2021-12-08 15:52:33,888 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7018  @50, Recall: 0.1719  MRR: 0.0379\n","2021-12-08 15:52:36,803 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6604  @50, Recall: 0.1797  MRR: 0.0441\n","2021-12-08 15:52:39,769 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4375  @50, Recall: 0.2070  MRR: 0.0425\n","2021-12-08 15:52:42,708 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5945  @50, Recall: 0.1836  MRR: 0.0358\n","2021-12-08 15:52:45,668 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5239  @50, Recall: 0.2266  MRR: 0.0499\n","2021-12-08 15:52:48,632 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3865  @50, Recall: 0.2031  MRR: 0.0387\n","2021-12-08 15:52:51,550 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8244  @50, Recall: 0.1523  MRR: 0.0314\n","2021-12-08 15:52:54,417 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7035  @50, Recall: 0.2070  MRR: 0.0398\n","2021-12-08 15:52:57,286 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7261  @50, Recall: 0.1992  MRR: 0.0290\n","2021-12-08 15:53:00,247 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6158  @50, Recall: 0.1953  MRR: 0.0254\n","2021-12-08 15:53:03,163 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4477  @50, Recall: 0.2305  MRR: 0.0456\n","2021-12-08 15:53:06,140 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6670  @50, Recall: 0.1680  MRR: 0.0323\n","2021-12-08 15:53:09,083 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6864  @50, Recall: 0.1602  MRR: 0.0349\n","2021-12-08 15:53:12,027 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4186  @50, Recall: 0.2188  MRR: 0.0358\n","2021-12-08 15:53:14,938 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5451  @50, Recall: 0.1758  MRR: 0.0408\n","2021-12-08 15:53:17,886 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6944  @50, Recall: 0.1914  MRR: 0.0319\n","2021-12-08 15:53:20,886 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8051  @50, Recall: 0.1641  MRR: 0.0320\n","2021-12-08 15:53:23,850 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7169  @50, Recall: 0.1641  MRR: 0.0293\n","2021-12-08 15:53:26,831 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6215  @50, Recall: 0.1797  MRR: 0.0342\n","2021-12-08 15:53:29,773 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4861  @50, Recall: 0.2070  MRR: 0.0376\n","2021-12-08 15:53:32,740 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8641  @50, Recall: 0.1406  MRR: 0.0273\n","2021-12-08 15:53:35,728 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7750  @50, Recall: 0.1602  MRR: 0.0344\n","2021-12-08 15:53:38,680 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5712  @50, Recall: 0.1797  MRR: 0.0282\n","2021-12-08 15:53:41,659 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5298  @50, Recall: 0.1797  MRR: 0.0377\n","2021-12-08 15:53:44,606 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8785  @50, Recall: 0.1484  MRR: 0.0238\n","2021-12-08 15:53:47,574 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6130  @50, Recall: 0.1914  MRR: 0.0472\n","2021-12-08 15:53:50,523 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6836  @50, Recall: 0.1797  MRR: 0.0223\n","2021-12-08 15:53:53,434 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7540  @50, Recall: 0.1758  MRR: 0.0304\n","2021-12-08 15:53:56,347 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4840  @50, Recall: 0.1953  MRR: 0.0389\n","2021-12-08 15:53:59,250 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6825  @50, Recall: 0.2070  MRR: 0.0418\n","2021-12-08 15:54:02,194 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7285  @50, Recall: 0.1836  MRR: 0.0277\n","2021-12-08 15:54:05,217 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5464  @50, Recall: 0.2227  MRR: 0.0421\n","2021-12-08 15:54:08,280 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8981  @50, Recall: 0.1211  MRR: 0.0278\n","2021-12-08 15:54:11,307 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5641  @50, Recall: 0.2070  MRR: 0.0524\n","2021-12-08 15:54:14,336 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5859  @50, Recall: 0.1953  MRR: 0.0324\n","2021-12-08 15:54:17,370 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7482  @50, Recall: 0.1680  MRR: 0.0327\n","2021-12-08 15:54:20,385 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6133  @50, Recall: 0.1953  MRR: 0.0422\n","2021-12-08 15:54:23,341 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8753  @50, Recall: 0.1797  MRR: 0.0292\n","2021-12-08 15:54:26,303 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4982  @50, Recall: 0.1914  MRR: 0.0331\n","2021-12-08 15:54:29,231 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7296  @50, Recall: 0.1719  MRR: 0.0230\n","2021-12-08 15:54:32,222 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8647  @50, Recall: 0.1445  MRR: 0.0219\n","2021-12-08 15:54:35,178 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7402  @50, Recall: 0.1523  MRR: 0.0218\n","2021-12-08 15:54:38,164 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6608  @50, Recall: 0.1641  MRR: 0.0257\n","2021-12-08 15:54:41,227 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7504  @50, Recall: 0.1406  MRR: 0.0278\n","2021-12-08 15:54:44,285 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7426  @50, Recall: 0.1719  MRR: 0.0282\n","2021-12-08 15:54:47,317 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9072  @50, Recall: 0.1445  MRR: 0.0266\n","2021-12-08 15:54:50,307 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9659  @50, Recall: 0.1367  MRR: 0.0228\n","2021-12-08 15:54:53,239 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8002  @50, Recall: 0.1523  MRR: 0.0280\n","2021-12-08 15:54:56,241 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6563  @50, Recall: 0.1680  MRR: 0.0346\n","2021-12-08 15:54:59,239 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5640  @50, Recall: 0.1602  MRR: 0.0198\n","2021-12-08 15:55:02,220 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5913  @50, Recall: 0.2148  MRR: 0.0378\n","2021-12-08 15:55:05,186 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6837  @50, Recall: 0.1875  MRR: 0.0223\n","2021-12-08 15:55:08,123 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8627  @50, Recall: 0.1602  MRR: 0.0180\n","2021-12-08 15:55:11,115 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5890  @50, Recall: 0.1797  MRR: 0.0349\n","2021-12-08 15:55:14,163 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4594  @50, Recall: 0.1875  MRR: 0.0345\n","2021-12-08 15:55:17,166 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6384  @50, Recall: 0.1602  MRR: 0.0281\n","2021-12-08 15:55:20,300 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7762  @50, Recall: 0.1406  MRR: 0.0166\n","2021-12-08 15:55:23,364 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6219  @50, Recall: 0.1641  MRR: 0.0334\n","2021-12-08 15:55:26,399 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0069  @50, Recall: 0.1250  MRR: 0.0278\n","2021-12-08 15:55:29,483 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8112  @50, Recall: 0.1719  MRR: 0.0291\n","2021-12-08 15:55:32,471 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6599  @50, Recall: 0.1875  MRR: 0.0329\n","2021-12-08 15:55:35,525 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4228  @50, Recall: 0.2148  MRR: 0.0431\n","2021-12-08 15:55:38,531 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6495  @50, Recall: 0.1406  MRR: 0.0274\n","2021-12-08 15:55:41,631 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7814  @50, Recall: 0.1367  MRR: 0.0224\n","2021-12-08 15:55:44,722 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5964  @50, Recall: 0.1875  MRR: 0.0397\n","2021-12-08 15:55:47,752 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5939  @50, Recall: 0.1836  MRR: 0.0308\n","2021-12-08 15:55:50,798 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8622  @50, Recall: 0.1523  MRR: 0.0220\n","2021-12-08 15:55:53,808 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5769  @50, Recall: 0.1602  MRR: 0.0203\n","2021-12-08 15:55:56,898 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8727  @50, Recall: 0.1328  MRR: 0.0284\n","2021-12-08 15:55:59,905 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9084  @50, Recall: 0.1172  MRR: 0.0208\n","2021-12-08 15:56:02,921 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7195  @50, Recall: 0.1445  MRR: 0.0254\n","2021-12-08 15:56:05,943 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7444  @50, Recall: 0.1523  MRR: 0.0198\n","2021-12-08 15:56:08,956 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5075  @50, Recall: 0.1875  MRR: 0.0344\n","2021-12-08 15:56:11,928 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6624  @50, Recall: 0.1719  MRR: 0.0316\n","2021-12-08 15:56:14,924 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7354  @50, Recall: 0.1445  MRR: 0.0343\n","2021-12-08 15:56:17,899 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9256  @50, Recall: 0.1484  MRR: 0.0227\n","2021-12-08 15:56:20,923 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4241  @50, Recall: 0.1953  MRR: 0.0444\n","2021-12-08 15:56:23,929 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7550  @50, Recall: 0.1719  MRR: 0.0314\n","2021-12-08 15:56:26,906 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7728  @50, Recall: 0.1758  MRR: 0.0301\n","2021-12-08 15:56:29,912 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5369  @50, Recall: 0.1719  MRR: 0.0233\n","2021-12-08 15:56:32,854 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6001  @50, Recall: 0.1797  MRR: 0.0323\n","2021-12-08 15:56:35,798 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7100  @50, Recall: 0.1523  MRR: 0.0265\n","2021-12-08 15:56:37,523 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8114  @50, Recall: 0.1149  MRR: 0.0169\n","2021-12-08 15:56:37,530 src.recall.sr_gnn.train.trainer:INFO:Epoch: 9 Train Loss: 8.3943 Test Loss: 9.6871 Recall: 0.1745 MRR: 0.0322\n","2021-12-08 15:56:37,531 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1779,  0.0322  Epoch: 7,  9\n","Epoch:  50% 10/20 [1:19:32<1:19:36, 477.69s/it]2021-12-08 15:56:38,030 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 3540\n","2021-12-08 15:56:38,072 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.54697\n","2021-12-08 15:56:46,000 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.47817\n","2021-12-08 15:56:54,068 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.55373\n","2021-12-08 15:57:02,042 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 7.63241\n","2021-12-08 15:57:09,939 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 7.70941\n","2021-12-08 15:57:17,931 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 7.78318\n","2021-12-08 15:57:26,094 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 7.84935\n","2021-12-08 15:57:34,268 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 7.91199\n","2021-12-08 15:57:42,326 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 7.97005\n","2021-12-08 15:57:50,377 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.02557\n","2021-12-08 15:57:58,491 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.07721\n","2021-12-08 15:58:06,486 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.12607\n","2021-12-08 15:58:14,405 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.17049\n","2021-12-08 15:58:22,273 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.21135\n","2021-12-08 15:58:30,143 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.25088\n","2021-12-08 15:58:37,999 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.28794\n","2021-12-08 15:58:45,910 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.32185\n","2021-12-08 15:58:53,796 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.35368\n","2021-12-08 15:59:02,191 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5560  @50, Recall: 0.1953  MRR: 0.0445\n","2021-12-08 15:59:05,221 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8285  @50, Recall: 0.1680  MRR: 0.0289\n","2021-12-08 15:59:08,212 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0565  @50, Recall: 0.1172  MRR: 0.0166\n","2021-12-08 15:59:11,204 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6925  @50, Recall: 0.2070  MRR: 0.0478\n","2021-12-08 15:59:14,159 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8443  @50, Recall: 0.1641  MRR: 0.0253\n","2021-12-08 15:59:17,163 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5328  @50, Recall: 0.2148  MRR: 0.0452\n","2021-12-08 15:59:20,214 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5731  @50, Recall: 0.1914  MRR: 0.0444\n","2021-12-08 15:59:23,242 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7462  @50, Recall: 0.1680  MRR: 0.0314\n","2021-12-08 15:59:26,264 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8990  @50, Recall: 0.1211  MRR: 0.0333\n","2021-12-08 15:59:29,316 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8508  @50, Recall: 0.1562  MRR: 0.0301\n","2021-12-08 15:59:32,373 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6130  @50, Recall: 0.1875  MRR: 0.0333\n","2021-12-08 15:59:35,463 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6225  @50, Recall: 0.2148  MRR: 0.0290\n","2021-12-08 15:59:38,506 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7165  @50, Recall: 0.1680  MRR: 0.0251\n","2021-12-08 15:59:41,543 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9828  @50, Recall: 0.1289  MRR: 0.0212\n","2021-12-08 15:59:44,564 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9703  @50, Recall: 0.1445  MRR: 0.0285\n","2021-12-08 15:59:47,586 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5057  @50, Recall: 0.1992  MRR: 0.0386\n","2021-12-08 15:59:50,624 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6622  @50, Recall: 0.1562  MRR: 0.0407\n","2021-12-08 15:59:53,638 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7165  @50, Recall: 0.1953  MRR: 0.0328\n","2021-12-08 15:59:56,690 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0699  @50, Recall: 0.1172  MRR: 0.0214\n","2021-12-08 15:59:59,752 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6229  @50, Recall: 0.2109  MRR: 0.0305\n","2021-12-08 16:00:02,807 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8347  @50, Recall: 0.1641  MRR: 0.0231\n","2021-12-08 16:00:05,843 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3464  @50, Recall: 0.2344  MRR: 0.0470\n","2021-12-08 16:00:08,907 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4258  @50, Recall: 0.2031  MRR: 0.0508\n","2021-12-08 16:00:11,961 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3977  @50, Recall: 0.2109  MRR: 0.0562\n","2021-12-08 16:00:15,005 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7790  @50, Recall: 0.1875  MRR: 0.0454\n","2021-12-08 16:00:18,007 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7527  @50, Recall: 0.1875  MRR: 0.0313\n","2021-12-08 16:00:21,012 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6591  @50, Recall: 0.1875  MRR: 0.0255\n","2021-12-08 16:00:23,956 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7519  @50, Recall: 0.1875  MRR: 0.0357\n","2021-12-08 16:00:26,901 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9197  @50, Recall: 0.1602  MRR: 0.0336\n","2021-12-08 16:00:29,879 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7363  @50, Recall: 0.1758  MRR: 0.0318\n","2021-12-08 16:00:32,832 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8679  @50, Recall: 0.1484  MRR: 0.0202\n","2021-12-08 16:00:35,810 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5154  @50, Recall: 0.1914  MRR: 0.0442\n","2021-12-08 16:00:38,817 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7204  @50, Recall: 0.1836  MRR: 0.0404\n","2021-12-08 16:00:41,838 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7023  @50, Recall: 0.1680  MRR: 0.0456\n","2021-12-08 16:00:44,826 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4423  @50, Recall: 0.2109  MRR: 0.0400\n","2021-12-08 16:00:47,783 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6412  @50, Recall: 0.1992  MRR: 0.0299\n","2021-12-08 16:00:50,756 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5455  @50, Recall: 0.2109  MRR: 0.0466\n","2021-12-08 16:00:53,753 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4200  @50, Recall: 0.2188  MRR: 0.0362\n","2021-12-08 16:00:56,694 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8576  @50, Recall: 0.1484  MRR: 0.0308\n","2021-12-08 16:00:59,646 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6840  @50, Recall: 0.2227  MRR: 0.0428\n","2021-12-08 16:01:02,565 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7338  @50, Recall: 0.1914  MRR: 0.0328\n","2021-12-08 16:01:05,533 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6269  @50, Recall: 0.1719  MRR: 0.0245\n","2021-12-08 16:01:08,514 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4862  @50, Recall: 0.2188  MRR: 0.0459\n","2021-12-08 16:01:11,517 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7130  @50, Recall: 0.1602  MRR: 0.0316\n","2021-12-08 16:01:14,580 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6959  @50, Recall: 0.1719  MRR: 0.0311\n","2021-12-08 16:01:17,647 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4318  @50, Recall: 0.2188  MRR: 0.0384\n","2021-12-08 16:01:20,662 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5257  @50, Recall: 0.1992  MRR: 0.0416\n","2021-12-08 16:01:23,658 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7227  @50, Recall: 0.1719  MRR: 0.0285\n","2021-12-08 16:01:26,700 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8308  @50, Recall: 0.1719  MRR: 0.0301\n","2021-12-08 16:01:29,671 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7313  @50, Recall: 0.1484  MRR: 0.0310\n","2021-12-08 16:01:32,685 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6590  @50, Recall: 0.1602  MRR: 0.0348\n","2021-12-08 16:01:35,676 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5790  @50, Recall: 0.2031  MRR: 0.0373\n","2021-12-08 16:01:38,646 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8922  @50, Recall: 0.1484  MRR: 0.0289\n","2021-12-08 16:01:41,633 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7596  @50, Recall: 0.1562  MRR: 0.0303\n","2021-12-08 16:01:44,624 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5766  @50, Recall: 0.2031  MRR: 0.0276\n","2021-12-08 16:01:47,622 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5781  @50, Recall: 0.1875  MRR: 0.0398\n","2021-12-08 16:01:50,624 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8739  @50, Recall: 0.1562  MRR: 0.0206\n","2021-12-08 16:01:53,741 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5932  @50, Recall: 0.1992  MRR: 0.0506\n","2021-12-08 16:01:56,753 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6635  @50, Recall: 0.1719  MRR: 0.0261\n","2021-12-08 16:01:59,821 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7679  @50, Recall: 0.1758  MRR: 0.0269\n","2021-12-08 16:02:02,876 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4919  @50, Recall: 0.2148  MRR: 0.0348\n","2021-12-08 16:02:05,870 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6963  @50, Recall: 0.2070  MRR: 0.0412\n","2021-12-08 16:02:08,847 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6851  @50, Recall: 0.1680  MRR: 0.0275\n","2021-12-08 16:02:11,773 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5980  @50, Recall: 0.2148  MRR: 0.0377\n","2021-12-08 16:02:14,704 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8753  @50, Recall: 0.1328  MRR: 0.0289\n","2021-12-08 16:02:17,666 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5625  @50, Recall: 0.2148  MRR: 0.0539\n","2021-12-08 16:02:20,664 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5985  @50, Recall: 0.2031  MRR: 0.0300\n","2021-12-08 16:02:23,655 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8183  @50, Recall: 0.1680  MRR: 0.0275\n","2021-12-08 16:02:26,597 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6010  @50, Recall: 0.1992  MRR: 0.0421\n","2021-12-08 16:02:29,536 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8429  @50, Recall: 0.1719  MRR: 0.0341\n","2021-12-08 16:02:32,454 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5115  @50, Recall: 0.1953  MRR: 0.0338\n","2021-12-08 16:02:35,365 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7642  @50, Recall: 0.1719  MRR: 0.0215\n","2021-12-08 16:02:38,279 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9410  @50, Recall: 0.1328  MRR: 0.0213\n","2021-12-08 16:02:41,202 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7393  @50, Recall: 0.1758  MRR: 0.0288\n","2021-12-08 16:02:44,111 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7039  @50, Recall: 0.1797  MRR: 0.0243\n","2021-12-08 16:02:47,075 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7419  @50, Recall: 0.1367  MRR: 0.0289\n","2021-12-08 16:02:49,985 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7328  @50, Recall: 0.1797  MRR: 0.0310\n","2021-12-08 16:02:52,927 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8897  @50, Recall: 0.1641  MRR: 0.0282\n","2021-12-08 16:02:55,921 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9447  @50, Recall: 0.1367  MRR: 0.0229\n","2021-12-08 16:02:58,885 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8362  @50, Recall: 0.1406  MRR: 0.0274\n","2021-12-08 16:03:01,825 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5810  @50, Recall: 0.1836  MRR: 0.0372\n","2021-12-08 16:03:04,782 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5529  @50, Recall: 0.1758  MRR: 0.0254\n","2021-12-08 16:03:07,755 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5932  @50, Recall: 0.2109  MRR: 0.0429\n","2021-12-08 16:03:10,741 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6348  @50, Recall: 0.1680  MRR: 0.0236\n","2021-12-08 16:03:13,696 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8714  @50, Recall: 0.1602  MRR: 0.0211\n","2021-12-08 16:03:16,750 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6479  @50, Recall: 0.1758  MRR: 0.0303\n","2021-12-08 16:03:19,758 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4518  @50, Recall: 0.1836  MRR: 0.0352\n","2021-12-08 16:03:22,735 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6202  @50, Recall: 0.1797  MRR: 0.0333\n","2021-12-08 16:03:25,720 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7567  @50, Recall: 0.1406  MRR: 0.0178\n","2021-12-08 16:03:28,773 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6521  @50, Recall: 0.1602  MRR: 0.0331\n","2021-12-08 16:03:31,762 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9671  @50, Recall: 0.1211  MRR: 0.0260\n","2021-12-08 16:03:34,774 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8490  @50, Recall: 0.1562  MRR: 0.0255\n","2021-12-08 16:03:37,768 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6441  @50, Recall: 0.1836  MRR: 0.0330\n","2021-12-08 16:03:40,774 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4073  @50, Recall: 0.2031  MRR: 0.0453\n","2021-12-08 16:03:43,728 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6369  @50, Recall: 0.1484  MRR: 0.0275\n","2021-12-08 16:03:46,721 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7593  @50, Recall: 0.1602  MRR: 0.0277\n","2021-12-08 16:03:49,693 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5845  @50, Recall: 0.1914  MRR: 0.0397\n","2021-12-08 16:03:52,675 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5962  @50, Recall: 0.1836  MRR: 0.0297\n","2021-12-08 16:03:55,652 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8451  @50, Recall: 0.1523  MRR: 0.0236\n","2021-12-08 16:03:58,632 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6059  @50, Recall: 0.1484  MRR: 0.0193\n","2021-12-08 16:04:01,668 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8639  @50, Recall: 0.1328  MRR: 0.0281\n","2021-12-08 16:04:04,654 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8830  @50, Recall: 0.1328  MRR: 0.0189\n","2021-12-08 16:04:07,627 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7710  @50, Recall: 0.1406  MRR: 0.0252\n","2021-12-08 16:04:10,610 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7637  @50, Recall: 0.1484  MRR: 0.0192\n","2021-12-08 16:04:13,531 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5093  @50, Recall: 0.1719  MRR: 0.0378\n","2021-12-08 16:04:16,512 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7203  @50, Recall: 0.1875  MRR: 0.0315\n","2021-12-08 16:04:19,478 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7481  @50, Recall: 0.1328  MRR: 0.0316\n","2021-12-08 16:04:22,349 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9174  @50, Recall: 0.1367  MRR: 0.0215\n","2021-12-08 16:04:25,217 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4236  @50, Recall: 0.1836  MRR: 0.0378\n","2021-12-08 16:04:28,086 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7780  @50, Recall: 0.1641  MRR: 0.0298\n","2021-12-08 16:04:30,982 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7962  @50, Recall: 0.1719  MRR: 0.0287\n","2021-12-08 16:04:33,936 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6069  @50, Recall: 0.1797  MRR: 0.0238\n","2021-12-08 16:04:36,890 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6376  @50, Recall: 0.1680  MRR: 0.0256\n","2021-12-08 16:04:39,841 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6915  @50, Recall: 0.1445  MRR: 0.0293\n","2021-12-08 16:04:41,565 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7699  @50, Recall: 0.1216  MRR: 0.0197\n","2021-12-08 16:04:41,573 src.recall.sr_gnn.train.trainer:INFO:Epoch: 10 Train Loss: 8.3751 Test Loss: 9.6975 Recall: 0.1743 MRR: 0.0322\n","2021-12-08 16:04:41,573 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1779,  0.0322  Epoch: 7,  9\n","2021-12-08 16:04:41,573 src.recall.sr_gnn.train.trainer:INFO:After 3 epochs not improve, early stop\n","Epoch:  50% 10/20 [1:27:36<1:27:36, 525.65s/it]\n","2021-12-08 16:04:41,593 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1779,  0.0322  Epoch: 7,  9\n"]}]},{"cell_type":"code","source":["!python src/recall/sr_gnn/train/main2.py --task train --node_count 117720 --checkpoint_path output/srgnn/online/srgnn_model.ckpt \\\n","--train_input Datasets/online/srgnn/train_item_seq_enhanced.txt --test_input Datasets/online/srgnn/test_item_seq.txt \\\n","--gru_step 2 --epochs 20 --lr 0.001 --lr_dc 2 --dc_rate 0.1 --early_stop_epoch 3 --hidden_size 256 --batch_size 256 \\\n","--max_len 20 --has_uid True --sigma 10 --sq_max_len 5 --batch_logging_step 200"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YCRCUagod8S7","executionInfo":{"status":"ok","timestamp":1638982453396,"user_tz":-480,"elapsed":2938178,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"5f7e05cd-9ca7-4f9d-ddf7-3614ec9462e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:10: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n","\n","2021-12-08 16:05:10,916 root:INFO:Data Loaded, Length: 1145704ï¼Œ Max Length: 10\n","2021-12-08 16:05:10,980 root:INFO:Data Loaded, Length: 5976ï¼Œ Max Length: 19\n","2021-12-08 16:05:10,981 src.recall.sr_gnn.train.trainer:INFO:Train: {'task': 'train', 'node_count': 117720, 'checkpoint_path': 'output/srgnn/online/srgnn_model.ckpt', 'l2': None, 'lr': 0.001, 'gru_step': 2, 'batch_size': 256, 'hidden_size': 256, 'epochs': 20, 'batch_logging_step': 200, 'save_step': None, 'max_test_batch': None, 'lr_dc': 8952.0, 'dc_rate': 0.1, 'early_stop_epochs': 3, 'sigma': 10.0, 'max_len': 20, 'has_uid': True, 'feature_init': None, 'node_weight': None, 'node_weight_trainable': None, 'sq_max_len': 5, 'train_input': 'Datasets/online/srgnn/train_item_seq_enhanced.txt', 'test_input': 'Datasets/online/srgnn/test_item_seq.txt', 'eval_input': None, 'session_input': None, 'item_lookup': None, 'item_feature': None, 'recommend_output': None, 'embedding_output': None, 'rec_extra_count': None, 'rec_count': None, 'remove_duplicates': None}\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:23: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","2021-12-08 16:05:10,981 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:23: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:24: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","2021-12-08 16:05:10,982 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:24: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:13: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","2021-12-08 16:05:11,001 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:13: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:18: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n","2021-12-08 16:05:11,025 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:18: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","2021-12-08 16:05:11,053 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","2021-12-08 16:05:11,059 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","2021-12-08 16:05:11,069 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:61: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","2021-12-08 16:05:11,134 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:61: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:62: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","2021-12-08 16:05:11,137 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:62: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","2021-12-08 16:05:11,137 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:72: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","2021-12-08 16:05:11,154 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:72: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","2021-12-08 16:05:11,161 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","2021-12-08 16:05:11,161 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","2021-12-08 16:05:11,242 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:79: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","2021-12-08 16:05:11,615 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:79: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:81: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","2021-12-08 16:05:11,615 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:81: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","2021-12-08 16:05:11.616141: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n","2021-12-08 16:05:11.621860: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz\n","2021-12-08 16:05:11.622345: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e37cc5ea00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-12-08 16:05:11.622380: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2021-12-08 16:05:11.624239: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-12-08 16:05:11.729183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 16:05:11.730048: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e37cc5ed80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2021-12-08 16:05:11.730095: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n","2021-12-08 16:05:11.730289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 16:05:11.730897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n","pciBusID: 0000:00:04.0\n","2021-12-08 16:05:11.731216: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-12-08 16:05:11.733057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-12-08 16:05:11.734179: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2021-12-08 16:05:11.734484: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2021-12-08 16:05:11.736387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2021-12-08 16:05:11.737347: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2021-12-08 16:05:11.741052: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2021-12-08 16:05:11.741170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 16:05:11.741850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 16:05:11.742404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2021-12-08 16:05:11.742483: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-12-08 16:05:11.743593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-12-08 16:05:11.743618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n","2021-12-08 16:05:11.743627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n","2021-12-08 16:05:11.743750: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 16:05:11.744379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 16:05:11.745031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15060 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:82: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n","\n","2021-12-08 16:05:11,746 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:82: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n","\n","2021-12-08 16:05:12,778 model:INFO:The passed save_path is not a valid checkpoint: output/srgnn/online/srgnn_model.ckpt\n","Epoch:   0% 0/20 [00:00<?, ?it/s]2021-12-08 16:05:13,130 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 4476\n","2021-12-08 16:05:13.344770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-12-08 16:05:13,610 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 12.11336\n","2021-12-08 16:05:21,579 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 11.84248\n","2021-12-08 16:05:29,433 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 11.73489\n","2021-12-08 16:05:37,289 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 11.62875\n","2021-12-08 16:05:45,146 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 11.52114\n","2021-12-08 16:05:53,023 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 11.42092\n","2021-12-08 16:06:00,833 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 11.33203\n","2021-12-08 16:06:08,626 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 11.24752\n","2021-12-08 16:06:16,435 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 11.17155\n","2021-12-08 16:06:24,150 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 11.09956\n","2021-12-08 16:06:31,825 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 11.03241\n","2021-12-08 16:06:39,517 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 10.97033\n","2021-12-08 16:06:47,332 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 10.91399\n","2021-12-08 16:06:55,262 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 10.85971\n","2021-12-08 16:07:03,064 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 10.81161\n","2021-12-08 16:07:10,943 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 10.76676\n","2021-12-08 16:07:18,954 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 10.72420\n","2021-12-08 16:07:26,952 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 10.68574\n","2021-12-08 16:07:34,797 src.recall.sr_gnn.train.trainer:INFO:Batch 3600, Loss: 10.64754\n","2021-12-08 16:07:42,598 src.recall.sr_gnn.train.trainer:INFO:Batch 3800, Loss: 10.61160\n","2021-12-08 16:07:50,470 src.recall.sr_gnn.train.trainer:INFO:Batch 4000, Loss: 10.57814\n","2021-12-08 16:08:06,033 src.recall.sr_gnn.train.trainer:INFO:Batch 4400, Loss: 10.51625\n","2021-12-08 16:08:11,945 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7525  @50, Recall: 0.1836  MRR: 0.0333\n","2021-12-08 16:08:15,001 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9445  @50, Recall: 0.1211  MRR: 0.0250\n","2021-12-08 16:08:17,898 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0217  @50, Recall: 0.1367  MRR: 0.0229\n","2021-12-08 16:08:20,861 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8444  @50, Recall: 0.1172  MRR: 0.0278\n","2021-12-08 16:08:23,791 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6725  @50, Recall: 0.1719  MRR: 0.0424\n","2021-12-08 16:08:26,717 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8779  @50, Recall: 0.1484  MRR: 0.0197\n","2021-12-08 16:08:29,633 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0355  @50, Recall: 0.1406  MRR: 0.0350\n","2021-12-08 16:08:32,576 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6933  @50, Recall: 0.1875  MRR: 0.0361\n","2021-12-08 16:08:35,515 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4814  @50, Recall: 0.1914  MRR: 0.0417\n","2021-12-08 16:08:38,436 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0998  @50, Recall: 0.1133  MRR: 0.0171\n","2021-12-08 16:08:41,377 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8809  @50, Recall: 0.1523  MRR: 0.0247\n","2021-12-08 16:08:44,302 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5383  @50, Recall: 0.1992  MRR: 0.0422\n","2021-12-08 16:08:47,264 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7415  @50, Recall: 0.1523  MRR: 0.0203\n","2021-12-08 16:08:50,183 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.9533  @50, Recall: 0.1328  MRR: 0.0266\n","2021-12-08 16:08:53,099 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6370  @50, Recall: 0.1523  MRR: 0.0352\n","2021-12-08 16:08:56,014 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5768  @50, Recall: 0.1758  MRR: 0.0241\n","2021-12-08 16:08:59,025 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7601  @50, Recall: 0.1523  MRR: 0.0433\n","2021-12-08 16:09:01,985 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7488  @50, Recall: 0.1211  MRR: 0.0205\n","2021-12-08 16:09:04,961 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7566  @50, Recall: 0.1523  MRR: 0.0374\n","2021-12-08 16:09:07,943 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6449  @50, Recall: 0.1641  MRR: 0.0457\n","2021-12-08 16:09:10,922 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7427  @50, Recall: 0.1328  MRR: 0.0260\n","2021-12-08 16:09:13,925 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7644  @50, Recall: 0.1055  MRR: 0.0258\n","2021-12-08 16:09:16,916 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 10.0073  @50, Recall: 0.1133  MRR: 0.0234\n","2021-12-08 16:09:17,943 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5546  @50, Recall: 0.1250  MRR: 0.0111\n","2021-12-08 16:09:17,945 src.recall.sr_gnn.train.trainer:INFO:Epoch: 0 Train Loss: 10.5058 Test Loss: 9.7804 Recall: 0.1483 MRR: 0.0300\n","2021-12-08 16:09:17,945 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1483,  0.0300  Epoch: 0,  0\n","Epoch:   5% 1/20 [04:05<1:17:45, 245.57s/it]2021-12-08 16:09:18,766 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 4476\n","2021-12-08 16:09:18,808 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 8.54225\n","2021-12-08 16:09:26,973 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 8.48922\n","2021-12-08 16:09:35,136 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 8.54980\n","2021-12-08 16:09:43,045 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 8.60012\n","2021-12-08 16:09:50,945 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 8.65360\n","2021-12-08 16:09:58,816 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 8.70129\n","2021-12-08 16:10:06,641 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.74453\n","2021-12-08 16:10:14,429 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.78503\n","2021-12-08 16:10:22,188 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.82376\n","2021-12-08 16:10:29,941 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.85704\n","2021-12-08 16:10:37,779 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.88955\n","2021-12-08 16:10:45,558 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.91810\n","2021-12-08 16:10:53,346 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.94479\n","2021-12-08 16:11:01,109 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.96891\n","2021-12-08 16:11:08,990 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.99180\n","2021-12-08 16:11:17,014 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 9.01380\n","2021-12-08 16:11:25,011 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 9.03442\n","2021-12-08 16:11:32,975 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 9.05314\n","2021-12-08 16:11:40,826 src.recall.sr_gnn.train.trainer:INFO:Batch 3600, Loss: 9.07071\n","2021-12-08 16:11:48,711 src.recall.sr_gnn.train.trainer:INFO:Batch 3800, Loss: 9.08783\n","2021-12-08 16:11:56,620 src.recall.sr_gnn.train.trainer:INFO:Batch 4000, Loss: 9.10353\n","2021-12-08 16:12:04,325 src.recall.sr_gnn.train.trainer:INFO:Batch 4200, Loss: 9.11795\n","2021-12-08 16:12:12,026 src.recall.sr_gnn.train.trainer:INFO:Batch 4400, Loss: 9.13177\n","2021-12-08 16:12:17,862 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5302  @50, Recall: 0.1953  MRR: 0.0346\n","2021-12-08 16:12:20,796 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7615  @50, Recall: 0.1406  MRR: 0.0316\n","2021-12-08 16:12:23,697 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8042  @50, Recall: 0.1406  MRR: 0.0213\n","2021-12-08 16:12:26,589 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6865  @50, Recall: 0.1523  MRR: 0.0228\n","2021-12-08 16:12:29,489 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4138  @50, Recall: 0.2344  MRR: 0.0420\n","2021-12-08 16:12:32,381 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6570  @50, Recall: 0.1680  MRR: 0.0224\n","2021-12-08 16:12:35,291 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8505  @50, Recall: 0.1484  MRR: 0.0327\n","2021-12-08 16:12:38,215 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5198  @50, Recall: 0.2109  MRR: 0.0355\n","2021-12-08 16:12:41,112 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2809  @50, Recall: 0.2188  MRR: 0.0406\n","2021-12-08 16:12:44,001 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7925  @50, Recall: 0.1602  MRR: 0.0228\n","2021-12-08 16:12:46,952 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6581  @50, Recall: 0.1680  MRR: 0.0287\n","2021-12-08 16:12:49,940 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2820  @50, Recall: 0.2148  MRR: 0.0439\n","2021-12-08 16:12:52,906 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5213  @50, Recall: 0.1602  MRR: 0.0211\n","2021-12-08 16:12:55,878 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6689  @50, Recall: 0.1484  MRR: 0.0260\n","2021-12-08 16:12:58,852 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4151  @50, Recall: 0.1680  MRR: 0.0340\n","2021-12-08 16:13:01,798 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4440  @50, Recall: 0.2227  MRR: 0.0255\n","2021-12-08 16:13:04,733 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5641  @50, Recall: 0.1758  MRR: 0.0354\n","2021-12-08 16:13:07,704 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5932  @50, Recall: 0.1484  MRR: 0.0240\n","2021-12-08 16:13:10,655 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5203  @50, Recall: 0.1758  MRR: 0.0371\n","2021-12-08 16:13:13,617 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3570  @50, Recall: 0.1914  MRR: 0.0384\n","2021-12-08 16:13:16,567 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4662  @50, Recall: 0.1836  MRR: 0.0267\n","2021-12-08 16:13:19,578 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5397  @50, Recall: 0.1250  MRR: 0.0261\n","2021-12-08 16:13:22,592 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7264  @50, Recall: 0.1328  MRR: 0.0232\n","2021-12-08 16:13:23,619 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.1669  @50, Recall: 0.2045  MRR: 0.0230\n","2021-12-08 16:13:23,621 src.recall.sr_gnn.train.trainer:INFO:Epoch: 1 Train Loss: 9.1370 Test Loss: 9.5508 Recall: 0.1737 MRR: 0.0302\n","2021-12-08 16:13:23,621 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1737,  0.0302  Epoch: 1,  1\n","Epoch:  10% 2/20 [08:11<1:13:40, 245.58s/it]2021-12-08 16:13:24,219 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 4476\n","2021-12-08 16:13:24,264 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 8.08024\n","2021-12-08 16:13:32,445 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 8.10794\n","2021-12-08 16:13:40,388 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 8.17295\n","2021-12-08 16:13:48,227 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 8.23840\n","2021-12-08 16:13:56,160 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 8.29986\n","2021-12-08 16:14:04,079 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 8.35816\n","2021-12-08 16:14:11,875 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.41076\n","2021-12-08 16:14:19,652 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.45973\n","2021-12-08 16:14:27,409 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.50527\n","2021-12-08 16:14:35,164 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.54800\n","2021-12-08 16:14:42,897 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.58832\n","2021-12-08 16:14:50,693 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.62381\n","2021-12-08 16:14:58,627 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.65872\n","2021-12-08 16:15:06,578 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.69213\n","2021-12-08 16:15:14,431 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.72196\n","2021-12-08 16:15:22,348 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.75036\n","2021-12-08 16:15:30,222 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.77689\n","2021-12-08 16:15:38,166 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.80059\n","2021-12-08 16:15:45,978 src.recall.sr_gnn.train.trainer:INFO:Batch 3600, Loss: 8.82324\n","2021-12-08 16:15:53,753 src.recall.sr_gnn.train.trainer:INFO:Batch 3800, Loss: 8.84529\n","2021-12-08 16:16:01,495 src.recall.sr_gnn.train.trainer:INFO:Batch 4000, Loss: 8.86622\n","2021-12-08 16:16:09,235 src.recall.sr_gnn.train.trainer:INFO:Batch 4200, Loss: 8.88635\n","2021-12-08 16:16:16,963 src.recall.sr_gnn.train.trainer:INFO:Batch 4400, Loss: 8.90435\n","2021-12-08 16:16:22,690 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5063  @50, Recall: 0.2109  MRR: 0.0364\n","2021-12-08 16:16:25,595 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6789  @50, Recall: 0.1719  MRR: 0.0336\n","2021-12-08 16:16:28,480 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7720  @50, Recall: 0.1445  MRR: 0.0232\n","2021-12-08 16:16:31,378 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6074  @50, Recall: 0.1484  MRR: 0.0253\n","2021-12-08 16:16:34,273 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2986  @50, Recall: 0.2188  MRR: 0.0436\n","2021-12-08 16:16:37,192 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5865  @50, Recall: 0.1602  MRR: 0.0286\n","2021-12-08 16:16:40,202 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8306  @50, Recall: 0.1406  MRR: 0.0326\n","2021-12-08 16:16:43,146 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4520  @50, Recall: 0.1953  MRR: 0.0381\n","2021-12-08 16:16:46,088 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.1950  @50, Recall: 0.2188  MRR: 0.0384\n","2021-12-08 16:16:49,083 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7992  @50, Recall: 0.1484  MRR: 0.0212\n","2021-12-08 16:16:52,070 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6115  @50, Recall: 0.1797  MRR: 0.0282\n","2021-12-08 16:16:55,191 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2272  @50, Recall: 0.2148  MRR: 0.0441\n","2021-12-08 16:16:58,176 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4758  @50, Recall: 0.1680  MRR: 0.0254\n","2021-12-08 16:17:01,134 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6128  @50, Recall: 0.1680  MRR: 0.0256\n","2021-12-08 16:17:04,112 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4215  @50, Recall: 0.1641  MRR: 0.0334\n","2021-12-08 16:17:07,071 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3742  @50, Recall: 0.2227  MRR: 0.0295\n","2021-12-08 16:17:10,048 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5131  @50, Recall: 0.1836  MRR: 0.0320\n","2021-12-08 16:17:13,075 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5746  @50, Recall: 0.1250  MRR: 0.0200\n","2021-12-08 16:17:16,102 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4692  @50, Recall: 0.1875  MRR: 0.0394\n","2021-12-08 16:17:19,110 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3728  @50, Recall: 0.1875  MRR: 0.0405\n","2021-12-08 16:17:22,082 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4882  @50, Recall: 0.1719  MRR: 0.0271\n","2021-12-08 16:17:25,035 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5274  @50, Recall: 0.1328  MRR: 0.0276\n","2021-12-08 16:17:27,986 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6552  @50, Recall: 0.1328  MRR: 0.0247\n","2021-12-08 16:17:29,024 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2235  @50, Recall: 0.1818  MRR: 0.0169\n","2021-12-08 16:17:29,026 src.recall.sr_gnn.train.trainer:INFO:Epoch: 2 Train Loss: 8.9110 Test Loss: 9.5114 Recall: 0.1739 MRR: 0.0310\n","2021-12-08 16:17:29,027 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1739,  0.0310  Epoch: 2,  2\n","Epoch:  15% 3/20 [12:16<1:09:33, 245.51s/it]2021-12-08 16:17:29,587 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 4476\n","2021-12-08 16:17:29,628 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.87881\n","2021-12-08 16:17:37,616 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 8.00894\n","2021-12-08 16:17:45,681 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 8.06849\n","2021-12-08 16:17:53,489 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 8.13908\n","2021-12-08 16:18:01,207 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 8.20397\n","2021-12-08 16:18:08,908 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 8.26487\n","2021-12-08 16:18:16,669 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.32097\n","2021-12-08 16:18:24,482 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.37082\n","2021-12-08 16:18:32,229 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.41626\n","2021-12-08 16:18:39,967 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.46201\n","2021-12-08 16:18:47,728 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.50156\n","2021-12-08 16:18:55,586 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.54033\n","2021-12-08 16:19:03,545 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.57575\n","2021-12-08 16:19:11,429 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.60918\n","2021-12-08 16:19:19,301 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.64005\n","2021-12-08 16:19:27,257 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.66861\n","2021-12-08 16:19:35,152 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.69561\n","2021-12-08 16:19:42,992 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.72161\n","2021-12-08 16:19:50,779 src.recall.sr_gnn.train.trainer:INFO:Batch 3600, Loss: 8.74558\n","2021-12-08 16:19:58,651 src.recall.sr_gnn.train.trainer:INFO:Batch 3800, Loss: 8.76828\n","2021-12-08 16:20:06,422 src.recall.sr_gnn.train.trainer:INFO:Batch 4000, Loss: 8.78962\n","2021-12-08 16:20:14,159 src.recall.sr_gnn.train.trainer:INFO:Batch 4200, Loss: 8.80966\n","2021-12-08 16:20:21,914 src.recall.sr_gnn.train.trainer:INFO:Batch 4400, Loss: 8.82901\n","2021-12-08 16:20:27,682 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4947  @50, Recall: 0.2188  MRR: 0.0341\n","2021-12-08 16:20:30,629 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6865  @50, Recall: 0.1484  MRR: 0.0344\n","2021-12-08 16:20:33,548 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7367  @50, Recall: 0.1602  MRR: 0.0251\n","2021-12-08 16:20:36,462 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5879  @50, Recall: 0.1602  MRR: 0.0259\n","2021-12-08 16:20:39,409 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3311  @50, Recall: 0.2227  MRR: 0.0413\n","2021-12-08 16:20:42,322 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5639  @50, Recall: 0.1719  MRR: 0.0276\n","2021-12-08 16:20:45,230 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8104  @50, Recall: 0.1641  MRR: 0.0283\n","2021-12-08 16:20:48,160 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3889  @50, Recall: 0.2227  MRR: 0.0403\n","2021-12-08 16:20:51,152 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.1824  @50, Recall: 0.2148  MRR: 0.0443\n","2021-12-08 16:20:54,158 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7394  @50, Recall: 0.1562  MRR: 0.0284\n","2021-12-08 16:20:57,134 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5719  @50, Recall: 0.1719  MRR: 0.0302\n","2021-12-08 16:21:00,171 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2371  @50, Recall: 0.2383  MRR: 0.0475\n","2021-12-08 16:21:03,209 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4214  @50, Recall: 0.1680  MRR: 0.0231\n","2021-12-08 16:21:06,267 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5844  @50, Recall: 0.1680  MRR: 0.0305\n","2021-12-08 16:21:09,265 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3820  @50, Recall: 0.1875  MRR: 0.0329\n","2021-12-08 16:21:12,260 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3909  @50, Recall: 0.2148  MRR: 0.0302\n","2021-12-08 16:21:15,218 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4594  @50, Recall: 0.1836  MRR: 0.0298\n","2021-12-08 16:21:18,199 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5472  @50, Recall: 0.1523  MRR: 0.0260\n","2021-12-08 16:21:21,195 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4382  @50, Recall: 0.1836  MRR: 0.0367\n","2021-12-08 16:21:24,174 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3611  @50, Recall: 0.2031  MRR: 0.0353\n","2021-12-08 16:21:27,140 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4906  @50, Recall: 0.1914  MRR: 0.0291\n","2021-12-08 16:21:30,078 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5422  @50, Recall: 0.1133  MRR: 0.0211\n","2021-12-08 16:21:33,013 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6920  @50, Recall: 0.1172  MRR: 0.0221\n","2021-12-08 16:21:34,018 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.1492  @50, Recall: 0.2159  MRR: 0.0206\n","2021-12-08 16:21:34,020 src.recall.sr_gnn.train.trainer:INFO:Epoch: 3 Train Loss: 8.8363 Test Loss: 9.4912 Recall: 0.1802 MRR: 0.0313\n","2021-12-08 16:21:34,020 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1802,  0.0313  Epoch: 3,  3\n","Epoch:  20% 4/20 [16:21<1:05:24, 245.30s/it]2021-12-08 16:21:34,604 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 4476\n","2021-12-08 16:21:34,646 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.88407\n","2021-12-08 16:21:42,660 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.94411\n","2021-12-08 16:21:50,588 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 8.01640\n","2021-12-08 16:21:58,476 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 8.08908\n","2021-12-08 16:22:06,216 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 8.15451\n","2021-12-08 16:22:14,014 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 8.21215\n","2021-12-08 16:22:21,798 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.26917\n","2021-12-08 16:22:29,541 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.32105\n","2021-12-08 16:22:37,295 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.36864\n","2021-12-08 16:22:45,230 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.41342\n","2021-12-08 16:22:53,241 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.45627\n","2021-12-08 16:23:01,199 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.49456\n","2021-12-08 16:23:09,070 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.52918\n","2021-12-08 16:23:16,913 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.56321\n","2021-12-08 16:23:24,767 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.59478\n","2021-12-08 16:23:32,587 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.62470\n","2021-12-08 16:23:40,443 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.65234\n","2021-12-08 16:23:48,302 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.67843\n","2021-12-08 16:23:56,093 src.recall.sr_gnn.train.trainer:INFO:Batch 3600, Loss: 8.70296\n","2021-12-08 16:24:03,780 src.recall.sr_gnn.train.trainer:INFO:Batch 3800, Loss: 8.72588\n","2021-12-08 16:24:11,438 src.recall.sr_gnn.train.trainer:INFO:Batch 4000, Loss: 8.74788\n","2021-12-08 16:24:19,161 src.recall.sr_gnn.train.trainer:INFO:Batch 4200, Loss: 8.76864\n","2021-12-08 16:24:26,923 src.recall.sr_gnn.train.trainer:INFO:Batch 4400, Loss: 8.78796\n","2021-12-08 16:24:32,703 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5261  @50, Recall: 0.2227  MRR: 0.0339\n","2021-12-08 16:24:35,586 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6913  @50, Recall: 0.1602  MRR: 0.0325\n","2021-12-08 16:24:38,459 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7172  @50, Recall: 0.1523  MRR: 0.0223\n","2021-12-08 16:24:41,390 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6110  @50, Recall: 0.1562  MRR: 0.0246\n","2021-12-08 16:24:44,376 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3707  @50, Recall: 0.2148  MRR: 0.0402\n","2021-12-08 16:24:47,373 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5782  @50, Recall: 0.1797  MRR: 0.0254\n","2021-12-08 16:24:50,333 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7663  @50, Recall: 0.1641  MRR: 0.0298\n","2021-12-08 16:24:53,290 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4272  @50, Recall: 0.2031  MRR: 0.0404\n","2021-12-08 16:24:56,336 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.1808  @50, Recall: 0.2070  MRR: 0.0485\n","2021-12-08 16:24:59,330 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7217  @50, Recall: 0.1758  MRR: 0.0235\n","2021-12-08 16:25:02,311 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6113  @50, Recall: 0.1875  MRR: 0.0255\n","2021-12-08 16:25:05,332 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3254  @50, Recall: 0.2188  MRR: 0.0445\n","2021-12-08 16:25:08,345 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4314  @50, Recall: 0.1758  MRR: 0.0216\n","2021-12-08 16:25:11,354 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5806  @50, Recall: 0.1680  MRR: 0.0251\n","2021-12-08 16:25:14,313 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3937  @50, Recall: 0.1914  MRR: 0.0325\n","2021-12-08 16:25:17,276 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4453  @50, Recall: 0.2109  MRR: 0.0273\n","2021-12-08 16:25:20,294 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4174  @50, Recall: 0.1914  MRR: 0.0338\n","2021-12-08 16:25:23,228 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5708  @50, Recall: 0.1523  MRR: 0.0258\n","2021-12-08 16:25:26,194 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4473  @50, Recall: 0.1836  MRR: 0.0362\n","2021-12-08 16:25:29,207 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3495  @50, Recall: 0.2070  MRR: 0.0349\n","2021-12-08 16:25:32,174 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4326  @50, Recall: 0.1875  MRR: 0.0300\n","2021-12-08 16:25:35,172 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4057  @50, Recall: 0.1289  MRR: 0.0272\n","2021-12-08 16:25:38,142 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7551  @50, Recall: 0.1328  MRR: 0.0240\n","2021-12-08 16:25:39,190 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2837  @50, Recall: 0.1932  MRR: 0.0175\n","2021-12-08 16:25:39,193 src.recall.sr_gnn.train.trainer:INFO:Epoch: 4 Train Loss: 8.7952 Test Loss: 9.5017 Recall: 0.1816 MRR: 0.0307\n","2021-12-08 16:25:39,193 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1816,  0.0313  Epoch: 4,  3\n","Epoch:  25% 5/20 [20:26<1:01:18, 245.25s/it]2021-12-08 16:25:39,767 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 4476\n","2021-12-08 16:25:39,807 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.83682\n","2021-12-08 16:25:47,709 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.91056\n","2021-12-08 16:25:55,536 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.98146\n","2021-12-08 16:26:03,357 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 8.05169\n","2021-12-08 16:26:11,122 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 8.11612\n","2021-12-08 16:26:18,924 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 8.17781\n","2021-12-08 16:26:26,623 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.23327\n","2021-12-08 16:26:34,404 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.28681\n","2021-12-08 16:26:42,344 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.33475\n","2021-12-08 16:26:50,329 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.38055\n","2021-12-08 16:26:58,412 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.42203\n","2021-12-08 16:27:06,308 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.46185\n","2021-12-08 16:27:14,254 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.49810\n","2021-12-08 16:27:22,125 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.53223\n","2021-12-08 16:27:29,957 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.56364\n","2021-12-08 16:27:37,739 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.59270\n","2021-12-08 16:27:45,610 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.62099\n","2021-12-08 16:27:53,320 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.64778\n","2021-12-08 16:28:01,033 src.recall.sr_gnn.train.trainer:INFO:Batch 3600, Loss: 8.67245\n","2021-12-08 16:28:08,926 src.recall.sr_gnn.train.trainer:INFO:Batch 3800, Loss: 8.69608\n","2021-12-08 16:28:16,798 src.recall.sr_gnn.train.trainer:INFO:Batch 4000, Loss: 8.71929\n","2021-12-08 16:28:24,620 src.recall.sr_gnn.train.trainer:INFO:Batch 4200, Loss: 8.74020\n","2021-12-08 16:28:32,337 src.recall.sr_gnn.train.trainer:INFO:Batch 4400, Loss: 8.75963\n","2021-12-08 16:28:38,137 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4272  @50, Recall: 0.2070  MRR: 0.0383\n","2021-12-08 16:28:41,076 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7003  @50, Recall: 0.1406  MRR: 0.0317\n","2021-12-08 16:28:44,048 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6845  @50, Recall: 0.1562  MRR: 0.0222\n","2021-12-08 16:28:47,112 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5610  @50, Recall: 0.1641  MRR: 0.0305\n","2021-12-08 16:28:50,158 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3125  @50, Recall: 0.2227  MRR: 0.0453\n","2021-12-08 16:28:53,138 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6230  @50, Recall: 0.1680  MRR: 0.0235\n","2021-12-08 16:28:56,117 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7934  @50, Recall: 0.1484  MRR: 0.0313\n","2021-12-08 16:28:59,098 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3982  @50, Recall: 0.2148  MRR: 0.0416\n","2021-12-08 16:29:02,053 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.1705  @50, Recall: 0.2148  MRR: 0.0449\n","2021-12-08 16:29:05,020 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7078  @50, Recall: 0.1719  MRR: 0.0236\n","2021-12-08 16:29:07,992 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5529  @50, Recall: 0.1914  MRR: 0.0303\n","2021-12-08 16:29:10,922 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2883  @50, Recall: 0.2188  MRR: 0.0460\n","2021-12-08 16:29:13,860 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4334  @50, Recall: 0.1797  MRR: 0.0250\n","2021-12-08 16:29:16,810 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5420  @50, Recall: 0.1680  MRR: 0.0276\n","2021-12-08 16:29:19,837 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3624  @50, Recall: 0.1797  MRR: 0.0353\n","2021-12-08 16:29:22,868 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4403  @50, Recall: 0.2109  MRR: 0.0250\n","2021-12-08 16:29:25,814 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4247  @50, Recall: 0.1953  MRR: 0.0321\n","2021-12-08 16:29:28,756 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5440  @50, Recall: 0.1680  MRR: 0.0295\n","2021-12-08 16:29:31,690 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4397  @50, Recall: 0.1797  MRR: 0.0436\n","2021-12-08 16:29:34,589 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2942  @50, Recall: 0.1953  MRR: 0.0469\n","2021-12-08 16:29:37,490 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3842  @50, Recall: 0.2070  MRR: 0.0307\n","2021-12-08 16:29:40,409 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4931  @50, Recall: 0.1406  MRR: 0.0263\n","2021-12-08 16:29:43,421 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7292  @50, Recall: 0.1250  MRR: 0.0219\n","2021-12-08 16:29:44,462 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2761  @50, Recall: 0.1932  MRR: 0.0189\n","2021-12-08 16:29:44,464 src.recall.sr_gnn.train.trainer:INFO:Epoch: 5 Train Loss: 8.7665 Test Loss: 9.4826 Recall: 0.1814 MRR: 0.0326\n","2021-12-08 16:29:44,464 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1816,  0.0326  Epoch: 4,  5\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","2021-12-08 16:29:44,732 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","Epoch:  30% 6/20 [24:32<57:13, 245.27s/it]  2021-12-08 16:29:45,029 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 4476\n","2021-12-08 16:29:45,071 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.89250\n","2021-12-08 16:29:52,951 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.89123\n","2021-12-08 16:30:00,775 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.96019\n","2021-12-08 16:30:08,564 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 8.02668\n","2021-12-08 16:30:16,278 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 8.09249\n","2021-12-08 16:30:24,049 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 8.15232\n","2021-12-08 16:30:31,877 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.20805\n","2021-12-08 16:30:39,811 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.25959\n","2021-12-08 16:30:47,827 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.30892\n","2021-12-08 16:30:55,775 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.35410\n","2021-12-08 16:31:03,781 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.39700\n","2021-12-08 16:31:11,716 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.43656\n","2021-12-08 16:31:19,565 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.47298\n","2021-12-08 16:31:27,415 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.50730\n","2021-12-08 16:31:35,213 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.53873\n","2021-12-08 16:31:42,961 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.56911\n","2021-12-08 16:31:50,657 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.59677\n","2021-12-08 16:31:58,486 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.62457\n","2021-12-08 16:32:06,230 src.recall.sr_gnn.train.trainer:INFO:Batch 3600, Loss: 8.64988\n","2021-12-08 16:32:13,971 src.recall.sr_gnn.train.trainer:INFO:Batch 3800, Loss: 8.67294\n","2021-12-08 16:32:21,705 src.recall.sr_gnn.train.trainer:INFO:Batch 4000, Loss: 8.69535\n","2021-12-08 16:32:29,602 src.recall.sr_gnn.train.trainer:INFO:Batch 4200, Loss: 8.71755\n","2021-12-08 16:32:37,491 src.recall.sr_gnn.train.trainer:INFO:Batch 4400, Loss: 8.73789\n","2021-12-08 16:32:43,373 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5019  @50, Recall: 0.2109  MRR: 0.0392\n","2021-12-08 16:32:46,343 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7192  @50, Recall: 0.1602  MRR: 0.0341\n","2021-12-08 16:32:49,354 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7328  @50, Recall: 0.1562  MRR: 0.0234\n","2021-12-08 16:32:52,304 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6038  @50, Recall: 0.1758  MRR: 0.0283\n","2021-12-08 16:32:55,260 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3320  @50, Recall: 0.2148  MRR: 0.0462\n","2021-12-08 16:32:58,219 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5552  @50, Recall: 0.1758  MRR: 0.0269\n","2021-12-08 16:33:01,191 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7451  @50, Recall: 0.1680  MRR: 0.0336\n","2021-12-08 16:33:04,161 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3862  @50, Recall: 0.2070  MRR: 0.0439\n","2021-12-08 16:33:07,125 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.1440  @50, Recall: 0.2188  MRR: 0.0500\n","2021-12-08 16:33:10,120 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7292  @50, Recall: 0.1562  MRR: 0.0216\n","2021-12-08 16:33:13,163 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5686  @50, Recall: 0.1875  MRR: 0.0290\n","2021-12-08 16:33:16,160 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2552  @50, Recall: 0.2305  MRR: 0.0455\n","2021-12-08 16:33:19,154 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4393  @50, Recall: 0.1680  MRR: 0.0207\n","2021-12-08 16:33:22,103 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5816  @50, Recall: 0.1719  MRR: 0.0265\n","2021-12-08 16:33:25,039 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3480  @50, Recall: 0.1797  MRR: 0.0347\n","2021-12-08 16:33:27,955 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4424  @50, Recall: 0.2109  MRR: 0.0264\n","2021-12-08 16:33:30,855 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4657  @50, Recall: 0.2070  MRR: 0.0350\n","2021-12-08 16:33:33,770 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5325  @50, Recall: 0.1641  MRR: 0.0258\n","2021-12-08 16:33:36,656 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4127  @50, Recall: 0.1680  MRR: 0.0400\n","2021-12-08 16:33:39,557 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3093  @50, Recall: 0.2070  MRR: 0.0409\n","2021-12-08 16:33:42,507 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4069  @50, Recall: 0.1992  MRR: 0.0275\n","2021-12-08 16:33:45,541 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4742  @50, Recall: 0.1328  MRR: 0.0290\n","2021-12-08 16:33:48,533 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7271  @50, Recall: 0.1445  MRR: 0.0237\n","2021-12-08 16:33:49,579 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2315  @50, Recall: 0.1818  MRR: 0.0201\n","2021-12-08 16:33:49,581 src.recall.sr_gnn.train.trainer:INFO:Epoch: 6 Train Loss: 8.7455 Test Loss: 9.4852 Recall: 0.1832 MRR: 0.0325\n","2021-12-08 16:33:49,581 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1832,  0.0326  Epoch: 6,  5\n","Epoch:  35% 7/20 [28:37<53:07, 245.21s/it]2021-12-08 16:33:50,283 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 4476\n","2021-12-08 16:33:50,323 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.79025\n","2021-12-08 16:33:58,082 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.87569\n","2021-12-08 16:34:05,891 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.93976\n","2021-12-08 16:34:13,664 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 8.00749\n","2021-12-08 16:34:21,505 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 8.07370\n","2021-12-08 16:34:29,430 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 8.13361\n","2021-12-08 16:34:37,314 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.18912\n","2021-12-08 16:34:45,248 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.24123\n","2021-12-08 16:34:53,218 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.29033\n","2021-12-08 16:35:01,189 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.33656\n","2021-12-08 16:35:09,135 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.37711\n","2021-12-08 16:35:16,990 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.41687\n","2021-12-08 16:35:24,873 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.45510\n","2021-12-08 16:35:32,769 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.48904\n","2021-12-08 16:35:40,488 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.52086\n","2021-12-08 16:35:48,302 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.55073\n","2021-12-08 16:35:56,033 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.57921\n","2021-12-08 16:36:03,750 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.60605\n","2021-12-08 16:36:11,500 src.recall.sr_gnn.train.trainer:INFO:Batch 3600, Loss: 8.63145\n","2021-12-08 16:36:19,197 src.recall.sr_gnn.train.trainer:INFO:Batch 3800, Loss: 8.65422\n","2021-12-08 16:36:27,100 src.recall.sr_gnn.train.trainer:INFO:Batch 4000, Loss: 8.67756\n","2021-12-08 16:36:35,058 src.recall.sr_gnn.train.trainer:INFO:Batch 4200, Loss: 8.69898\n","2021-12-08 16:36:42,876 src.recall.sr_gnn.train.trainer:INFO:Batch 4400, Loss: 8.71879\n","2021-12-08 16:36:48,801 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4776  @50, Recall: 0.2148  MRR: 0.0338\n","2021-12-08 16:36:51,764 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6403  @50, Recall: 0.1523  MRR: 0.0343\n","2021-12-08 16:36:54,876 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7073  @50, Recall: 0.1562  MRR: 0.0236\n","2021-12-08 16:36:57,830 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6291  @50, Recall: 0.1562  MRR: 0.0244\n","2021-12-08 16:37:00,825 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3590  @50, Recall: 0.2188  MRR: 0.0423\n","2021-12-08 16:37:03,847 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5690  @50, Recall: 0.1953  MRR: 0.0252\n","2021-12-08 16:37:06,842 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.8082  @50, Recall: 0.1484  MRR: 0.0318\n","2021-12-08 16:37:09,818 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4526  @50, Recall: 0.2031  MRR: 0.0379\n","2021-12-08 16:37:12,821 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.1149  @50, Recall: 0.2266  MRR: 0.0534\n","2021-12-08 16:37:15,860 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7301  @50, Recall: 0.1562  MRR: 0.0222\n","2021-12-08 16:37:18,915 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5445  @50, Recall: 0.1953  MRR: 0.0276\n","2021-12-08 16:37:21,897 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2600  @50, Recall: 0.2188  MRR: 0.0505\n","2021-12-08 16:37:24,914 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4525  @50, Recall: 0.1680  MRR: 0.0224\n","2021-12-08 16:37:27,859 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5765  @50, Recall: 0.1797  MRR: 0.0271\n","2021-12-08 16:37:30,784 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4155  @50, Recall: 0.1758  MRR: 0.0316\n","2021-12-08 16:37:33,711 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4404  @50, Recall: 0.2227  MRR: 0.0281\n","2021-12-08 16:37:36,694 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4575  @50, Recall: 0.1992  MRR: 0.0335\n","2021-12-08 16:37:39,670 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5612  @50, Recall: 0.1641  MRR: 0.0252\n","2021-12-08 16:37:42,654 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4354  @50, Recall: 0.1719  MRR: 0.0417\n","2021-12-08 16:37:45,693 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2862  @50, Recall: 0.2109  MRR: 0.0438\n","2021-12-08 16:37:48,687 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3691  @50, Recall: 0.2148  MRR: 0.0330\n","2021-12-08 16:37:51,590 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4583  @50, Recall: 0.1406  MRR: 0.0266\n","2021-12-08 16:37:54,514 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6661  @50, Recall: 0.1211  MRR: 0.0268\n","2021-12-08 16:37:55,549 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.1624  @50, Recall: 0.2159  MRR: 0.0178\n","2021-12-08 16:37:55,551 src.recall.sr_gnn.train.trainer:INFO:Epoch: 7 Train Loss: 8.7261 Test Loss: 9.4822 Recall: 0.1836 MRR: 0.0322\n","2021-12-08 16:37:55,551 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1836,  0.0326  Epoch: 7,  5\n","Epoch:  40% 8/20 [32:43<49:05, 245.45s/it]2021-12-08 16:37:56,102 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 4476\n","2021-12-08 16:37:56,142 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.71538\n","2021-12-08 16:38:03,873 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.84077\n","2021-12-08 16:38:11,770 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.90879\n","2021-12-08 16:38:19,707 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 7.98404\n","2021-12-08 16:38:27,705 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 8.05124\n","2021-12-08 16:38:35,782 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 8.11070\n","2021-12-08 16:38:43,866 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.16687\n","2021-12-08 16:38:51,698 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.22126\n","2021-12-08 16:38:59,467 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.27040\n","2021-12-08 16:39:07,251 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.31578\n","2021-12-08 16:39:15,105 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.35751\n","2021-12-08 16:39:22,860 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.39781\n","2021-12-08 16:39:30,524 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.43452\n","2021-12-08 16:39:38,155 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.46938\n","2021-12-08 16:39:45,816 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.50156\n","2021-12-08 16:39:53,414 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.53289\n","2021-12-08 16:40:00,988 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.56112\n","2021-12-08 16:40:08,610 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.58825\n","2021-12-08 16:40:16,318 src.recall.sr_gnn.train.trainer:INFO:Batch 3600, Loss: 8.61362\n","2021-12-08 16:40:24,280 src.recall.sr_gnn.train.trainer:INFO:Batch 3800, Loss: 8.63812\n","2021-12-08 16:40:32,168 src.recall.sr_gnn.train.trainer:INFO:Batch 4000, Loss: 8.66095\n","2021-12-08 16:40:39,901 src.recall.sr_gnn.train.trainer:INFO:Batch 4200, Loss: 8.68263\n","2021-12-08 16:40:47,575 src.recall.sr_gnn.train.trainer:INFO:Batch 4400, Loss: 8.70302\n","2021-12-08 16:40:53,465 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5113  @50, Recall: 0.2188  MRR: 0.0368\n","2021-12-08 16:40:56,429 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6831  @50, Recall: 0.1523  MRR: 0.0300\n","2021-12-08 16:40:59,439 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7418  @50, Recall: 0.1602  MRR: 0.0233\n","2021-12-08 16:41:02,373 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6083  @50, Recall: 0.1523  MRR: 0.0293\n","2021-12-08 16:41:05,373 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3287  @50, Recall: 0.2070  MRR: 0.0454\n","2021-12-08 16:41:08,328 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5601  @50, Recall: 0.1992  MRR: 0.0286\n","2021-12-08 16:41:11,311 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7869  @50, Recall: 0.1602  MRR: 0.0311\n","2021-12-08 16:41:14,251 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3804  @50, Recall: 0.2070  MRR: 0.0407\n","2021-12-08 16:41:17,220 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.1521  @50, Recall: 0.2188  MRR: 0.0490\n","2021-12-08 16:41:20,136 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7152  @50, Recall: 0.1641  MRR: 0.0258\n","2021-12-08 16:41:23,064 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6330  @50, Recall: 0.1797  MRR: 0.0276\n","2021-12-08 16:41:26,018 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3107  @50, Recall: 0.2266  MRR: 0.0425\n","2021-12-08 16:41:28,953 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3992  @50, Recall: 0.1875  MRR: 0.0198\n","2021-12-08 16:41:31,868 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5666  @50, Recall: 0.1719  MRR: 0.0305\n","2021-12-08 16:41:34,801 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3959  @50, Recall: 0.1797  MRR: 0.0353\n","2021-12-08 16:41:37,736 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4239  @50, Recall: 0.2188  MRR: 0.0278\n","2021-12-08 16:41:40,609 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4532  @50, Recall: 0.1914  MRR: 0.0308\n","2021-12-08 16:41:43,496 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5052  @50, Recall: 0.1641  MRR: 0.0247\n","2021-12-08 16:41:46,406 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4931  @50, Recall: 0.1680  MRR: 0.0393\n","2021-12-08 16:41:49,327 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3221  @50, Recall: 0.2266  MRR: 0.0403\n","2021-12-08 16:41:52,252 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3931  @50, Recall: 0.2031  MRR: 0.0283\n","2021-12-08 16:41:55,212 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4926  @50, Recall: 0.1328  MRR: 0.0290\n","2021-12-08 16:41:58,087 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6850  @50, Recall: 0.1445  MRR: 0.0225\n","2021-12-08 16:41:59,109 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2674  @50, Recall: 0.1705  MRR: 0.0107\n","2021-12-08 16:41:59,111 src.recall.sr_gnn.train.trainer:INFO:Epoch: 8 Train Loss: 8.7108 Test Loss: 9.4920 Recall: 0.1839 MRR: 0.0318\n","2021-12-08 16:41:59,111 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1839,  0.0326  Epoch: 8,  5\n","Epoch:  45% 9/20 [36:46<44:53, 244.86s/it]2021-12-08 16:41:59,680 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 4476\n","2021-12-08 16:41:59,721 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.82744\n","2021-12-08 16:42:07,510 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.82914\n","2021-12-08 16:42:15,483 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.89707\n","2021-12-08 16:42:23,390 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 7.96598\n","2021-12-08 16:42:31,080 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 8.03427\n","2021-12-08 16:42:38,811 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 8.09589\n","2021-12-08 16:42:46,488 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.15209\n","2021-12-08 16:42:54,104 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.20300\n","2021-12-08 16:43:01,749 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.25344\n","2021-12-08 16:43:09,511 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.29930\n","2021-12-08 16:43:17,147 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.34161\n","2021-12-08 16:43:24,705 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.38112\n","2021-12-08 16:43:32,189 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.41810\n","2021-12-08 16:43:39,775 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.45316\n","2021-12-08 16:43:47,330 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.48591\n","2021-12-08 16:43:54,820 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.51650\n","2021-12-08 16:44:02,415 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.54679\n","2021-12-08 16:44:10,203 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.57409\n","2021-12-08 16:44:18,034 src.recall.sr_gnn.train.trainer:INFO:Batch 3600, Loss: 8.59949\n","2021-12-08 16:44:25,758 src.recall.sr_gnn.train.trainer:INFO:Batch 3800, Loss: 8.62314\n","2021-12-08 16:44:33,445 src.recall.sr_gnn.train.trainer:INFO:Batch 4000, Loss: 8.64571\n","2021-12-08 16:44:41,018 src.recall.sr_gnn.train.trainer:INFO:Batch 4200, Loss: 8.66772\n","2021-12-08 16:44:48,604 src.recall.sr_gnn.train.trainer:INFO:Batch 4400, Loss: 8.68861\n","2021-12-08 16:44:54,330 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5273  @50, Recall: 0.2031  MRR: 0.0385\n","2021-12-08 16:44:57,227 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6461  @50, Recall: 0.1484  MRR: 0.0346\n","2021-12-08 16:45:00,079 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7269  @50, Recall: 0.1523  MRR: 0.0260\n","2021-12-08 16:45:02,978 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5733  @50, Recall: 0.1523  MRR: 0.0311\n","2021-12-08 16:45:05,851 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3588  @50, Recall: 0.2148  MRR: 0.0407\n","2021-12-08 16:45:08,706 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5411  @50, Recall: 0.1914  MRR: 0.0258\n","2021-12-08 16:45:11,567 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7717  @50, Recall: 0.1523  MRR: 0.0340\n","2021-12-08 16:45:14,421 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4210  @50, Recall: 0.1992  MRR: 0.0423\n","2021-12-08 16:45:17,336 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.1424  @50, Recall: 0.1992  MRR: 0.0434\n","2021-12-08 16:45:20,358 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7162  @50, Recall: 0.1523  MRR: 0.0213\n","2021-12-08 16:45:23,226 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6107  @50, Recall: 0.1914  MRR: 0.0294\n","2021-12-08 16:45:26,099 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2643  @50, Recall: 0.2070  MRR: 0.0433\n","2021-12-08 16:45:28,989 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4224  @50, Recall: 0.1680  MRR: 0.0216\n","2021-12-08 16:45:31,836 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5958  @50, Recall: 0.1758  MRR: 0.0288\n","2021-12-08 16:45:34,708 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3868  @50, Recall: 0.1992  MRR: 0.0331\n","2021-12-08 16:45:37,587 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3950  @50, Recall: 0.2305  MRR: 0.0289\n","2021-12-08 16:45:40,433 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4380  @50, Recall: 0.1836  MRR: 0.0340\n","2021-12-08 16:45:43,307 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5820  @50, Recall: 0.1484  MRR: 0.0285\n","2021-12-08 16:45:46,165 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4391  @50, Recall: 0.1836  MRR: 0.0447\n","2021-12-08 16:45:49,012 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3671  @50, Recall: 0.2109  MRR: 0.0413\n","2021-12-08 16:45:51,940 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3947  @50, Recall: 0.2070  MRR: 0.0291\n","2021-12-08 16:45:54,872 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4398  @50, Recall: 0.1367  MRR: 0.0296\n","2021-12-08 16:45:57,846 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7038  @50, Recall: 0.1523  MRR: 0.0245\n","2021-12-08 16:45:58,842 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2682  @50, Recall: 0.1705  MRR: 0.0156\n","2021-12-08 16:45:58,844 src.recall.sr_gnn.train.trainer:INFO:Epoch: 9 Train Loss: 8.6963 Test Loss: 9.4889 Recall: 0.1807 MRR: 0.0325\n","2021-12-08 16:45:58,844 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1839,  0.0326  Epoch: 8,  5\n","Epoch:  50% 10/20 [40:46<40:31, 243.17s/it]2021-12-08 16:45:59,065 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 4476\n","2021-12-08 16:45:59,104 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.75218\n","2021-12-08 16:46:06,911 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.80639\n","2021-12-08 16:46:14,713 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.87394\n","2021-12-08 16:46:22,488 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 7.94520\n","2021-12-08 16:46:30,350 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 8.01448\n","2021-12-08 16:46:38,045 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 8.07657\n","2021-12-08 16:46:45,687 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.13156\n","2021-12-08 16:46:53,339 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.18392\n","2021-12-08 16:47:00,891 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.23508\n","2021-12-08 16:47:08,445 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.28331\n","2021-12-08 16:47:15,911 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.32620\n","2021-12-08 16:47:23,421 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.36628\n","2021-12-08 16:47:30,936 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.40309\n","2021-12-08 16:47:38,448 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.43966\n","2021-12-08 16:47:45,974 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.47188\n","2021-12-08 16:47:53,662 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.50299\n","2021-12-08 16:48:01,387 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.53238\n","2021-12-08 16:48:09,178 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.55944\n","2021-12-08 16:48:16,803 src.recall.sr_gnn.train.trainer:INFO:Batch 3600, Loss: 8.58537\n","2021-12-08 16:48:24,423 src.recall.sr_gnn.train.trainer:INFO:Batch 3800, Loss: 8.60986\n","2021-12-08 16:48:32,036 src.recall.sr_gnn.train.trainer:INFO:Batch 4000, Loss: 8.63264\n","2021-12-08 16:48:39,643 src.recall.sr_gnn.train.trainer:INFO:Batch 4200, Loss: 8.65444\n","2021-12-08 16:48:47,195 src.recall.sr_gnn.train.trainer:INFO:Batch 4400, Loss: 8.67563\n","2021-12-08 16:48:52,918 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5247  @50, Recall: 0.2148  MRR: 0.0375\n","2021-12-08 16:48:55,792 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7098  @50, Recall: 0.1523  MRR: 0.0304\n","2021-12-08 16:48:58,682 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7340  @50, Recall: 0.1523  MRR: 0.0249\n","2021-12-08 16:49:01,561 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5483  @50, Recall: 0.1680  MRR: 0.0272\n","2021-12-08 16:49:04,387 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3265  @50, Recall: 0.2109  MRR: 0.0420\n","2021-12-08 16:49:07,236 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5488  @50, Recall: 0.1836  MRR: 0.0276\n","2021-12-08 16:49:10,193 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7418  @50, Recall: 0.1445  MRR: 0.0308\n","2021-12-08 16:49:13,072 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4058  @50, Recall: 0.2148  MRR: 0.0392\n","2021-12-08 16:49:15,928 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.1098  @50, Recall: 0.2227  MRR: 0.0551\n","2021-12-08 16:49:18,810 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7036  @50, Recall: 0.1602  MRR: 0.0194\n","2021-12-08 16:49:21,668 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5711  @50, Recall: 0.1914  MRR: 0.0348\n","2021-12-08 16:49:24,501 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3209  @50, Recall: 0.2070  MRR: 0.0424\n","2021-12-08 16:49:27,382 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4250  @50, Recall: 0.1758  MRR: 0.0218\n","2021-12-08 16:49:30,198 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5967  @50, Recall: 0.1680  MRR: 0.0326\n","2021-12-08 16:49:33,096 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3734  @50, Recall: 0.1875  MRR: 0.0321\n","2021-12-08 16:49:35,956 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4279  @50, Recall: 0.2266  MRR: 0.0286\n","2021-12-08 16:49:38,875 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4808  @50, Recall: 0.1953  MRR: 0.0317\n","2021-12-08 16:49:41,859 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5420  @50, Recall: 0.1484  MRR: 0.0239\n","2021-12-08 16:49:44,826 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4711  @50, Recall: 0.1758  MRR: 0.0432\n","2021-12-08 16:49:47,803 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3232  @50, Recall: 0.2031  MRR: 0.0441\n","2021-12-08 16:49:50,765 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3921  @50, Recall: 0.1875  MRR: 0.0274\n","2021-12-08 16:49:53,737 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4617  @50, Recall: 0.1367  MRR: 0.0243\n","2021-12-08 16:49:56,650 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7146  @50, Recall: 0.1328  MRR: 0.0213\n","2021-12-08 16:49:57,665 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2610  @50, Recall: 0.1932  MRR: 0.0193\n","2021-12-08 16:49:57,667 src.recall.sr_gnn.train.trainer:INFO:Epoch: 10 Train Loss: 8.6832 Test Loss: 9.4881 Recall: 0.1811 MRR: 0.0321\n","2021-12-08 16:49:57,667 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1839,  0.0326  Epoch: 8,  5\n","Epoch:  55% 11/20 [44:44<36:16, 241.84s/it]2021-12-08 16:49:57,886 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 4476\n","2021-12-08 16:49:57,927 src.recall.sr_gnn.train.trainer:INFO:Batch 0, Loss: 7.73676\n","2021-12-08 16:50:05,790 src.recall.sr_gnn.train.trainer:INFO:Batch 200, Loss: 7.80111\n","2021-12-08 16:50:13,483 src.recall.sr_gnn.train.trainer:INFO:Batch 400, Loss: 7.86411\n","2021-12-08 16:50:21,361 src.recall.sr_gnn.train.trainer:INFO:Batch 600, Loss: 7.92741\n","2021-12-08 16:50:29,093 src.recall.sr_gnn.train.trainer:INFO:Batch 800, Loss: 8.00043\n","2021-12-08 16:50:36,726 src.recall.sr_gnn.train.trainer:INFO:Batch 1000, Loss: 8.06148\n","2021-12-08 16:50:44,368 src.recall.sr_gnn.train.trainer:INFO:Batch 1200, Loss: 8.11743\n","2021-12-08 16:50:51,967 src.recall.sr_gnn.train.trainer:INFO:Batch 1400, Loss: 8.17107\n","2021-12-08 16:50:59,519 src.recall.sr_gnn.train.trainer:INFO:Batch 1600, Loss: 8.22104\n","2021-12-08 16:51:07,002 src.recall.sr_gnn.train.trainer:INFO:Batch 1800, Loss: 8.26850\n","2021-12-08 16:51:14,502 src.recall.sr_gnn.train.trainer:INFO:Batch 2000, Loss: 8.31288\n","2021-12-08 16:51:22,125 src.recall.sr_gnn.train.trainer:INFO:Batch 2200, Loss: 8.35376\n","2021-12-08 16:51:29,625 src.recall.sr_gnn.train.trainer:INFO:Batch 2400, Loss: 8.39075\n","2021-12-08 16:51:37,279 src.recall.sr_gnn.train.trainer:INFO:Batch 2600, Loss: 8.42603\n","2021-12-08 16:51:45,039 src.recall.sr_gnn.train.trainer:INFO:Batch 2800, Loss: 8.45931\n","2021-12-08 16:51:52,817 src.recall.sr_gnn.train.trainer:INFO:Batch 3000, Loss: 8.49007\n","2021-12-08 16:52:00,585 src.recall.sr_gnn.train.trainer:INFO:Batch 3200, Loss: 8.51933\n","2021-12-08 16:52:08,228 src.recall.sr_gnn.train.trainer:INFO:Batch 3400, Loss: 8.54645\n","2021-12-08 16:52:15,871 src.recall.sr_gnn.train.trainer:INFO:Batch 3600, Loss: 8.57257\n","2021-12-08 16:52:23,619 src.recall.sr_gnn.train.trainer:INFO:Batch 3800, Loss: 8.59669\n","2021-12-08 16:52:31,286 src.recall.sr_gnn.train.trainer:INFO:Batch 4000, Loss: 8.62039\n","2021-12-08 16:52:38,888 src.recall.sr_gnn.train.trainer:INFO:Batch 4200, Loss: 8.64312\n","2021-12-08 16:52:46,397 src.recall.sr_gnn.train.trainer:INFO:Batch 4400, Loss: 8.66356\n","2021-12-08 16:52:52,091 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4937  @50, Recall: 0.2148  MRR: 0.0361\n","2021-12-08 16:52:54,989 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6888  @50, Recall: 0.1562  MRR: 0.0336\n","2021-12-08 16:52:57,917 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7567  @50, Recall: 0.1523  MRR: 0.0220\n","2021-12-08 16:53:00,964 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6227  @50, Recall: 0.1602  MRR: 0.0279\n","2021-12-08 16:53:03,923 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3915  @50, Recall: 0.2070  MRR: 0.0412\n","2021-12-08 16:53:06,881 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5570  @50, Recall: 0.1797  MRR: 0.0255\n","2021-12-08 16:53:09,796 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7827  @50, Recall: 0.1523  MRR: 0.0308\n","2021-12-08 16:53:12,710 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4096  @50, Recall: 0.2188  MRR: 0.0404\n","2021-12-08 16:53:15,626 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.1436  @50, Recall: 0.2148  MRR: 0.0535\n","2021-12-08 16:53:18,545 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7700  @50, Recall: 0.1680  MRR: 0.0237\n","2021-12-08 16:53:21,406 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6042  @50, Recall: 0.1836  MRR: 0.0296\n","2021-12-08 16:53:24,327 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2945  @50, Recall: 0.2227  MRR: 0.0506\n","2021-12-08 16:53:27,256 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4428  @50, Recall: 0.1641  MRR: 0.0191\n","2021-12-08 16:53:30,208 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.6135  @50, Recall: 0.1602  MRR: 0.0264\n","2021-12-08 16:53:33,224 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3617  @50, Recall: 0.1875  MRR: 0.0404\n","2021-12-08 16:53:36,214 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4214  @50, Recall: 0.2148  MRR: 0.0290\n","2021-12-08 16:53:39,218 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4847  @50, Recall: 0.1914  MRR: 0.0341\n","2021-12-08 16:53:42,208 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.5762  @50, Recall: 0.1445  MRR: 0.0259\n","2021-12-08 16:53:45,206 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4738  @50, Recall: 0.1836  MRR: 0.0420\n","2021-12-08 16:53:48,242 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.3335  @50, Recall: 0.2305  MRR: 0.0418\n","2021-12-08 16:53:51,220 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4222  @50, Recall: 0.2031  MRR: 0.0272\n","2021-12-08 16:53:54,265 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.4622  @50, Recall: 0.1250  MRR: 0.0257\n","2021-12-08 16:53:57,215 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.7120  @50, Recall: 0.1328  MRR: 0.0255\n","2021-12-08 16:53:58,241 src.recall.sr_gnn.train.trainer:INFO:Test Loss: 9.2896  @50, Recall: 0.1932  MRR: 0.0151\n","2021-12-08 16:53:58,244 src.recall.sr_gnn.train.trainer:INFO:Epoch: 11 Train Loss: 8.6710 Test Loss: 9.5045 Recall: 0.1814 MRR: 0.0324\n","2021-12-08 16:53:58,244 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1839,  0.0326  Epoch: 8,  5\n","2021-12-08 16:53:58,244 src.recall.sr_gnn.train.trainer:INFO:After 3 epochs not improve, early stop\n","Epoch:  55% 11/20 [48:45<39:53, 265.95s/it]\n","2021-12-08 16:53:58,262 src.recall.sr_gnn.train.trainer:INFO:Best Recall and MRR: 0.1839,  0.0326  Epoch: 8,  5\n"]}]},{"cell_type":"markdown","source":["3. Get the save file address of the best model"],"metadata":{"id":"1O0au12KuOGk"}},{"cell_type":"code","source":["import re\n","import os\n","def find_checkpoint_path(checkpoint_prefix='srgnn_model.ckpt', mode=\"offline\"):\n","    checkpoint_dir = \"output/srgnn/{}\".format(mode)\n","    step_max = 0\n","    re_cp = re.compile(\"{}-(\\d+)\\.\".format(checkpoint_prefix))\n","    for file in os.listdir(checkpoint_dir):\n","        so = re_cp.search(file)\n","        if so:\n","            step = int(so.group(1))\n","            step_max = step if step > step_max else step_max\n","    checkpoint_path = '{}/{}-{}'.format(checkpoint_dir, checkpoint_prefix, step_max)\n","    print('CheckPoint: {}'.format(checkpoint_path))\n","    return checkpoint_path"],"metadata":{"id":"IBiDDg1X0pNJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["find_checkpoint_path(checkpoint_prefix='srgnn_model.ckpt', mode=\"offline\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"JFRsNuoK04wV","executionInfo":{"status":"ok","timestamp":1638982861794,"user_tz":-480,"elapsed":783,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"dd256f53-3384-4ae7-b241-fc14d55ca5c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CheckPoint: output/srgnn/offline/srgnn_model.ckpt-35400\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'output/srgnn/offline/srgnn_model.ckpt-35400'"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["find_checkpoint_path(checkpoint_prefix='srgnn_model.ckpt', mode=\"online\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"lJ7gTOuD059T","executionInfo":{"status":"ok","timestamp":1638982865747,"user_tz":-480,"elapsed":646,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"3bf7511a-a118-4437-f5ed-32788944f778"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CheckPoint: output/srgnn/online/srgnn_model.ckpt-40284\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'output/srgnn/online/srgnn_model.ckpt-40284'"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["4. Use SR-GNN model for recommendation"],"metadata":{"id":"BsyJqytEucAP"}},{"cell_type":"code","source":["!python src/recall/sr_gnn/train/main2.py --task recommend --node_count 117682 \\\n","    --checkpoint_path output/srgnn/offline/srgnn_model.ckpt-35400 --item_lookup Datasets/offline/srgnn/item_lookup.txt \\\n","    --recommend_output output/offline/Candidate/srgnn_recall_v1.txt --session_input Datasets/offline/srgnn/test_user_sess.txt --gru_step 2 \\\n","    --hidden_size 256 --batch_size 256 --rec_count 200 --rec_extra_count 50 --has_uid True \\\n","    --max_len 10 --sigma 10 --sq_max_len 5 \\"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yg0QpkyV08Oq","executionInfo":{"status":"ok","timestamp":1638983255054,"user_tz":-480,"elapsed":366387,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"ca5defc6-e07a-471c-a8c6-b7c41d1d26f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:10: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n","\n","2021-12-08 17:01:17,609 root:INFO:Data Loaded, Length: 29444ï¼Œ Max Length: 10\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:23: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","2021-12-08 17:01:17,609 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:23: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:24: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","2021-12-08 17:01:17,611 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:24: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:13: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","2021-12-08 17:01:17,628 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:13: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:18: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n","2021-12-08 17:01:17,654 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:18: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","2021-12-08 17:01:17,683 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","2021-12-08 17:01:17,690 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","2021-12-08 17:01:17,699 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:61: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","2021-12-08 17:01:17,768 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:61: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:62: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","2021-12-08 17:01:17,771 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:62: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","2021-12-08 17:01:17,771 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:72: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","2021-12-08 17:01:17,790 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:72: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","2021-12-08 17:01:17,797 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","2021-12-08 17:01:17,798 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","2021-12-08 17:01:17,877 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:79: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","2021-12-08 17:01:18,341 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:79: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:81: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","2021-12-08 17:01:18,341 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:81: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","2021-12-08 17:01:18.341795: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n","2021-12-08 17:01:18.352407: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz\n","2021-12-08 17:01:18.352886: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5648966b4a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-12-08 17:01:18.352944: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2021-12-08 17:01:18.354943: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-12-08 17:01:18.469222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 17:01:18.470214: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5648966b4d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2021-12-08 17:01:18.470261: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n","2021-12-08 17:01:18.470455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 17:01:18.471135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n","pciBusID: 0000:00:04.0\n","2021-12-08 17:01:18.471467: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-12-08 17:01:18.473233: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-12-08 17:01:18.474470: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2021-12-08 17:01:18.474795: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2021-12-08 17:01:18.476751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2021-12-08 17:01:18.477730: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2021-12-08 17:01:18.481857: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2021-12-08 17:01:18.481991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 17:01:18.482670: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 17:01:18.483324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2021-12-08 17:01:18.483428: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-12-08 17:01:18.484829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-12-08 17:01:18.484855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n","2021-12-08 17:01:18.484864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n","2021-12-08 17:01:18.485012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 17:01:18.485650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 17:01:18.486397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15060 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:82: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n","\n","2021-12-08 17:01:18,487 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:82: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n","\n","INFO:tensorflow:Restoring parameters from output/srgnn/offline/srgnn_model.ckpt-35400\n","2021-12-08 17:01:19,532 tensorflow:INFO:Restoring parameters from output/srgnn/offline/srgnn_model.ckpt-35400\n","2021-12-08 17:01:19,777 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 116\n","2021-12-08 17:01:19.840137: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-12-08 17:01:23,275 src.recall.sr_gnn.train.trainer:INFO:Batch 0 Finished, users: 256\n","2021-12-08 17:07:20,765 src.recall.sr_gnn.train.trainer:INFO:Recommend Finished, users: 29444\n"]}]},{"cell_type":"code","source":["!python src/recall/sr_gnn/train/main2.py --task recommend --node_count 117720 \\\n","    --checkpoint_path output/srgnn/online/srgnn_model.ckpt-40284 --item_lookup Datasets/online/srgnn/item_lookup.txt \\\n","    --recommend_output output/online/Candidate/srgnn_recall_v1.txt --session_input Datasets/online/srgnn/test_user_sess.txt --gru_step 2 \\\n","    --hidden_size 256 --batch_size 256 --rec_count 200 --rec_extra_count 50 --has_uid True \\\n","    --max_len 10 --sigma 10 --sq_max_len 5 \\"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-pNvv-Q51Ams","executionInfo":{"status":"ok","timestamp":1638983333828,"user_tz":-480,"elapsed":78779,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"ae5e1a42-c9f4-4d1a-fc70-aeaf15658a8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:10: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n","\n","2021-12-08 17:07:23,392 root:INFO:Data Loaded, Length: 6000ï¼Œ Max Length: 10\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:23: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","2021-12-08 17:07:23,392 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:23: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:24: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","2021-12-08 17:07:23,393 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:24: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:13: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","2021-12-08 17:07:23,410 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:13: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:18: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n","2021-12-08 17:07:23,435 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/layers/ggnn.py:18: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","2021-12-08 17:07:23,465 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.add_weight` method instead.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","2021-12-08 17:07:23,472 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:565: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","2021-12-08 17:07:23,482 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:61: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","2021-12-08 17:07:23,550 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:61: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:62: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","2021-12-08 17:07:23,553 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:62: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","2021-12-08 17:07:23,554 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:63: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:72: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","2021-12-08 17:07:23,571 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:72: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","2021-12-08 17:07:23,578 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","2021-12-08 17:07:23,578 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:75: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","2021-12-08 17:07:23,657 tensorflow:WARNING:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:79: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","2021-12-08 17:07:24,036 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:79: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:81: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","2021-12-08 17:07:24,036 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:81: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","2021-12-08 17:07:24.037162: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX512F\n","2021-12-08 17:07:24.042664: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz\n","2021-12-08 17:07:24.043141: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558443214a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-12-08 17:07:24.043175: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2021-12-08 17:07:24.044885: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-12-08 17:07:24.152669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 17:07:24.153600: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558443214d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2021-12-08 17:07:24.153643: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n","2021-12-08 17:07:24.153834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 17:07:24.154443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n","name: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\n","pciBusID: 0000:00:04.0\n","2021-12-08 17:07:24.154773: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-12-08 17:07:24.156289: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-12-08 17:07:24.157237: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n","2021-12-08 17:07:24.157521: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n","2021-12-08 17:07:24.159105: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n","2021-12-08 17:07:24.159846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n","2021-12-08 17:07:24.163026: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n","2021-12-08 17:07:24.163137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 17:07:24.163791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 17:07:24.164369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n","2021-12-08 17:07:24.164421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n","2021-12-08 17:07:24.165515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2021-12-08 17:07:24.165538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n","2021-12-08 17:07:24.165545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n","2021-12-08 17:07:24.165650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 17:07:24.166284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2021-12-08 17:07:24.166884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15060 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n","WARNING:tensorflow:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:82: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n","\n","2021-12-08 17:07:24,168 tensorflow:WARNING:From /content/drive/My Drive/Msc Project/src/recall/sr_gnn/modeling/SR_GNN.py:82: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n","\n","INFO:tensorflow:Restoring parameters from output/srgnn/online/srgnn_model.ckpt-40284\n","2021-12-08 17:07:25,202 tensorflow:INFO:Restoring parameters from output/srgnn/online/srgnn_model.ckpt-40284\n","2021-12-08 17:07:25,440 src.recall.sr_gnn.train.trainer:INFO:Total Batch: 24\n","2021-12-08 17:07:25.502689: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n","2021-12-08 17:07:29,007 src.recall.sr_gnn.train.trainer:INFO:Batch 0 Finished, users: 256\n","2021-12-08 17:08:39,836 src.recall.sr_gnn.train.trainer:INFO:Recommend Finished, users: 6000\n"]}]},{"cell_type":"markdown","source":["5. Read SR-GNN model output results"],"metadata":{"id":"491NJ14Wukk2"}},{"cell_type":"code","source":["!python src/recall/main.py --task srgnn --mode offline --data_dir Datasets/ \\\\\n","--recall_item_num 200 --metric_recall --save_dir output/offline/Candidate/ \\\\\n","--candidate_file output/offline/Candidate/srgnn_recall_v1.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FU8uz2IOD01R","executionInfo":{"status":"ok","timestamp":1639063539138,"user_tz":-480,"elapsed":71312,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"84423a5a-b8df-49f5-cc5c-11d0db19194f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["read sr-gnn done, num=29444\n"," topk:  10  :  hit_num:  2055 hit_rate:  0.06979 user_num :  29444\n"," topk:  20  :  hit_num:  2759 hit_rate:  0.0937 user_num :  29444\n"," topk:  30  :  hit_num:  3207 hit_rate:  0.10892 user_num :  29444\n"," topk:  40  :  hit_num:  3564 hit_rate:  0.12104 user_num :  29444\n"," topk:  50  :  hit_num:  3874 hit_rate:  0.13157 user_num :  29444\n"," topk:  60  :  hit_num:  4126 hit_rate:  0.14013 user_num :  29444\n"," topk:  70  :  hit_num:  4361 hit_rate:  0.14811 user_num :  29444\n"," topk:  80  :  hit_num:  4560 hit_rate:  0.15487 user_num :  29444\n"," topk:  90  :  hit_num:  4742 hit_rate:  0.16105 user_num :  29444\n"," topk:  100  :  hit_num:  4908 hit_rate:  0.16669 user_num :  29444\n"," topk:  110  :  hit_num:  5053 hit_rate:  0.17161 user_num :  29444\n"," topk:  120  :  hit_num:  5204 hit_rate:  0.17674 user_num :  29444\n"," topk:  130  :  hit_num:  5336 hit_rate:  0.18123 user_num :  29444\n"," topk:  140  :  hit_num:  5467 hit_rate:  0.18567 user_num :  29444\n"," topk:  150  :  hit_num:  5610 hit_rate:  0.19053 user_num :  29444\n"," topk:  160  :  hit_num:  5730 hit_rate:  0.19461 user_num :  29444\n"," topk:  170  :  hit_num:  5819 hit_rate:  0.19763 user_num :  29444\n"," topk:  180  :  hit_num:  5918 hit_rate:  0.20099 user_num :  29444\n"," topk:  190  :  hit_num:  6020 hit_rate:  0.20446 user_num :  29444\n"," topk:  200  :  hit_num:  6119 hit_rate:  0.20782 user_num :  29444\n"]}]},{"cell_type":"code","source":["!python src/recall/main.py --task srgnn --mode online --data_dir Datasets/ \\\\\n","--recall_item_num 200 --metric_recall --save_dir output/online/Candidate/ \\\\\n","--candidate_file output/online/Candidate/srgnn_recall_v1.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eqecomh2Dl91","executionInfo":{"status":"ok","timestamp":1639063550120,"user_tz":-480,"elapsed":10987,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"b329c089-e692-4b61-f860-12f5b945e04d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["read sr-gnn done, num=6000\n"," topk:  10  :  hit_num:  463 hit_rate:  0.07717 user_num :  6000\n"," topk:  20  :  hit_num:  634 hit_rate:  0.10567 user_num :  6000\n"," topk:  30  :  hit_num:  728 hit_rate:  0.12133 user_num :  6000\n"," topk:  40  :  hit_num:  804 hit_rate:  0.134 user_num :  6000\n"," topk:  50  :  hit_num:  874 hit_rate:  0.14567 user_num :  6000\n"," topk:  60  :  hit_num:  933 hit_rate:  0.1555 user_num :  6000\n"," topk:  70  :  hit_num:  979 hit_rate:  0.16317 user_num :  6000\n"," topk:  80  :  hit_num:  1035 hit_rate:  0.1725 user_num :  6000\n"," topk:  90  :  hit_num:  1090 hit_rate:  0.18167 user_num :  6000\n"," topk:  100  :  hit_num:  1125 hit_rate:  0.1875 user_num :  6000\n"," topk:  110  :  hit_num:  1155 hit_rate:  0.1925 user_num :  6000\n"," topk:  120  :  hit_num:  1197 hit_rate:  0.1995 user_num :  6000\n"," topk:  130  :  hit_num:  1231 hit_rate:  0.20517 user_num :  6000\n"," topk:  140  :  hit_num:  1256 hit_rate:  0.20933 user_num :  6000\n"," topk:  150  :  hit_num:  1283 hit_rate:  0.21383 user_num :  6000\n"," topk:  160  :  hit_num:  1306 hit_rate:  0.21767 user_num :  6000\n"," topk:  170  :  hit_num:  1337 hit_rate:  0.22283 user_num :  6000\n"," topk:  180  :  hit_num:  1365 hit_rate:  0.2275 user_num :  6000\n"," topk:  190  :  hit_num:  1388 hit_rate:  0.23133 user_num :  6000\n"," topk:  200  :  hit_num:  1408 hit_rate:  0.23467 user_num :  6000\n"]}]},{"cell_type":"markdown","source":["# Construct GBDT model input data and feature extraction"],"metadata":{"id":"5wJeCC05utOK"}},{"cell_type":"markdown","source":["1. Generate input data"],"metadata":{"id":"_d3KvVQdu1cs"}},{"cell_type":"code","source":["!python src/data_process/generate_rank_input.py --task itemcf"],"metadata":{"id":"hugCvJcnFB8a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639062642067,"user_tz":-480,"elapsed":158049,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"50ce33f4-a7ae-4bd3-c9dd-24033c607ab6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 29444/29444 [00:11<00:00, 2561.83it/s]\n","100% 6000/6000 [00:00<00:00, 16663.79it/s]\n","pos_data_num: 3241 neg_data_num: 5885559 pos/neg: 0.0005506698684016251\n","pos_data_num: 3241 neg_data_num: 146467 pos/neg: 0.022127851324871814\n","Done!\n"]}]},{"cell_type":"code","source":["!python src/data_process/generate_rank_input.py --task usercf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Spl9eifWvLmn","executionInfo":{"status":"ok","timestamp":1639062782367,"user_tz":-480,"elapsed":138825,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"7d9e5a1b-3a9e-4173-d85f-90e264940d3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 29444/29444 [00:06<00:00, 4477.58it/s]\n","100% 6000/6000 [00:00<00:00, 20792.82it/s]\n","pos_data_num: 1500 neg_data_num: 5887300 pos/neg: 0.0002547857252051025\n","pos_data_num: 1500 neg_data_num: 142741 pos/neg: 0.010508543445821453\n","Done!\n"]}]},{"cell_type":"code","source":["!python src/data_process/generate_rank_input.py --task srgnn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6_8Mki6jvNbH","executionInfo":{"status":"ok","timestamp":1639063837658,"user_tz":-480,"elapsed":129824,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"ea15163c-3081-444a-d258-9fdacac29528"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["100% 29444/29444 [00:07<00:00, 4076.01it/s]\n","100% 6000/6000 [00:00<00:00, 20339.45it/s]\n","pos_data_num: 6119 neg_data_num: 5882681 pos/neg: 0.0010401719896081395\n","pos_data_num: 6119 neg_data_num: 133418 pos/neg: 0.04586337675576009\n","Done!\n"]}]},{"cell_type":"markdown","source":["2. feature extraction"],"metadata":{"id":"B9Yq4Vt5vReH"}},{"cell_type":"code","source":["!python src/rank/Word2Vec/train_w2v.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MdPGGNQ2vPnu","executionInfo":{"status":"ok","timestamp":1639062865128,"user_tz":-480,"elapsed":47825,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"6396d464-44b4-4fe7-cec9-7f355770308b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-12-09 15:13:28,544:INFO:collecting all words and their counts\n","2021-12-09 15:13:28,545:INFO:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","2021-12-09 15:13:28,715:INFO:PROGRESS: at sentence #10000, processed 396825 words, keeping 96529 word types\n","2021-12-09 15:13:28,891:INFO:PROGRESS: at sentence #20000, processed 775521 words, keeping 114141 word types\n","2021-12-09 15:13:29,042:INFO:PROGRESS: at sentence #30000, processed 1105926 words, keeping 117605 word types\n","2021-12-09 15:13:29,113:INFO:collected 117720 word types from a corpus of 1257931 raw words and 35444 sentences\n","2021-12-09 15:13:29,113:INFO:Loading a fresh vocabulary\n","2021-12-09 15:13:29,562:INFO:effective_min_count=1 retains 117720 unique words (100% of original 117720, drops 0)\n","2021-12-09 15:13:29,563:INFO:effective_min_count=1 leaves 1257931 word corpus (100% of original 1257931, drops 0)\n","2021-12-09 15:13:29,912:INFO:deleting the raw counts dictionary of 117720 items\n","2021-12-09 15:13:29,915:INFO:sample=0.001 downsamples 0 most-common words\n","2021-12-09 15:13:29,916:INFO:downsampling leaves estimated 1257931 word corpus (100.0% of prior 1257931)\n","2021-12-09 15:13:30,350:INFO:estimated required memory for 117720 words and 16 dimensions: 73928160 bytes\n","2021-12-09 15:13:30,350:INFO:resetting layer weights\n","2021-12-09 15:13:53,678:INFO:training model with 24 workers on 117720 vocabulary and 16 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n","2021-12-09 15:13:54,875:INFO:EPOCH 1 - PROGRESS: at 36.95% examples, 471643 words/s, in_qsize 47, out_qsize 0\n","2021-12-09 15:13:55,700:INFO:worker thread finished; awaiting finish of 23 more threads\n","2021-12-09 15:13:55,724:INFO:worker thread finished; awaiting finish of 22 more threads\n","2021-12-09 15:13:55,770:INFO:worker thread finished; awaiting finish of 21 more threads\n","2021-12-09 15:13:55,794:INFO:worker thread finished; awaiting finish of 20 more threads\n","2021-12-09 15:13:55,797:INFO:worker thread finished; awaiting finish of 19 more threads\n","2021-12-09 15:13:55,813:INFO:worker thread finished; awaiting finish of 18 more threads\n","2021-12-09 15:13:55,814:INFO:worker thread finished; awaiting finish of 17 more threads\n","2021-12-09 15:13:55,816:INFO:worker thread finished; awaiting finish of 16 more threads\n","2021-12-09 15:13:55,859:INFO:worker thread finished; awaiting finish of 15 more threads\n","2021-12-09 15:13:55,859:INFO:worker thread finished; awaiting finish of 14 more threads\n","2021-12-09 15:13:55,880:INFO:EPOCH 1 - PROGRESS: at 87.67% examples, 540014 words/s, in_qsize 13, out_qsize 1\n","2021-12-09 15:13:55,880:INFO:worker thread finished; awaiting finish of 13 more threads\n","2021-12-09 15:13:55,909:INFO:worker thread finished; awaiting finish of 12 more threads\n","2021-12-09 15:13:55,927:INFO:worker thread finished; awaiting finish of 11 more threads\n","2021-12-09 15:13:55,937:INFO:worker thread finished; awaiting finish of 10 more threads\n","2021-12-09 15:13:55,938:INFO:worker thread finished; awaiting finish of 9 more threads\n","2021-12-09 15:13:55,947:INFO:worker thread finished; awaiting finish of 8 more threads\n","2021-12-09 15:13:55,955:INFO:worker thread finished; awaiting finish of 7 more threads\n","2021-12-09 15:13:55,976:INFO:worker thread finished; awaiting finish of 6 more threads\n","2021-12-09 15:13:55,983:INFO:worker thread finished; awaiting finish of 5 more threads\n","2021-12-09 15:13:55,997:INFO:worker thread finished; awaiting finish of 4 more threads\n","2021-12-09 15:13:56,000:INFO:worker thread finished; awaiting finish of 3 more threads\n","2021-12-09 15:13:56,006:INFO:worker thread finished; awaiting finish of 2 more threads\n","2021-12-09 15:13:56,007:INFO:worker thread finished; awaiting finish of 1 more threads\n","2021-12-09 15:13:56,010:INFO:worker thread finished; awaiting finish of 0 more threads\n","2021-12-09 15:13:56,010:INFO:EPOCH - 1 : training on 1257931 raw words (1257931 effective words) took 2.2s, 563517 effective words/s\n","2021-12-09 15:13:57,075:INFO:EPOCH 2 - PROGRESS: at 34.86% examples, 480954 words/s, in_qsize 48, out_qsize 0\n","2021-12-09 15:13:58,048:INFO:worker thread finished; awaiting finish of 23 more threads\n","2021-12-09 15:13:58,048:INFO:worker thread finished; awaiting finish of 22 more threads\n","2021-12-09 15:13:58,071:INFO:worker thread finished; awaiting finish of 21 more threads\n","2021-12-09 15:13:58,071:INFO:worker thread finished; awaiting finish of 20 more threads\n","2021-12-09 15:13:58,071:INFO:worker thread finished; awaiting finish of 19 more threads\n","2021-12-09 15:13:58,071:INFO:worker thread finished; awaiting finish of 18 more threads\n","2021-12-09 15:13:58,137:INFO:EPOCH 2 - PROGRESS: at 83.75% examples, 527611 words/s, in_qsize 17, out_qsize 1\n","2021-12-09 15:13:58,138:INFO:worker thread finished; awaiting finish of 17 more threads\n","2021-12-09 15:13:58,147:INFO:worker thread finished; awaiting finish of 16 more threads\n","2021-12-09 15:13:58,149:INFO:worker thread finished; awaiting finish of 15 more threads\n","2021-12-09 15:13:58,171:INFO:worker thread finished; awaiting finish of 14 more threads\n","2021-12-09 15:13:58,187:INFO:worker thread finished; awaiting finish of 13 more threads\n","2021-12-09 15:13:58,215:INFO:worker thread finished; awaiting finish of 12 more threads\n","2021-12-09 15:13:58,218:INFO:worker thread finished; awaiting finish of 11 more threads\n","2021-12-09 15:13:58,236:INFO:worker thread finished; awaiting finish of 10 more threads\n","2021-12-09 15:13:58,265:INFO:worker thread finished; awaiting finish of 9 more threads\n","2021-12-09 15:13:58,270:INFO:worker thread finished; awaiting finish of 8 more threads\n","2021-12-09 15:13:58,278:INFO:worker thread finished; awaiting finish of 7 more threads\n","2021-12-09 15:13:58,288:INFO:worker thread finished; awaiting finish of 6 more threads\n","2021-12-09 15:13:58,296:INFO:worker thread finished; awaiting finish of 5 more threads\n","2021-12-09 15:13:58,302:INFO:worker thread finished; awaiting finish of 4 more threads\n","2021-12-09 15:13:58,325:INFO:worker thread finished; awaiting finish of 3 more threads\n","2021-12-09 15:13:58,328:INFO:worker thread finished; awaiting finish of 2 more threads\n","2021-12-09 15:13:58,332:INFO:worker thread finished; awaiting finish of 1 more threads\n","2021-12-09 15:13:58,337:INFO:worker thread finished; awaiting finish of 0 more threads\n","2021-12-09 15:13:58,337:INFO:EPOCH - 2 : training on 1257931 raw words (1257931 effective words) took 2.3s, 552639 effective words/s\n","2021-12-09 15:13:59,363:INFO:EPOCH 3 - PROGRESS: at 33.67% examples, 462820 words/s, in_qsize 45, out_qsize 2\n","2021-12-09 15:14:00,300:INFO:worker thread finished; awaiting finish of 23 more threads\n","2021-12-09 15:14:00,326:INFO:worker thread finished; awaiting finish of 22 more threads\n","2021-12-09 15:14:00,407:INFO:EPOCH 3 - PROGRESS: at 80.39% examples, 513835 words/s, in_qsize 21, out_qsize 1\n","2021-12-09 15:14:00,407:INFO:worker thread finished; awaiting finish of 21 more threads\n","2021-12-09 15:14:00,408:INFO:worker thread finished; awaiting finish of 20 more threads\n","2021-12-09 15:14:00,422:INFO:worker thread finished; awaiting finish of 19 more threads\n","2021-12-09 15:14:00,430:INFO:worker thread finished; awaiting finish of 18 more threads\n","2021-12-09 15:14:00,438:INFO:worker thread finished; awaiting finish of 17 more threads\n","2021-12-09 15:14:00,450:INFO:worker thread finished; awaiting finish of 16 more threads\n","2021-12-09 15:14:00,483:INFO:worker thread finished; awaiting finish of 15 more threads\n","2021-12-09 15:14:00,526:INFO:worker thread finished; awaiting finish of 14 more threads\n","2021-12-09 15:14:00,535:INFO:worker thread finished; awaiting finish of 13 more threads\n","2021-12-09 15:14:00,535:INFO:worker thread finished; awaiting finish of 12 more threads\n","2021-12-09 15:14:00,547:INFO:worker thread finished; awaiting finish of 11 more threads\n","2021-12-09 15:14:00,583:INFO:worker thread finished; awaiting finish of 10 more threads\n","2021-12-09 15:14:00,590:INFO:worker thread finished; awaiting finish of 9 more threads\n","2021-12-09 15:14:00,593:INFO:worker thread finished; awaiting finish of 8 more threads\n","2021-12-09 15:14:00,595:INFO:worker thread finished; awaiting finish of 7 more threads\n","2021-12-09 15:14:00,599:INFO:worker thread finished; awaiting finish of 6 more threads\n","2021-12-09 15:14:00,609:INFO:worker thread finished; awaiting finish of 5 more threads\n","2021-12-09 15:14:00,628:INFO:worker thread finished; awaiting finish of 4 more threads\n","2021-12-09 15:14:00,634:INFO:worker thread finished; awaiting finish of 3 more threads\n","2021-12-09 15:14:00,657:INFO:worker thread finished; awaiting finish of 2 more threads\n","2021-12-09 15:14:00,671:INFO:worker thread finished; awaiting finish of 1 more threads\n","2021-12-09 15:14:00,671:INFO:worker thread finished; awaiting finish of 0 more threads\n","2021-12-09 15:14:00,671:INFO:EPOCH - 3 : training on 1257931 raw words (1257931 effective words) took 2.3s, 542428 effective words/s\n","2021-12-09 15:14:00,672:INFO:training on a 3773793 raw words (3773793 effective words) took 7.0s, 539658 effective words/s\n","src/rank/Word2Vec/train_w2v.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  item_w2v_emb_dict = {k: model[k] for k in click_df['item_id']}\n"]}]},{"cell_type":"code","source":["!python src/data_process/generate_rank_feature.py --task itemcf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xac8gVIWwSIX","executionInfo":{"status":"ok","timestamp":1639065685855,"user_tz":-480,"elapsed":1848202,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"eb3826f6-2028-4c10-feca-98811ef71adc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tcmalloc: large alloc 1073741824 bytes == 0x5557882ee000 @  0x7fd3216b02a4 0x55572bdc97d2 0x55572bcb4ae0 0x55572bd1246c 0x55572bd12240 0x55572bd860f3 0x55572bd809ee 0x55572bd806f3 0x55572be4a4c2 0x55572be4a83d 0x55572be4a6e6 0x55572be22163 0x55572be21e0c 0x7fd320499bf7 0x55572be21cea\n","tcmalloc: large alloc 2147483648 bytes == 0x5557e25de000 @  0x7fd3216b02a4 0x55572bdc97d2 0x55572bcb4ae0 0x55572bd1246c 0x55572bd12240 0x55572bd860f3 0x55572bd809ee 0x55572bd806f3 0x55572be4a4c2 0x55572be4a83d 0x55572be4a6e6 0x55572be22163 0x55572be21e0c 0x7fd320499bf7 0x55572be21cea\n","149708it [00:28, 5196.94it/s]\n","149708it [00:24, 6024.71it/s]\n","149708it [01:52, 1334.03it/s]\n","149708it [00:20, 7213.89it/s]\n","tcmalloc: large alloc 2147483648 bytes == 0x5558a7bd0000 @  0x7fd3216b02a4 0x55572bdc97d2 0x55572bcb4ae0 0x55572bd1246c 0x55572bd12240 0x55572bd860f3 0x55572bd809ee 0x55572bd806f3 0x55572be4a4c2 0x55572be4a83d 0x55572be4a6e6 0x55572be22163 0x55572be21e0c 0x7fd320499bf7 0x55572be21cea\n","1200000it [03:26, 5812.43it/s]\n","1200000it [03:10, 6306.46it/s]\n","1200000it [13:24, 1491.47it/s]\n","1200000it [02:42, 7363.44it/s]\n"]}]},{"cell_type":"code","source":["!python src/data_process/generate_rank_feature.py --task usercf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jbcaf6eewSjS","executionInfo":{"status":"ok","timestamp":1639067452823,"user_tz":-480,"elapsed":1766978,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"3aa02744-18d2-483d-a982-e3a61361fb4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tcmalloc: large alloc 1073741824 bytes == 0x55ca2fae8000 @  0x7f3d753462a4 0x55c9d2d317d2 0x55c9d2c1cae0 0x55c9d2c7a46c 0x55c9d2c7a240 0x55c9d2cee0f3 0x55c9d2ce89ee 0x55c9d2ce86f3 0x55c9d2db24c2 0x55c9d2db283d 0x55c9d2db26e6 0x55c9d2d8a163 0x55c9d2d89e0c 0x7f3d7412fbf7 0x55c9d2d89cea\n","tcmalloc: large alloc 2147483648 bytes == 0x55ca89dd8000 @  0x7f3d753462a4 0x55c9d2d317d2 0x55c9d2c1cae0 0x55c9d2c7a46c 0x55c9d2c7a240 0x55c9d2cee0f3 0x55c9d2ce89ee 0x55c9d2ce86f3 0x55c9d2db24c2 0x55c9d2db283d 0x55c9d2db26e6 0x55c9d2d8a163 0x55c9d2d89e0c 0x7f3d7412fbf7 0x55c9d2d89cea\n","144241it [00:26, 5470.86it/s]\n","144241it [00:22, 6419.19it/s]\n","144241it [01:32, 1562.81it/s]\n","144241it [00:19, 7333.59it/s]\n","tcmalloc: large alloc 2147483648 bytes == 0x55cb4f100000 @  0x7f3d753462a4 0x55c9d2d317d2 0x55c9d2c1cae0 0x55c9d2c7a46c 0x55c9d2c7a240 0x55c9d2cee0f3 0x55c9d2ce89ee 0x55c9d2ce86f3 0x55c9d2db24c2 0x55c9d2db283d 0x55c9d2db26e6 0x55c9d2d8a163 0x55c9d2d89e0c 0x7f3d7412fbf7 0x55c9d2d89cea\n","1200000it [03:23, 5905.25it/s]\n","1200000it [03:05, 6453.36it/s]\n","1200000it [13:24, 1491.39it/s]\n","1200000it [02:38, 7549.66it/s]\n"]}]},{"cell_type":"code","source":["!python src/data_process/generate_rank_feature.py --task srgnn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iwtmvUrtwUls","executionInfo":{"status":"ok","timestamp":1639069193573,"user_tz":-480,"elapsed":1740761,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"578a8f9c-e787-4782-9de0-d1fa4d65c7f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tcmalloc: large alloc 1073741824 bytes == 0x5585779e8000 @  0x7f8a09ecc2a4 0x55851adc97d2 0x55851acb4ae0 0x55851ad1246c 0x55851ad12240 0x55851ad860f3 0x55851ad809ee 0x55851ad806f3 0x55851ae4a4c2 0x55851ae4a83d 0x55851ae4a6e6 0x55851ae22163 0x55851ae21e0c 0x7f8a08cb5bf7 0x55851ae21cea\n","tcmalloc: large alloc 2147483648 bytes == 0x5585d1cd8000 @  0x7f8a09ecc2a4 0x55851adc97d2 0x55851acb4ae0 0x55851ad1246c 0x55851ad12240 0x55851ad860f3 0x55851ad809ee 0x55851ad806f3 0x55851ae4a4c2 0x55851ae4a83d 0x55851ae4a6e6 0x55851ae22163 0x55851ae21e0c 0x7f8a08cb5bf7 0x55851ae21cea\n","139537it [00:26, 5303.33it/s]\n","139537it [00:22, 6324.78it/s]\n","139537it [01:33, 1489.81it/s]\n","139537it [00:18, 7348.56it/s]\n","tcmalloc: large alloc 2147483648 bytes == 0x558696df8000 @  0x7f8a09ecc2a4 0x55851adc97d2 0x55851acb4ae0 0x55851ad1246c 0x55851ad12240 0x55851ad860f3 0x55851ad809ee 0x55851ad806f3 0x55851ae4a4c2 0x55851ae4a83d 0x55851ae4a6e6 0x55851ae22163 0x55851ae21e0c 0x7f8a08cb5bf7 0x55851ae21cea\n","1200000it [03:24, 5874.13it/s]\n","1200000it [03:03, 6552.76it/s]\n","1200000it [13:01, 1535.19it/s]\n","1200000it [02:35, 7704.43it/s]\n"]}]},{"cell_type":"markdown","source":["# Use GBDT model to rank the candidate set"],"metadata":{"id":"TYyOJgN53S7U"}},{"cell_type":"markdown","source":["1. Use the LGBRanker model to sort the candidate set returned by each model"],"metadata":{"id":"FYe1dls63fn6"}},{"cell_type":"code","source":["!python src/rank/GBDT_rank.py --task itemcf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OyEylQGq3Spe","executionInfo":{"status":"ok","timestamp":1639076235566,"user_tz":-480,"elapsed":40873,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"7d3d3e77-e3d0-409a-fad2-f658d00bac45"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["[1]\tvalid_0's ndcg@10: 0.978355\tvalid_0's ndcg@20: 0.978355\tvalid_0's ndcg@30: 0.978355\tvalid_0's ndcg@40: 0.978355\tvalid_0's ndcg@50: 0.978355\tvalid_0's ndcg@10: 0.978355\tvalid_0's ndcg@20: 0.978355\tvalid_0's ndcg@30: 0.978355\tvalid_0's ndcg@40: 0.978355\tvalid_0's ndcg@50: 0.978355\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.978294\tvalid_0's ndcg@20: 0.978294\tvalid_0's ndcg@30: 0.978294\tvalid_0's ndcg@40: 0.978294\tvalid_0's ndcg@50: 0.978294\tvalid_0's ndcg@10: 0.978294\tvalid_0's ndcg@20: 0.978294\tvalid_0's ndcg@30: 0.978294\tvalid_0's ndcg@40: 0.978294\tvalid_0's ndcg@50: 0.978294\n","[3]\tvalid_0's ndcg@10: 0.980734\tvalid_0's ndcg@20: 0.980734\tvalid_0's ndcg@30: 0.980734\tvalid_0's ndcg@40: 0.980734\tvalid_0's ndcg@50: 0.980734\tvalid_0's ndcg@10: 0.980734\tvalid_0's ndcg@20: 0.980734\tvalid_0's ndcg@30: 0.980734\tvalid_0's ndcg@40: 0.980734\tvalid_0's ndcg@50: 0.980734\n","[4]\tvalid_0's ndcg@10: 0.981138\tvalid_0's ndcg@20: 0.981138\tvalid_0's ndcg@30: 0.981138\tvalid_0's ndcg@40: 0.981138\tvalid_0's ndcg@50: 0.981138\tvalid_0's ndcg@10: 0.981138\tvalid_0's ndcg@20: 0.981138\tvalid_0's ndcg@30: 0.981138\tvalid_0's ndcg@40: 0.981138\tvalid_0's ndcg@50: 0.981138\n","[5]\tvalid_0's ndcg@10: 0.98147\tvalid_0's ndcg@20: 0.98147\tvalid_0's ndcg@30: 0.98147\tvalid_0's ndcg@40: 0.98147\tvalid_0's ndcg@50: 0.98147\tvalid_0's ndcg@10: 0.98147\tvalid_0's ndcg@20: 0.98147\tvalid_0's ndcg@30: 0.98147\tvalid_0's ndcg@40: 0.98147\tvalid_0's ndcg@50: 0.98147\n","[6]\tvalid_0's ndcg@10: 0.981678\tvalid_0's ndcg@20: 0.981678\tvalid_0's ndcg@30: 0.981678\tvalid_0's ndcg@40: 0.981678\tvalid_0's ndcg@50: 0.981678\tvalid_0's ndcg@10: 0.981678\tvalid_0's ndcg@20: 0.981678\tvalid_0's ndcg@30: 0.981678\tvalid_0's ndcg@40: 0.981678\tvalid_0's ndcg@50: 0.981678\n","[7]\tvalid_0's ndcg@10: 0.981952\tvalid_0's ndcg@20: 0.981952\tvalid_0's ndcg@30: 0.981952\tvalid_0's ndcg@40: 0.981952\tvalid_0's ndcg@50: 0.981952\tvalid_0's ndcg@10: 0.981952\tvalid_0's ndcg@20: 0.981952\tvalid_0's ndcg@30: 0.981952\tvalid_0's ndcg@40: 0.981952\tvalid_0's ndcg@50: 0.981952\n","[8]\tvalid_0's ndcg@10: 0.981926\tvalid_0's ndcg@20: 0.981926\tvalid_0's ndcg@30: 0.981926\tvalid_0's ndcg@40: 0.981926\tvalid_0's ndcg@50: 0.981926\tvalid_0's ndcg@10: 0.981926\tvalid_0's ndcg@20: 0.981926\tvalid_0's ndcg@30: 0.981926\tvalid_0's ndcg@40: 0.981926\tvalid_0's ndcg@50: 0.981926\n","[9]\tvalid_0's ndcg@10: 0.981782\tvalid_0's ndcg@20: 0.981782\tvalid_0's ndcg@30: 0.981782\tvalid_0's ndcg@40: 0.981782\tvalid_0's ndcg@50: 0.981782\tvalid_0's ndcg@10: 0.981782\tvalid_0's ndcg@20: 0.981782\tvalid_0's ndcg@30: 0.981782\tvalid_0's ndcg@40: 0.981782\tvalid_0's ndcg@50: 0.981782\n","[10]\tvalid_0's ndcg@10: 0.981709\tvalid_0's ndcg@20: 0.981709\tvalid_0's ndcg@30: 0.981709\tvalid_0's ndcg@40: 0.981709\tvalid_0's ndcg@50: 0.981709\tvalid_0's ndcg@10: 0.981709\tvalid_0's ndcg@20: 0.981709\tvalid_0's ndcg@30: 0.981709\tvalid_0's ndcg@40: 0.981709\tvalid_0's ndcg@50: 0.981709\n","[11]\tvalid_0's ndcg@10: 0.981464\tvalid_0's ndcg@20: 0.981464\tvalid_0's ndcg@30: 0.981464\tvalid_0's ndcg@40: 0.981464\tvalid_0's ndcg@50: 0.981464\tvalid_0's ndcg@10: 0.981464\tvalid_0's ndcg@20: 0.981464\tvalid_0's ndcg@30: 0.981464\tvalid_0's ndcg@40: 0.981464\tvalid_0's ndcg@50: 0.981464\n","[12]\tvalid_0's ndcg@10: 0.981448\tvalid_0's ndcg@20: 0.981448\tvalid_0's ndcg@30: 0.981448\tvalid_0's ndcg@40: 0.981448\tvalid_0's ndcg@50: 0.981448\tvalid_0's ndcg@10: 0.981448\tvalid_0's ndcg@20: 0.981448\tvalid_0's ndcg@30: 0.981448\tvalid_0's ndcg@40: 0.981448\tvalid_0's ndcg@50: 0.981448\n","[13]\tvalid_0's ndcg@10: 0.981689\tvalid_0's ndcg@20: 0.981689\tvalid_0's ndcg@30: 0.981689\tvalid_0's ndcg@40: 0.981689\tvalid_0's ndcg@50: 0.981689\tvalid_0's ndcg@10: 0.981689\tvalid_0's ndcg@20: 0.981689\tvalid_0's ndcg@30: 0.981689\tvalid_0's ndcg@40: 0.981689\tvalid_0's ndcg@50: 0.981689\n","[14]\tvalid_0's ndcg@10: 0.982052\tvalid_0's ndcg@20: 0.982052\tvalid_0's ndcg@30: 0.982052\tvalid_0's ndcg@40: 0.982052\tvalid_0's ndcg@50: 0.982052\tvalid_0's ndcg@10: 0.982052\tvalid_0's ndcg@20: 0.982052\tvalid_0's ndcg@30: 0.982052\tvalid_0's ndcg@40: 0.982052\tvalid_0's ndcg@50: 0.982052\n","[15]\tvalid_0's ndcg@10: 0.981659\tvalid_0's ndcg@20: 0.981659\tvalid_0's ndcg@30: 0.981659\tvalid_0's ndcg@40: 0.981659\tvalid_0's ndcg@50: 0.981659\tvalid_0's ndcg@10: 0.981659\tvalid_0's ndcg@20: 0.981659\tvalid_0's ndcg@30: 0.981659\tvalid_0's ndcg@40: 0.981659\tvalid_0's ndcg@50: 0.981659\n","[16]\tvalid_0's ndcg@10: 0.981915\tvalid_0's ndcg@20: 0.981915\tvalid_0's ndcg@30: 0.981915\tvalid_0's ndcg@40: 0.981915\tvalid_0's ndcg@50: 0.981915\tvalid_0's ndcg@10: 0.981915\tvalid_0's ndcg@20: 0.981915\tvalid_0's ndcg@30: 0.981915\tvalid_0's ndcg@40: 0.981915\tvalid_0's ndcg@50: 0.981915\n","[17]\tvalid_0's ndcg@10: 0.981893\tvalid_0's ndcg@20: 0.981893\tvalid_0's ndcg@30: 0.981893\tvalid_0's ndcg@40: 0.981893\tvalid_0's ndcg@50: 0.981893\tvalid_0's ndcg@10: 0.981893\tvalid_0's ndcg@20: 0.981893\tvalid_0's ndcg@30: 0.981893\tvalid_0's ndcg@40: 0.981893\tvalid_0's ndcg@50: 0.981893\n","[18]\tvalid_0's ndcg@10: 0.982083\tvalid_0's ndcg@20: 0.982083\tvalid_0's ndcg@30: 0.982083\tvalid_0's ndcg@40: 0.982083\tvalid_0's ndcg@50: 0.982083\tvalid_0's ndcg@10: 0.982083\tvalid_0's ndcg@20: 0.982083\tvalid_0's ndcg@30: 0.982083\tvalid_0's ndcg@40: 0.982083\tvalid_0's ndcg@50: 0.982083\n","[19]\tvalid_0's ndcg@10: 0.982041\tvalid_0's ndcg@20: 0.982041\tvalid_0's ndcg@30: 0.982041\tvalid_0's ndcg@40: 0.982041\tvalid_0's ndcg@50: 0.982041\tvalid_0's ndcg@10: 0.982041\tvalid_0's ndcg@20: 0.982041\tvalid_0's ndcg@30: 0.982041\tvalid_0's ndcg@40: 0.982041\tvalid_0's ndcg@50: 0.982041\n","[20]\tvalid_0's ndcg@10: 0.982155\tvalid_0's ndcg@20: 0.982155\tvalid_0's ndcg@30: 0.982155\tvalid_0's ndcg@40: 0.982155\tvalid_0's ndcg@50: 0.982155\tvalid_0's ndcg@10: 0.982155\tvalid_0's ndcg@20: 0.982155\tvalid_0's ndcg@30: 0.982155\tvalid_0's ndcg@40: 0.982155\tvalid_0's ndcg@50: 0.982155\n","[21]\tvalid_0's ndcg@10: 0.981968\tvalid_0's ndcg@20: 0.981968\tvalid_0's ndcg@30: 0.981968\tvalid_0's ndcg@40: 0.981968\tvalid_0's ndcg@50: 0.981968\tvalid_0's ndcg@10: 0.981968\tvalid_0's ndcg@20: 0.981968\tvalid_0's ndcg@30: 0.981968\tvalid_0's ndcg@40: 0.981968\tvalid_0's ndcg@50: 0.981968\n","[22]\tvalid_0's ndcg@10: 0.982063\tvalid_0's ndcg@20: 0.982063\tvalid_0's ndcg@30: 0.982063\tvalid_0's ndcg@40: 0.982063\tvalid_0's ndcg@50: 0.982063\tvalid_0's ndcg@10: 0.982063\tvalid_0's ndcg@20: 0.982063\tvalid_0's ndcg@30: 0.982063\tvalid_0's ndcg@40: 0.982063\tvalid_0's ndcg@50: 0.982063\n","[23]\tvalid_0's ndcg@10: 0.981987\tvalid_0's ndcg@20: 0.981987\tvalid_0's ndcg@30: 0.981987\tvalid_0's ndcg@40: 0.981987\tvalid_0's ndcg@50: 0.981987\tvalid_0's ndcg@10: 0.981987\tvalid_0's ndcg@20: 0.981987\tvalid_0's ndcg@30: 0.981987\tvalid_0's ndcg@40: 0.981987\tvalid_0's ndcg@50: 0.981987\n","[24]\tvalid_0's ndcg@10: 0.981933\tvalid_0's ndcg@20: 0.981933\tvalid_0's ndcg@30: 0.981933\tvalid_0's ndcg@40: 0.981933\tvalid_0's ndcg@50: 0.981933\tvalid_0's ndcg@10: 0.981933\tvalid_0's ndcg@20: 0.981933\tvalid_0's ndcg@30: 0.981933\tvalid_0's ndcg@40: 0.981933\tvalid_0's ndcg@50: 0.981933\n","[25]\tvalid_0's ndcg@10: 0.981989\tvalid_0's ndcg@20: 0.981989\tvalid_0's ndcg@30: 0.981989\tvalid_0's ndcg@40: 0.981989\tvalid_0's ndcg@50: 0.981989\tvalid_0's ndcg@10: 0.981989\tvalid_0's ndcg@20: 0.981989\tvalid_0's ndcg@30: 0.981989\tvalid_0's ndcg@40: 0.981989\tvalid_0's ndcg@50: 0.981989\n","[26]\tvalid_0's ndcg@10: 0.981932\tvalid_0's ndcg@20: 0.981932\tvalid_0's ndcg@30: 0.981932\tvalid_0's ndcg@40: 0.981932\tvalid_0's ndcg@50: 0.981932\tvalid_0's ndcg@10: 0.981932\tvalid_0's ndcg@20: 0.981932\tvalid_0's ndcg@30: 0.981932\tvalid_0's ndcg@40: 0.981932\tvalid_0's ndcg@50: 0.981932\n","[27]\tvalid_0's ndcg@10: 0.981884\tvalid_0's ndcg@20: 0.981884\tvalid_0's ndcg@30: 0.981884\tvalid_0's ndcg@40: 0.981884\tvalid_0's ndcg@50: 0.981884\tvalid_0's ndcg@10: 0.981884\tvalid_0's ndcg@20: 0.981884\tvalid_0's ndcg@30: 0.981884\tvalid_0's ndcg@40: 0.981884\tvalid_0's ndcg@50: 0.981884\n","[28]\tvalid_0's ndcg@10: 0.982022\tvalid_0's ndcg@20: 0.982022\tvalid_0's ndcg@30: 0.982022\tvalid_0's ndcg@40: 0.982022\tvalid_0's ndcg@50: 0.982022\tvalid_0's ndcg@10: 0.982022\tvalid_0's ndcg@20: 0.982022\tvalid_0's ndcg@30: 0.982022\tvalid_0's ndcg@40: 0.982022\tvalid_0's ndcg@50: 0.982022\n","[29]\tvalid_0's ndcg@10: 0.981834\tvalid_0's ndcg@20: 0.981834\tvalid_0's ndcg@30: 0.981834\tvalid_0's ndcg@40: 0.981834\tvalid_0's ndcg@50: 0.981834\tvalid_0's ndcg@10: 0.981834\tvalid_0's ndcg@20: 0.981834\tvalid_0's ndcg@30: 0.981834\tvalid_0's ndcg@40: 0.981834\tvalid_0's ndcg@50: 0.981834\n","[30]\tvalid_0's ndcg@10: 0.981846\tvalid_0's ndcg@20: 0.981846\tvalid_0's ndcg@30: 0.981846\tvalid_0's ndcg@40: 0.981846\tvalid_0's ndcg@50: 0.981846\tvalid_0's ndcg@10: 0.981846\tvalid_0's ndcg@20: 0.981846\tvalid_0's ndcg@30: 0.981846\tvalid_0's ndcg@40: 0.981846\tvalid_0's ndcg@50: 0.981846\n","[31]\tvalid_0's ndcg@10: 0.981893\tvalid_0's ndcg@20: 0.981893\tvalid_0's ndcg@30: 0.981893\tvalid_0's ndcg@40: 0.981893\tvalid_0's ndcg@50: 0.981893\tvalid_0's ndcg@10: 0.981893\tvalid_0's ndcg@20: 0.981893\tvalid_0's ndcg@30: 0.981893\tvalid_0's ndcg@40: 0.981893\tvalid_0's ndcg@50: 0.981893\n","[32]\tvalid_0's ndcg@10: 0.981934\tvalid_0's ndcg@20: 0.981934\tvalid_0's ndcg@30: 0.981934\tvalid_0's ndcg@40: 0.981934\tvalid_0's ndcg@50: 0.981934\tvalid_0's ndcg@10: 0.981934\tvalid_0's ndcg@20: 0.981934\tvalid_0's ndcg@30: 0.981934\tvalid_0's ndcg@40: 0.981934\tvalid_0's ndcg@50: 0.981934\n","[33]\tvalid_0's ndcg@10: 0.982102\tvalid_0's ndcg@20: 0.982102\tvalid_0's ndcg@30: 0.982102\tvalid_0's ndcg@40: 0.982102\tvalid_0's ndcg@50: 0.982102\tvalid_0's ndcg@10: 0.982102\tvalid_0's ndcg@20: 0.982102\tvalid_0's ndcg@30: 0.982102\tvalid_0's ndcg@40: 0.982102\tvalid_0's ndcg@50: 0.982102\n","[34]\tvalid_0's ndcg@10: 0.981985\tvalid_0's ndcg@20: 0.981985\tvalid_0's ndcg@30: 0.981985\tvalid_0's ndcg@40: 0.981985\tvalid_0's ndcg@50: 0.981985\tvalid_0's ndcg@10: 0.981985\tvalid_0's ndcg@20: 0.981985\tvalid_0's ndcg@30: 0.981985\tvalid_0's ndcg@40: 0.981985\tvalid_0's ndcg@50: 0.981985\n","[35]\tvalid_0's ndcg@10: 0.981925\tvalid_0's ndcg@20: 0.981925\tvalid_0's ndcg@30: 0.981925\tvalid_0's ndcg@40: 0.981925\tvalid_0's ndcg@50: 0.981925\tvalid_0's ndcg@10: 0.981925\tvalid_0's ndcg@20: 0.981925\tvalid_0's ndcg@30: 0.981925\tvalid_0's ndcg@40: 0.981925\tvalid_0's ndcg@50: 0.981925\n","[36]\tvalid_0's ndcg@10: 0.981996\tvalid_0's ndcg@20: 0.981996\tvalid_0's ndcg@30: 0.981996\tvalid_0's ndcg@40: 0.981996\tvalid_0's ndcg@50: 0.981996\tvalid_0's ndcg@10: 0.981996\tvalid_0's ndcg@20: 0.981996\tvalid_0's ndcg@30: 0.981996\tvalid_0's ndcg@40: 0.981996\tvalid_0's ndcg@50: 0.981996\n","[37]\tvalid_0's ndcg@10: 0.982081\tvalid_0's ndcg@20: 0.982081\tvalid_0's ndcg@30: 0.982081\tvalid_0's ndcg@40: 0.982081\tvalid_0's ndcg@50: 0.982081\tvalid_0's ndcg@10: 0.982081\tvalid_0's ndcg@20: 0.982081\tvalid_0's ndcg@30: 0.982081\tvalid_0's ndcg@40: 0.982081\tvalid_0's ndcg@50: 0.982081\n","[38]\tvalid_0's ndcg@10: 0.982141\tvalid_0's ndcg@20: 0.982141\tvalid_0's ndcg@30: 0.982141\tvalid_0's ndcg@40: 0.982141\tvalid_0's ndcg@50: 0.982141\tvalid_0's ndcg@10: 0.982141\tvalid_0's ndcg@20: 0.982141\tvalid_0's ndcg@30: 0.982141\tvalid_0's ndcg@40: 0.982141\tvalid_0's ndcg@50: 0.982141\n","[39]\tvalid_0's ndcg@10: 0.981991\tvalid_0's ndcg@20: 0.981991\tvalid_0's ndcg@30: 0.981991\tvalid_0's ndcg@40: 0.981991\tvalid_0's ndcg@50: 0.981991\tvalid_0's ndcg@10: 0.981991\tvalid_0's ndcg@20: 0.981991\tvalid_0's ndcg@30: 0.981991\tvalid_0's ndcg@40: 0.981991\tvalid_0's ndcg@50: 0.981991\n","[40]\tvalid_0's ndcg@10: 0.982027\tvalid_0's ndcg@20: 0.982027\tvalid_0's ndcg@30: 0.982027\tvalid_0's ndcg@40: 0.982027\tvalid_0's ndcg@50: 0.982027\tvalid_0's ndcg@10: 0.982027\tvalid_0's ndcg@20: 0.982027\tvalid_0's ndcg@30: 0.982027\tvalid_0's ndcg@40: 0.982027\tvalid_0's ndcg@50: 0.982027\n","[41]\tvalid_0's ndcg@10: 0.982176\tvalid_0's ndcg@20: 0.982176\tvalid_0's ndcg@30: 0.982176\tvalid_0's ndcg@40: 0.982176\tvalid_0's ndcg@50: 0.982176\tvalid_0's ndcg@10: 0.982176\tvalid_0's ndcg@20: 0.982176\tvalid_0's ndcg@30: 0.982176\tvalid_0's ndcg@40: 0.982176\tvalid_0's ndcg@50: 0.982176\n","[42]\tvalid_0's ndcg@10: 0.981905\tvalid_0's ndcg@20: 0.981905\tvalid_0's ndcg@30: 0.981905\tvalid_0's ndcg@40: 0.981905\tvalid_0's ndcg@50: 0.981905\tvalid_0's ndcg@10: 0.981905\tvalid_0's ndcg@20: 0.981905\tvalid_0's ndcg@30: 0.981905\tvalid_0's ndcg@40: 0.981905\tvalid_0's ndcg@50: 0.981905\n","[43]\tvalid_0's ndcg@10: 0.981865\tvalid_0's ndcg@20: 0.981865\tvalid_0's ndcg@30: 0.981865\tvalid_0's ndcg@40: 0.981865\tvalid_0's ndcg@50: 0.981865\tvalid_0's ndcg@10: 0.981865\tvalid_0's ndcg@20: 0.981865\tvalid_0's ndcg@30: 0.981865\tvalid_0's ndcg@40: 0.981865\tvalid_0's ndcg@50: 0.981865\n","[44]\tvalid_0's ndcg@10: 0.981894\tvalid_0's ndcg@20: 0.981894\tvalid_0's ndcg@30: 0.981894\tvalid_0's ndcg@40: 0.981894\tvalid_0's ndcg@50: 0.981894\tvalid_0's ndcg@10: 0.981894\tvalid_0's ndcg@20: 0.981894\tvalid_0's ndcg@30: 0.981894\tvalid_0's ndcg@40: 0.981894\tvalid_0's ndcg@50: 0.981894\n","[45]\tvalid_0's ndcg@10: 0.982013\tvalid_0's ndcg@20: 0.982013\tvalid_0's ndcg@30: 0.982013\tvalid_0's ndcg@40: 0.982013\tvalid_0's ndcg@50: 0.982013\tvalid_0's ndcg@10: 0.982013\tvalid_0's ndcg@20: 0.982013\tvalid_0's ndcg@30: 0.982013\tvalid_0's ndcg@40: 0.982013\tvalid_0's ndcg@50: 0.982013\n","[46]\tvalid_0's ndcg@10: 0.982056\tvalid_0's ndcg@20: 0.982056\tvalid_0's ndcg@30: 0.982056\tvalid_0's ndcg@40: 0.982056\tvalid_0's ndcg@50: 0.982056\tvalid_0's ndcg@10: 0.982056\tvalid_0's ndcg@20: 0.982056\tvalid_0's ndcg@30: 0.982056\tvalid_0's ndcg@40: 0.982056\tvalid_0's ndcg@50: 0.982056\n","[47]\tvalid_0's ndcg@10: 0.981932\tvalid_0's ndcg@20: 0.981932\tvalid_0's ndcg@30: 0.981932\tvalid_0's ndcg@40: 0.981932\tvalid_0's ndcg@50: 0.981932\tvalid_0's ndcg@10: 0.981932\tvalid_0's ndcg@20: 0.981932\tvalid_0's ndcg@30: 0.981932\tvalid_0's ndcg@40: 0.981932\tvalid_0's ndcg@50: 0.981932\n","[48]\tvalid_0's ndcg@10: 0.981807\tvalid_0's ndcg@20: 0.981807\tvalid_0's ndcg@30: 0.981807\tvalid_0's ndcg@40: 0.981807\tvalid_0's ndcg@50: 0.981807\tvalid_0's ndcg@10: 0.981807\tvalid_0's ndcg@20: 0.981807\tvalid_0's ndcg@30: 0.981807\tvalid_0's ndcg@40: 0.981807\tvalid_0's ndcg@50: 0.981807\n","[49]\tvalid_0's ndcg@10: 0.981809\tvalid_0's ndcg@20: 0.981809\tvalid_0's ndcg@30: 0.981809\tvalid_0's ndcg@40: 0.981809\tvalid_0's ndcg@50: 0.981809\tvalid_0's ndcg@10: 0.981809\tvalid_0's ndcg@20: 0.981809\tvalid_0's ndcg@30: 0.981809\tvalid_0's ndcg@40: 0.981809\tvalid_0's ndcg@50: 0.981809\n","[50]\tvalid_0's ndcg@10: 0.981957\tvalid_0's ndcg@20: 0.981957\tvalid_0's ndcg@30: 0.981957\tvalid_0's ndcg@40: 0.981957\tvalid_0's ndcg@50: 0.981957\tvalid_0's ndcg@10: 0.981957\tvalid_0's ndcg@20: 0.981957\tvalid_0's ndcg@30: 0.981957\tvalid_0's ndcg@40: 0.981957\tvalid_0's ndcg@50: 0.981957\n","[51]\tvalid_0's ndcg@10: 0.981885\tvalid_0's ndcg@20: 0.981885\tvalid_0's ndcg@30: 0.981885\tvalid_0's ndcg@40: 0.981885\tvalid_0's ndcg@50: 0.981885\tvalid_0's ndcg@10: 0.981885\tvalid_0's ndcg@20: 0.981885\tvalid_0's ndcg@30: 0.981885\tvalid_0's ndcg@40: 0.981885\tvalid_0's ndcg@50: 0.981885\n","[52]\tvalid_0's ndcg@10: 0.981901\tvalid_0's ndcg@20: 0.981901\tvalid_0's ndcg@30: 0.981901\tvalid_0's ndcg@40: 0.981901\tvalid_0's ndcg@50: 0.981901\tvalid_0's ndcg@10: 0.981901\tvalid_0's ndcg@20: 0.981901\tvalid_0's ndcg@30: 0.981901\tvalid_0's ndcg@40: 0.981901\tvalid_0's ndcg@50: 0.981901\n","[53]\tvalid_0's ndcg@10: 0.981863\tvalid_0's ndcg@20: 0.981863\tvalid_0's ndcg@30: 0.981863\tvalid_0's ndcg@40: 0.981863\tvalid_0's ndcg@50: 0.981863\tvalid_0's ndcg@10: 0.981863\tvalid_0's ndcg@20: 0.981863\tvalid_0's ndcg@30: 0.981863\tvalid_0's ndcg@40: 0.981863\tvalid_0's ndcg@50: 0.981863\n","[54]\tvalid_0's ndcg@10: 0.98177\tvalid_0's ndcg@20: 0.98177\tvalid_0's ndcg@30: 0.98177\tvalid_0's ndcg@40: 0.98177\tvalid_0's ndcg@50: 0.98177\tvalid_0's ndcg@10: 0.98177\tvalid_0's ndcg@20: 0.98177\tvalid_0's ndcg@30: 0.98177\tvalid_0's ndcg@40: 0.98177\tvalid_0's ndcg@50: 0.98177\n","[55]\tvalid_0's ndcg@10: 0.981851\tvalid_0's ndcg@20: 0.981851\tvalid_0's ndcg@30: 0.981851\tvalid_0's ndcg@40: 0.981851\tvalid_0's ndcg@50: 0.981851\tvalid_0's ndcg@10: 0.981851\tvalid_0's ndcg@20: 0.981851\tvalid_0's ndcg@30: 0.981851\tvalid_0's ndcg@40: 0.981851\tvalid_0's ndcg@50: 0.981851\n","[56]\tvalid_0's ndcg@10: 0.981855\tvalid_0's ndcg@20: 0.981855\tvalid_0's ndcg@30: 0.981855\tvalid_0's ndcg@40: 0.981855\tvalid_0's ndcg@50: 0.981855\tvalid_0's ndcg@10: 0.981855\tvalid_0's ndcg@20: 0.981855\tvalid_0's ndcg@30: 0.981855\tvalid_0's ndcg@40: 0.981855\tvalid_0's ndcg@50: 0.981855\n","[57]\tvalid_0's ndcg@10: 0.98182\tvalid_0's ndcg@20: 0.98182\tvalid_0's ndcg@30: 0.98182\tvalid_0's ndcg@40: 0.98182\tvalid_0's ndcg@50: 0.98182\tvalid_0's ndcg@10: 0.98182\tvalid_0's ndcg@20: 0.98182\tvalid_0's ndcg@30: 0.98182\tvalid_0's ndcg@40: 0.98182\tvalid_0's ndcg@50: 0.98182\n","[58]\tvalid_0's ndcg@10: 0.981868\tvalid_0's ndcg@20: 0.981868\tvalid_0's ndcg@30: 0.981868\tvalid_0's ndcg@40: 0.981868\tvalid_0's ndcg@50: 0.981868\tvalid_0's ndcg@10: 0.981868\tvalid_0's ndcg@20: 0.981868\tvalid_0's ndcg@30: 0.981868\tvalid_0's ndcg@40: 0.981868\tvalid_0's ndcg@50: 0.981868\n","[59]\tvalid_0's ndcg@10: 0.982034\tvalid_0's ndcg@20: 0.982034\tvalid_0's ndcg@30: 0.982034\tvalid_0's ndcg@40: 0.982034\tvalid_0's ndcg@50: 0.982034\tvalid_0's ndcg@10: 0.982034\tvalid_0's ndcg@20: 0.982034\tvalid_0's ndcg@30: 0.982034\tvalid_0's ndcg@40: 0.982034\tvalid_0's ndcg@50: 0.982034\n","[60]\tvalid_0's ndcg@10: 0.982016\tvalid_0's ndcg@20: 0.982016\tvalid_0's ndcg@30: 0.982016\tvalid_0's ndcg@40: 0.982016\tvalid_0's ndcg@50: 0.982016\tvalid_0's ndcg@10: 0.982016\tvalid_0's ndcg@20: 0.982016\tvalid_0's ndcg@30: 0.982016\tvalid_0's ndcg@40: 0.982016\tvalid_0's ndcg@50: 0.982016\n","[61]\tvalid_0's ndcg@10: 0.982123\tvalid_0's ndcg@20: 0.982123\tvalid_0's ndcg@30: 0.982123\tvalid_0's ndcg@40: 0.982123\tvalid_0's ndcg@50: 0.982123\tvalid_0's ndcg@10: 0.982123\tvalid_0's ndcg@20: 0.982123\tvalid_0's ndcg@30: 0.982123\tvalid_0's ndcg@40: 0.982123\tvalid_0's ndcg@50: 0.982123\n","[62]\tvalid_0's ndcg@10: 0.982152\tvalid_0's ndcg@20: 0.982152\tvalid_0's ndcg@30: 0.982152\tvalid_0's ndcg@40: 0.982152\tvalid_0's ndcg@50: 0.982152\tvalid_0's ndcg@10: 0.982152\tvalid_0's ndcg@20: 0.982152\tvalid_0's ndcg@30: 0.982152\tvalid_0's ndcg@40: 0.982152\tvalid_0's ndcg@50: 0.982152\n","[63]\tvalid_0's ndcg@10: 0.982194\tvalid_0's ndcg@20: 0.982194\tvalid_0's ndcg@30: 0.982194\tvalid_0's ndcg@40: 0.982194\tvalid_0's ndcg@50: 0.982194\tvalid_0's ndcg@10: 0.982194\tvalid_0's ndcg@20: 0.982194\tvalid_0's ndcg@30: 0.982194\tvalid_0's ndcg@40: 0.982194\tvalid_0's ndcg@50: 0.982194\n","[64]\tvalid_0's ndcg@10: 0.982185\tvalid_0's ndcg@20: 0.982185\tvalid_0's ndcg@30: 0.982185\tvalid_0's ndcg@40: 0.982185\tvalid_0's ndcg@50: 0.982185\tvalid_0's ndcg@10: 0.982185\tvalid_0's ndcg@20: 0.982185\tvalid_0's ndcg@30: 0.982185\tvalid_0's ndcg@40: 0.982185\tvalid_0's ndcg@50: 0.982185\n","[65]\tvalid_0's ndcg@10: 0.982163\tvalid_0's ndcg@20: 0.982163\tvalid_0's ndcg@30: 0.982163\tvalid_0's ndcg@40: 0.982163\tvalid_0's ndcg@50: 0.982163\tvalid_0's ndcg@10: 0.982163\tvalid_0's ndcg@20: 0.982163\tvalid_0's ndcg@30: 0.982163\tvalid_0's ndcg@40: 0.982163\tvalid_0's ndcg@50: 0.982163\n","[66]\tvalid_0's ndcg@10: 0.982333\tvalid_0's ndcg@20: 0.982333\tvalid_0's ndcg@30: 0.982333\tvalid_0's ndcg@40: 0.982333\tvalid_0's ndcg@50: 0.982333\tvalid_0's ndcg@10: 0.982333\tvalid_0's ndcg@20: 0.982333\tvalid_0's ndcg@30: 0.982333\tvalid_0's ndcg@40: 0.982333\tvalid_0's ndcg@50: 0.982333\n","[67]\tvalid_0's ndcg@10: 0.982185\tvalid_0's ndcg@20: 0.982185\tvalid_0's ndcg@30: 0.982185\tvalid_0's ndcg@40: 0.982185\tvalid_0's ndcg@50: 0.982185\tvalid_0's ndcg@10: 0.982185\tvalid_0's ndcg@20: 0.982185\tvalid_0's ndcg@30: 0.982185\tvalid_0's ndcg@40: 0.982185\tvalid_0's ndcg@50: 0.982185\n","[68]\tvalid_0's ndcg@10: 0.982315\tvalid_0's ndcg@20: 0.982315\tvalid_0's ndcg@30: 0.982315\tvalid_0's ndcg@40: 0.982315\tvalid_0's ndcg@50: 0.982315\tvalid_0's ndcg@10: 0.982315\tvalid_0's ndcg@20: 0.982315\tvalid_0's ndcg@30: 0.982315\tvalid_0's ndcg@40: 0.982315\tvalid_0's ndcg@50: 0.982315\n","[69]\tvalid_0's ndcg@10: 0.982303\tvalid_0's ndcg@20: 0.982303\tvalid_0's ndcg@30: 0.982303\tvalid_0's ndcg@40: 0.982303\tvalid_0's ndcg@50: 0.982303\tvalid_0's ndcg@10: 0.982303\tvalid_0's ndcg@20: 0.982303\tvalid_0's ndcg@30: 0.982303\tvalid_0's ndcg@40: 0.982303\tvalid_0's ndcg@50: 0.982303\n","[70]\tvalid_0's ndcg@10: 0.982379\tvalid_0's ndcg@20: 0.982379\tvalid_0's ndcg@30: 0.982379\tvalid_0's ndcg@40: 0.982379\tvalid_0's ndcg@50: 0.982379\tvalid_0's ndcg@10: 0.982379\tvalid_0's ndcg@20: 0.982379\tvalid_0's ndcg@30: 0.982379\tvalid_0's ndcg@40: 0.982379\tvalid_0's ndcg@50: 0.982379\n","[71]\tvalid_0's ndcg@10: 0.982168\tvalid_0's ndcg@20: 0.982168\tvalid_0's ndcg@30: 0.982168\tvalid_0's ndcg@40: 0.982168\tvalid_0's ndcg@50: 0.982168\tvalid_0's ndcg@10: 0.982168\tvalid_0's ndcg@20: 0.982168\tvalid_0's ndcg@30: 0.982168\tvalid_0's ndcg@40: 0.982168\tvalid_0's ndcg@50: 0.982168\n","[72]\tvalid_0's ndcg@10: 0.982214\tvalid_0's ndcg@20: 0.982214\tvalid_0's ndcg@30: 0.982214\tvalid_0's ndcg@40: 0.982214\tvalid_0's ndcg@50: 0.982214\tvalid_0's ndcg@10: 0.982214\tvalid_0's ndcg@20: 0.982214\tvalid_0's ndcg@30: 0.982214\tvalid_0's ndcg@40: 0.982214\tvalid_0's ndcg@50: 0.982214\n","[73]\tvalid_0's ndcg@10: 0.982224\tvalid_0's ndcg@20: 0.982224\tvalid_0's ndcg@30: 0.982224\tvalid_0's ndcg@40: 0.982224\tvalid_0's ndcg@50: 0.982224\tvalid_0's ndcg@10: 0.982224\tvalid_0's ndcg@20: 0.982224\tvalid_0's ndcg@30: 0.982224\tvalid_0's ndcg@40: 0.982224\tvalid_0's ndcg@50: 0.982224\n","[74]\tvalid_0's ndcg@10: 0.982207\tvalid_0's ndcg@20: 0.982207\tvalid_0's ndcg@30: 0.982207\tvalid_0's ndcg@40: 0.982207\tvalid_0's ndcg@50: 0.982207\tvalid_0's ndcg@10: 0.982207\tvalid_0's ndcg@20: 0.982207\tvalid_0's ndcg@30: 0.982207\tvalid_0's ndcg@40: 0.982207\tvalid_0's ndcg@50: 0.982207\n","[75]\tvalid_0's ndcg@10: 0.982268\tvalid_0's ndcg@20: 0.982268\tvalid_0's ndcg@30: 0.982268\tvalid_0's ndcg@40: 0.982268\tvalid_0's ndcg@50: 0.982268\tvalid_0's ndcg@10: 0.982268\tvalid_0's ndcg@20: 0.982268\tvalid_0's ndcg@30: 0.982268\tvalid_0's ndcg@40: 0.982268\tvalid_0's ndcg@50: 0.982268\n","[76]\tvalid_0's ndcg@10: 0.982267\tvalid_0's ndcg@20: 0.982267\tvalid_0's ndcg@30: 0.982267\tvalid_0's ndcg@40: 0.982267\tvalid_0's ndcg@50: 0.982267\tvalid_0's ndcg@10: 0.982267\tvalid_0's ndcg@20: 0.982267\tvalid_0's ndcg@30: 0.982267\tvalid_0's ndcg@40: 0.982267\tvalid_0's ndcg@50: 0.982267\n","[77]\tvalid_0's ndcg@10: 0.98227\tvalid_0's ndcg@20: 0.98227\tvalid_0's ndcg@30: 0.98227\tvalid_0's ndcg@40: 0.98227\tvalid_0's ndcg@50: 0.98227\tvalid_0's ndcg@10: 0.98227\tvalid_0's ndcg@20: 0.98227\tvalid_0's ndcg@30: 0.98227\tvalid_0's ndcg@40: 0.98227\tvalid_0's ndcg@50: 0.98227\n","[78]\tvalid_0's ndcg@10: 0.982355\tvalid_0's ndcg@20: 0.982355\tvalid_0's ndcg@30: 0.982355\tvalid_0's ndcg@40: 0.982355\tvalid_0's ndcg@50: 0.982355\tvalid_0's ndcg@10: 0.982355\tvalid_0's ndcg@20: 0.982355\tvalid_0's ndcg@30: 0.982355\tvalid_0's ndcg@40: 0.982355\tvalid_0's ndcg@50: 0.982355\n","[79]\tvalid_0's ndcg@10: 0.982217\tvalid_0's ndcg@20: 0.982217\tvalid_0's ndcg@30: 0.982217\tvalid_0's ndcg@40: 0.982217\tvalid_0's ndcg@50: 0.982217\tvalid_0's ndcg@10: 0.982217\tvalid_0's ndcg@20: 0.982217\tvalid_0's ndcg@30: 0.982217\tvalid_0's ndcg@40: 0.982217\tvalid_0's ndcg@50: 0.982217\n","[80]\tvalid_0's ndcg@10: 0.982302\tvalid_0's ndcg@20: 0.982302\tvalid_0's ndcg@30: 0.982302\tvalid_0's ndcg@40: 0.982302\tvalid_0's ndcg@50: 0.982302\tvalid_0's ndcg@10: 0.982302\tvalid_0's ndcg@20: 0.982302\tvalid_0's ndcg@30: 0.982302\tvalid_0's ndcg@40: 0.982302\tvalid_0's ndcg@50: 0.982302\n","[81]\tvalid_0's ndcg@10: 0.982312\tvalid_0's ndcg@20: 0.982312\tvalid_0's ndcg@30: 0.982312\tvalid_0's ndcg@40: 0.982312\tvalid_0's ndcg@50: 0.982312\tvalid_0's ndcg@10: 0.982312\tvalid_0's ndcg@20: 0.982312\tvalid_0's ndcg@30: 0.982312\tvalid_0's ndcg@40: 0.982312\tvalid_0's ndcg@50: 0.982312\n","[82]\tvalid_0's ndcg@10: 0.982203\tvalid_0's ndcg@20: 0.982203\tvalid_0's ndcg@30: 0.982203\tvalid_0's ndcg@40: 0.982203\tvalid_0's ndcg@50: 0.982203\tvalid_0's ndcg@10: 0.982203\tvalid_0's ndcg@20: 0.982203\tvalid_0's ndcg@30: 0.982203\tvalid_0's ndcg@40: 0.982203\tvalid_0's ndcg@50: 0.982203\n","[83]\tvalid_0's ndcg@10: 0.982151\tvalid_0's ndcg@20: 0.982151\tvalid_0's ndcg@30: 0.982151\tvalid_0's ndcg@40: 0.982151\tvalid_0's ndcg@50: 0.982151\tvalid_0's ndcg@10: 0.982151\tvalid_0's ndcg@20: 0.982151\tvalid_0's ndcg@30: 0.982151\tvalid_0's ndcg@40: 0.982151\tvalid_0's ndcg@50: 0.982151\n","[84]\tvalid_0's ndcg@10: 0.982221\tvalid_0's ndcg@20: 0.982221\tvalid_0's ndcg@30: 0.982221\tvalid_0's ndcg@40: 0.982221\tvalid_0's ndcg@50: 0.982221\tvalid_0's ndcg@10: 0.982221\tvalid_0's ndcg@20: 0.982221\tvalid_0's ndcg@30: 0.982221\tvalid_0's ndcg@40: 0.982221\tvalid_0's ndcg@50: 0.982221\n","[85]\tvalid_0's ndcg@10: 0.982199\tvalid_0's ndcg@20: 0.982199\tvalid_0's ndcg@30: 0.982199\tvalid_0's ndcg@40: 0.982199\tvalid_0's ndcg@50: 0.982199\tvalid_0's ndcg@10: 0.982199\tvalid_0's ndcg@20: 0.982199\tvalid_0's ndcg@30: 0.982199\tvalid_0's ndcg@40: 0.982199\tvalid_0's ndcg@50: 0.982199\n","[86]\tvalid_0's ndcg@10: 0.982102\tvalid_0's ndcg@20: 0.982102\tvalid_0's ndcg@30: 0.982102\tvalid_0's ndcg@40: 0.982102\tvalid_0's ndcg@50: 0.982102\tvalid_0's ndcg@10: 0.982102\tvalid_0's ndcg@20: 0.982102\tvalid_0's ndcg@30: 0.982102\tvalid_0's ndcg@40: 0.982102\tvalid_0's ndcg@50: 0.982102\n","[87]\tvalid_0's ndcg@10: 0.982091\tvalid_0's ndcg@20: 0.982091\tvalid_0's ndcg@30: 0.982091\tvalid_0's ndcg@40: 0.982091\tvalid_0's ndcg@50: 0.982091\tvalid_0's ndcg@10: 0.982091\tvalid_0's ndcg@20: 0.982091\tvalid_0's ndcg@30: 0.982091\tvalid_0's ndcg@40: 0.982091\tvalid_0's ndcg@50: 0.982091\n","[88]\tvalid_0's ndcg@10: 0.982153\tvalid_0's ndcg@20: 0.982153\tvalid_0's ndcg@30: 0.982153\tvalid_0's ndcg@40: 0.982153\tvalid_0's ndcg@50: 0.982153\tvalid_0's ndcg@10: 0.982153\tvalid_0's ndcg@20: 0.982153\tvalid_0's ndcg@30: 0.982153\tvalid_0's ndcg@40: 0.982153\tvalid_0's ndcg@50: 0.982153\n","[89]\tvalid_0's ndcg@10: 0.982134\tvalid_0's ndcg@20: 0.982134\tvalid_0's ndcg@30: 0.982134\tvalid_0's ndcg@40: 0.982134\tvalid_0's ndcg@50: 0.982134\tvalid_0's ndcg@10: 0.982134\tvalid_0's ndcg@20: 0.982134\tvalid_0's ndcg@30: 0.982134\tvalid_0's ndcg@40: 0.982134\tvalid_0's ndcg@50: 0.982134\n","[90]\tvalid_0's ndcg@10: 0.982134\tvalid_0's ndcg@20: 0.982134\tvalid_0's ndcg@30: 0.982134\tvalid_0's ndcg@40: 0.982134\tvalid_0's ndcg@50: 0.982134\tvalid_0's ndcg@10: 0.982134\tvalid_0's ndcg@20: 0.982134\tvalid_0's ndcg@30: 0.982134\tvalid_0's ndcg@40: 0.982134\tvalid_0's ndcg@50: 0.982134\n","[91]\tvalid_0's ndcg@10: 0.982109\tvalid_0's ndcg@20: 0.982109\tvalid_0's ndcg@30: 0.982109\tvalid_0's ndcg@40: 0.982109\tvalid_0's ndcg@50: 0.982109\tvalid_0's ndcg@10: 0.982109\tvalid_0's ndcg@20: 0.982109\tvalid_0's ndcg@30: 0.982109\tvalid_0's ndcg@40: 0.982109\tvalid_0's ndcg@50: 0.982109\n","[92]\tvalid_0's ndcg@10: 0.982097\tvalid_0's ndcg@20: 0.982097\tvalid_0's ndcg@30: 0.982097\tvalid_0's ndcg@40: 0.982097\tvalid_0's ndcg@50: 0.982097\tvalid_0's ndcg@10: 0.982097\tvalid_0's ndcg@20: 0.982097\tvalid_0's ndcg@30: 0.982097\tvalid_0's ndcg@40: 0.982097\tvalid_0's ndcg@50: 0.982097\n","[93]\tvalid_0's ndcg@10: 0.982175\tvalid_0's ndcg@20: 0.982175\tvalid_0's ndcg@30: 0.982175\tvalid_0's ndcg@40: 0.982175\tvalid_0's ndcg@50: 0.982175\tvalid_0's ndcg@10: 0.982175\tvalid_0's ndcg@20: 0.982175\tvalid_0's ndcg@30: 0.982175\tvalid_0's ndcg@40: 0.982175\tvalid_0's ndcg@50: 0.982175\n","[94]\tvalid_0's ndcg@10: 0.982173\tvalid_0's ndcg@20: 0.982173\tvalid_0's ndcg@30: 0.982173\tvalid_0's ndcg@40: 0.982173\tvalid_0's ndcg@50: 0.982173\tvalid_0's ndcg@10: 0.982173\tvalid_0's ndcg@20: 0.982173\tvalid_0's ndcg@30: 0.982173\tvalid_0's ndcg@40: 0.982173\tvalid_0's ndcg@50: 0.982173\n","[95]\tvalid_0's ndcg@10: 0.982144\tvalid_0's ndcg@20: 0.982144\tvalid_0's ndcg@30: 0.982144\tvalid_0's ndcg@40: 0.982144\tvalid_0's ndcg@50: 0.982144\tvalid_0's ndcg@10: 0.982144\tvalid_0's ndcg@20: 0.982144\tvalid_0's ndcg@30: 0.982144\tvalid_0's ndcg@40: 0.982144\tvalid_0's ndcg@50: 0.982144\n","[96]\tvalid_0's ndcg@10: 0.982052\tvalid_0's ndcg@20: 0.982052\tvalid_0's ndcg@30: 0.982052\tvalid_0's ndcg@40: 0.982052\tvalid_0's ndcg@50: 0.982052\tvalid_0's ndcg@10: 0.982052\tvalid_0's ndcg@20: 0.982052\tvalid_0's ndcg@30: 0.982052\tvalid_0's ndcg@40: 0.982052\tvalid_0's ndcg@50: 0.982052\n","[97]\tvalid_0's ndcg@10: 0.982095\tvalid_0's ndcg@20: 0.982095\tvalid_0's ndcg@30: 0.982095\tvalid_0's ndcg@40: 0.982095\tvalid_0's ndcg@50: 0.982095\tvalid_0's ndcg@10: 0.982095\tvalid_0's ndcg@20: 0.982095\tvalid_0's ndcg@30: 0.982095\tvalid_0's ndcg@40: 0.982095\tvalid_0's ndcg@50: 0.982095\n","[98]\tvalid_0's ndcg@10: 0.982139\tvalid_0's ndcg@20: 0.982139\tvalid_0's ndcg@30: 0.982139\tvalid_0's ndcg@40: 0.982139\tvalid_0's ndcg@50: 0.982139\tvalid_0's ndcg@10: 0.982139\tvalid_0's ndcg@20: 0.982139\tvalid_0's ndcg@30: 0.982139\tvalid_0's ndcg@40: 0.982139\tvalid_0's ndcg@50: 0.982139\n","[99]\tvalid_0's ndcg@10: 0.982112\tvalid_0's ndcg@20: 0.982112\tvalid_0's ndcg@30: 0.982112\tvalid_0's ndcg@40: 0.982112\tvalid_0's ndcg@50: 0.982112\tvalid_0's ndcg@10: 0.982112\tvalid_0's ndcg@20: 0.982112\tvalid_0's ndcg@30: 0.982112\tvalid_0's ndcg@40: 0.982112\tvalid_0's ndcg@50: 0.982112\n","[100]\tvalid_0's ndcg@10: 0.982158\tvalid_0's ndcg@20: 0.982158\tvalid_0's ndcg@30: 0.982158\tvalid_0's ndcg@40: 0.982158\tvalid_0's ndcg@50: 0.982158\tvalid_0's ndcg@10: 0.982158\tvalid_0's ndcg@20: 0.982158\tvalid_0's ndcg@30: 0.982158\tvalid_0's ndcg@40: 0.982158\tvalid_0's ndcg@50: 0.982158\n","Did not meet early stopping. Best iteration is:\n","[70]\tvalid_0's ndcg@10: 0.982379\tvalid_0's ndcg@20: 0.982379\tvalid_0's ndcg@30: 0.982379\tvalid_0's ndcg@40: 0.982379\tvalid_0's ndcg@50: 0.982379\tvalid_0's ndcg@10: 0.982379\tvalid_0's ndcg@20: 0.982379\tvalid_0's ndcg@30: 0.982379\tvalid_0's ndcg@40: 0.982379\tvalid_0's ndcg@50: 0.982379\n","[1]\tvalid_0's ndcg@10: 0.974666\tvalid_0's ndcg@20: 0.974666\tvalid_0's ndcg@30: 0.974666\tvalid_0's ndcg@40: 0.974666\tvalid_0's ndcg@50: 0.974666\tvalid_0's ndcg@10: 0.974666\tvalid_0's ndcg@20: 0.974666\tvalid_0's ndcg@30: 0.974666\tvalid_0's ndcg@40: 0.974666\tvalid_0's ndcg@50: 0.974666\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.976182\tvalid_0's ndcg@20: 0.976182\tvalid_0's ndcg@30: 0.976182\tvalid_0's ndcg@40: 0.976182\tvalid_0's ndcg@50: 0.976182\tvalid_0's ndcg@10: 0.976182\tvalid_0's ndcg@20: 0.976182\tvalid_0's ndcg@30: 0.976182\tvalid_0's ndcg@40: 0.976182\tvalid_0's ndcg@50: 0.976182\n","[3]\tvalid_0's ndcg@10: 0.977202\tvalid_0's ndcg@20: 0.977202\tvalid_0's ndcg@30: 0.977202\tvalid_0's ndcg@40: 0.977202\tvalid_0's ndcg@50: 0.977202\tvalid_0's ndcg@10: 0.977202\tvalid_0's ndcg@20: 0.977202\tvalid_0's ndcg@30: 0.977202\tvalid_0's ndcg@40: 0.977202\tvalid_0's ndcg@50: 0.977202\n","[4]\tvalid_0's ndcg@10: 0.978143\tvalid_0's ndcg@20: 0.978143\tvalid_0's ndcg@30: 0.978143\tvalid_0's ndcg@40: 0.978143\tvalid_0's ndcg@50: 0.978143\tvalid_0's ndcg@10: 0.978143\tvalid_0's ndcg@20: 0.978143\tvalid_0's ndcg@30: 0.978143\tvalid_0's ndcg@40: 0.978143\tvalid_0's ndcg@50: 0.978143\n","[5]\tvalid_0's ndcg@10: 0.977995\tvalid_0's ndcg@20: 0.977995\tvalid_0's ndcg@30: 0.977995\tvalid_0's ndcg@40: 0.977995\tvalid_0's ndcg@50: 0.977995\tvalid_0's ndcg@10: 0.977995\tvalid_0's ndcg@20: 0.977995\tvalid_0's ndcg@30: 0.977995\tvalid_0's ndcg@40: 0.977995\tvalid_0's ndcg@50: 0.977995\n","[6]\tvalid_0's ndcg@10: 0.978895\tvalid_0's ndcg@20: 0.978895\tvalid_0's ndcg@30: 0.978895\tvalid_0's ndcg@40: 0.978895\tvalid_0's ndcg@50: 0.978895\tvalid_0's ndcg@10: 0.978895\tvalid_0's ndcg@20: 0.978895\tvalid_0's ndcg@30: 0.978895\tvalid_0's ndcg@40: 0.978895\tvalid_0's ndcg@50: 0.978895\n","[7]\tvalid_0's ndcg@10: 0.97971\tvalid_0's ndcg@20: 0.97971\tvalid_0's ndcg@30: 0.97971\tvalid_0's ndcg@40: 0.97971\tvalid_0's ndcg@50: 0.97971\tvalid_0's ndcg@10: 0.97971\tvalid_0's ndcg@20: 0.97971\tvalid_0's ndcg@30: 0.97971\tvalid_0's ndcg@40: 0.97971\tvalid_0's ndcg@50: 0.97971\n","[8]\tvalid_0's ndcg@10: 0.980303\tvalid_0's ndcg@20: 0.980303\tvalid_0's ndcg@30: 0.980303\tvalid_0's ndcg@40: 0.980303\tvalid_0's ndcg@50: 0.980303\tvalid_0's ndcg@10: 0.980303\tvalid_0's ndcg@20: 0.980303\tvalid_0's ndcg@30: 0.980303\tvalid_0's ndcg@40: 0.980303\tvalid_0's ndcg@50: 0.980303\n","[9]\tvalid_0's ndcg@10: 0.979976\tvalid_0's ndcg@20: 0.979976\tvalid_0's ndcg@30: 0.979976\tvalid_0's ndcg@40: 0.979976\tvalid_0's ndcg@50: 0.979976\tvalid_0's ndcg@10: 0.979976\tvalid_0's ndcg@20: 0.979976\tvalid_0's ndcg@30: 0.979976\tvalid_0's ndcg@40: 0.979976\tvalid_0's ndcg@50: 0.979976\n","[10]\tvalid_0's ndcg@10: 0.980133\tvalid_0's ndcg@20: 0.980133\tvalid_0's ndcg@30: 0.980133\tvalid_0's ndcg@40: 0.980133\tvalid_0's ndcg@50: 0.980133\tvalid_0's ndcg@10: 0.980133\tvalid_0's ndcg@20: 0.980133\tvalid_0's ndcg@30: 0.980133\tvalid_0's ndcg@40: 0.980133\tvalid_0's ndcg@50: 0.980133\n","[11]\tvalid_0's ndcg@10: 0.980468\tvalid_0's ndcg@20: 0.980468\tvalid_0's ndcg@30: 0.980468\tvalid_0's ndcg@40: 0.980468\tvalid_0's ndcg@50: 0.980468\tvalid_0's ndcg@10: 0.980468\tvalid_0's ndcg@20: 0.980468\tvalid_0's ndcg@30: 0.980468\tvalid_0's ndcg@40: 0.980468\tvalid_0's ndcg@50: 0.980468\n","[12]\tvalid_0's ndcg@10: 0.980574\tvalid_0's ndcg@20: 0.980574\tvalid_0's ndcg@30: 0.980574\tvalid_0's ndcg@40: 0.980574\tvalid_0's ndcg@50: 0.980574\tvalid_0's ndcg@10: 0.980574\tvalid_0's ndcg@20: 0.980574\tvalid_0's ndcg@30: 0.980574\tvalid_0's ndcg@40: 0.980574\tvalid_0's ndcg@50: 0.980574\n","[13]\tvalid_0's ndcg@10: 0.980938\tvalid_0's ndcg@20: 0.980938\tvalid_0's ndcg@30: 0.980938\tvalid_0's ndcg@40: 0.980938\tvalid_0's ndcg@50: 0.980938\tvalid_0's ndcg@10: 0.980938\tvalid_0's ndcg@20: 0.980938\tvalid_0's ndcg@30: 0.980938\tvalid_0's ndcg@40: 0.980938\tvalid_0's ndcg@50: 0.980938\n","[14]\tvalid_0's ndcg@10: 0.981003\tvalid_0's ndcg@20: 0.981003\tvalid_0's ndcg@30: 0.981003\tvalid_0's ndcg@40: 0.981003\tvalid_0's ndcg@50: 0.981003\tvalid_0's ndcg@10: 0.981003\tvalid_0's ndcg@20: 0.981003\tvalid_0's ndcg@30: 0.981003\tvalid_0's ndcg@40: 0.981003\tvalid_0's ndcg@50: 0.981003\n","[15]\tvalid_0's ndcg@10: 0.981117\tvalid_0's ndcg@20: 0.981117\tvalid_0's ndcg@30: 0.981117\tvalid_0's ndcg@40: 0.981117\tvalid_0's ndcg@50: 0.981117\tvalid_0's ndcg@10: 0.981117\tvalid_0's ndcg@20: 0.981117\tvalid_0's ndcg@30: 0.981117\tvalid_0's ndcg@40: 0.981117\tvalid_0's ndcg@50: 0.981117\n","[16]\tvalid_0's ndcg@10: 0.980818\tvalid_0's ndcg@20: 0.980818\tvalid_0's ndcg@30: 0.980818\tvalid_0's ndcg@40: 0.980818\tvalid_0's ndcg@50: 0.980818\tvalid_0's ndcg@10: 0.980818\tvalid_0's ndcg@20: 0.980818\tvalid_0's ndcg@30: 0.980818\tvalid_0's ndcg@40: 0.980818\tvalid_0's ndcg@50: 0.980818\n","[17]\tvalid_0's ndcg@10: 0.981297\tvalid_0's ndcg@20: 0.981297\tvalid_0's ndcg@30: 0.981297\tvalid_0's ndcg@40: 0.981297\tvalid_0's ndcg@50: 0.981297\tvalid_0's ndcg@10: 0.981297\tvalid_0's ndcg@20: 0.981297\tvalid_0's ndcg@30: 0.981297\tvalid_0's ndcg@40: 0.981297\tvalid_0's ndcg@50: 0.981297\n","[18]\tvalid_0's ndcg@10: 0.981134\tvalid_0's ndcg@20: 0.981134\tvalid_0's ndcg@30: 0.981134\tvalid_0's ndcg@40: 0.981134\tvalid_0's ndcg@50: 0.981134\tvalid_0's ndcg@10: 0.981134\tvalid_0's ndcg@20: 0.981134\tvalid_0's ndcg@30: 0.981134\tvalid_0's ndcg@40: 0.981134\tvalid_0's ndcg@50: 0.981134\n","[19]\tvalid_0's ndcg@10: 0.981197\tvalid_0's ndcg@20: 0.981197\tvalid_0's ndcg@30: 0.981197\tvalid_0's ndcg@40: 0.981197\tvalid_0's ndcg@50: 0.981197\tvalid_0's ndcg@10: 0.981197\tvalid_0's ndcg@20: 0.981197\tvalid_0's ndcg@30: 0.981197\tvalid_0's ndcg@40: 0.981197\tvalid_0's ndcg@50: 0.981197\n","[20]\tvalid_0's ndcg@10: 0.98131\tvalid_0's ndcg@20: 0.98131\tvalid_0's ndcg@30: 0.98131\tvalid_0's ndcg@40: 0.98131\tvalid_0's ndcg@50: 0.98131\tvalid_0's ndcg@10: 0.98131\tvalid_0's ndcg@20: 0.98131\tvalid_0's ndcg@30: 0.98131\tvalid_0's ndcg@40: 0.98131\tvalid_0's ndcg@50: 0.98131\n","[21]\tvalid_0's ndcg@10: 0.980923\tvalid_0's ndcg@20: 0.980923\tvalid_0's ndcg@30: 0.980923\tvalid_0's ndcg@40: 0.980923\tvalid_0's ndcg@50: 0.980923\tvalid_0's ndcg@10: 0.980923\tvalid_0's ndcg@20: 0.980923\tvalid_0's ndcg@30: 0.980923\tvalid_0's ndcg@40: 0.980923\tvalid_0's ndcg@50: 0.980923\n","[22]\tvalid_0's ndcg@10: 0.980975\tvalid_0's ndcg@20: 0.980975\tvalid_0's ndcg@30: 0.980975\tvalid_0's ndcg@40: 0.980975\tvalid_0's ndcg@50: 0.980975\tvalid_0's ndcg@10: 0.980975\tvalid_0's ndcg@20: 0.980975\tvalid_0's ndcg@30: 0.980975\tvalid_0's ndcg@40: 0.980975\tvalid_0's ndcg@50: 0.980975\n","[23]\tvalid_0's ndcg@10: 0.980724\tvalid_0's ndcg@20: 0.980724\tvalid_0's ndcg@30: 0.980724\tvalid_0's ndcg@40: 0.980724\tvalid_0's ndcg@50: 0.980724\tvalid_0's ndcg@10: 0.980724\tvalid_0's ndcg@20: 0.980724\tvalid_0's ndcg@30: 0.980724\tvalid_0's ndcg@40: 0.980724\tvalid_0's ndcg@50: 0.980724\n","[24]\tvalid_0's ndcg@10: 0.980999\tvalid_0's ndcg@20: 0.980999\tvalid_0's ndcg@30: 0.980999\tvalid_0's ndcg@40: 0.980999\tvalid_0's ndcg@50: 0.980999\tvalid_0's ndcg@10: 0.980999\tvalid_0's ndcg@20: 0.980999\tvalid_0's ndcg@30: 0.980999\tvalid_0's ndcg@40: 0.980999\tvalid_0's ndcg@50: 0.980999\n","[25]\tvalid_0's ndcg@10: 0.980894\tvalid_0's ndcg@20: 0.980894\tvalid_0's ndcg@30: 0.980894\tvalid_0's ndcg@40: 0.980894\tvalid_0's ndcg@50: 0.980894\tvalid_0's ndcg@10: 0.980894\tvalid_0's ndcg@20: 0.980894\tvalid_0's ndcg@30: 0.980894\tvalid_0's ndcg@40: 0.980894\tvalid_0's ndcg@50: 0.980894\n","[26]\tvalid_0's ndcg@10: 0.98094\tvalid_0's ndcg@20: 0.98094\tvalid_0's ndcg@30: 0.98094\tvalid_0's ndcg@40: 0.98094\tvalid_0's ndcg@50: 0.98094\tvalid_0's ndcg@10: 0.98094\tvalid_0's ndcg@20: 0.98094\tvalid_0's ndcg@30: 0.98094\tvalid_0's ndcg@40: 0.98094\tvalid_0's ndcg@50: 0.98094\n","[27]\tvalid_0's ndcg@10: 0.98098\tvalid_0's ndcg@20: 0.98098\tvalid_0's ndcg@30: 0.98098\tvalid_0's ndcg@40: 0.98098\tvalid_0's ndcg@50: 0.98098\tvalid_0's ndcg@10: 0.98098\tvalid_0's ndcg@20: 0.98098\tvalid_0's ndcg@30: 0.98098\tvalid_0's ndcg@40: 0.98098\tvalid_0's ndcg@50: 0.98098\n","[28]\tvalid_0's ndcg@10: 0.980617\tvalid_0's ndcg@20: 0.980617\tvalid_0's ndcg@30: 0.980617\tvalid_0's ndcg@40: 0.980617\tvalid_0's ndcg@50: 0.980617\tvalid_0's ndcg@10: 0.980617\tvalid_0's ndcg@20: 0.980617\tvalid_0's ndcg@30: 0.980617\tvalid_0's ndcg@40: 0.980617\tvalid_0's ndcg@50: 0.980617\n","[29]\tvalid_0's ndcg@10: 0.980752\tvalid_0's ndcg@20: 0.980752\tvalid_0's ndcg@30: 0.980752\tvalid_0's ndcg@40: 0.980752\tvalid_0's ndcg@50: 0.980752\tvalid_0's ndcg@10: 0.980752\tvalid_0's ndcg@20: 0.980752\tvalid_0's ndcg@30: 0.980752\tvalid_0's ndcg@40: 0.980752\tvalid_0's ndcg@50: 0.980752\n","[30]\tvalid_0's ndcg@10: 0.980741\tvalid_0's ndcg@20: 0.980741\tvalid_0's ndcg@30: 0.980741\tvalid_0's ndcg@40: 0.980741\tvalid_0's ndcg@50: 0.980741\tvalid_0's ndcg@10: 0.980741\tvalid_0's ndcg@20: 0.980741\tvalid_0's ndcg@30: 0.980741\tvalid_0's ndcg@40: 0.980741\tvalid_0's ndcg@50: 0.980741\n","[31]\tvalid_0's ndcg@10: 0.980991\tvalid_0's ndcg@20: 0.980991\tvalid_0's ndcg@30: 0.980991\tvalid_0's ndcg@40: 0.980991\tvalid_0's ndcg@50: 0.980991\tvalid_0's ndcg@10: 0.980991\tvalid_0's ndcg@20: 0.980991\tvalid_0's ndcg@30: 0.980991\tvalid_0's ndcg@40: 0.980991\tvalid_0's ndcg@50: 0.980991\n","[32]\tvalid_0's ndcg@10: 0.981007\tvalid_0's ndcg@20: 0.981007\tvalid_0's ndcg@30: 0.981007\tvalid_0's ndcg@40: 0.981007\tvalid_0's ndcg@50: 0.981007\tvalid_0's ndcg@10: 0.981007\tvalid_0's ndcg@20: 0.981007\tvalid_0's ndcg@30: 0.981007\tvalid_0's ndcg@40: 0.981007\tvalid_0's ndcg@50: 0.981007\n","[33]\tvalid_0's ndcg@10: 0.980995\tvalid_0's ndcg@20: 0.980995\tvalid_0's ndcg@30: 0.980995\tvalid_0's ndcg@40: 0.980995\tvalid_0's ndcg@50: 0.980995\tvalid_0's ndcg@10: 0.980995\tvalid_0's ndcg@20: 0.980995\tvalid_0's ndcg@30: 0.980995\tvalid_0's ndcg@40: 0.980995\tvalid_0's ndcg@50: 0.980995\n","[34]\tvalid_0's ndcg@10: 0.981292\tvalid_0's ndcg@20: 0.981292\tvalid_0's ndcg@30: 0.981292\tvalid_0's ndcg@40: 0.981292\tvalid_0's ndcg@50: 0.981292\tvalid_0's ndcg@10: 0.981292\tvalid_0's ndcg@20: 0.981292\tvalid_0's ndcg@30: 0.981292\tvalid_0's ndcg@40: 0.981292\tvalid_0's ndcg@50: 0.981292\n","[35]\tvalid_0's ndcg@10: 0.980939\tvalid_0's ndcg@20: 0.980939\tvalid_0's ndcg@30: 0.980939\tvalid_0's ndcg@40: 0.980939\tvalid_0's ndcg@50: 0.980939\tvalid_0's ndcg@10: 0.980939\tvalid_0's ndcg@20: 0.980939\tvalid_0's ndcg@30: 0.980939\tvalid_0's ndcg@40: 0.980939\tvalid_0's ndcg@50: 0.980939\n","[36]\tvalid_0's ndcg@10: 0.980917\tvalid_0's ndcg@20: 0.980917\tvalid_0's ndcg@30: 0.980917\tvalid_0's ndcg@40: 0.980917\tvalid_0's ndcg@50: 0.980917\tvalid_0's ndcg@10: 0.980917\tvalid_0's ndcg@20: 0.980917\tvalid_0's ndcg@30: 0.980917\tvalid_0's ndcg@40: 0.980917\tvalid_0's ndcg@50: 0.980917\n","[37]\tvalid_0's ndcg@10: 0.980843\tvalid_0's ndcg@20: 0.980843\tvalid_0's ndcg@30: 0.980843\tvalid_0's ndcg@40: 0.980843\tvalid_0's ndcg@50: 0.980843\tvalid_0's ndcg@10: 0.980843\tvalid_0's ndcg@20: 0.980843\tvalid_0's ndcg@30: 0.980843\tvalid_0's ndcg@40: 0.980843\tvalid_0's ndcg@50: 0.980843\n","[38]\tvalid_0's ndcg@10: 0.981088\tvalid_0's ndcg@20: 0.981088\tvalid_0's ndcg@30: 0.981088\tvalid_0's ndcg@40: 0.981088\tvalid_0's ndcg@50: 0.981088\tvalid_0's ndcg@10: 0.981088\tvalid_0's ndcg@20: 0.981088\tvalid_0's ndcg@30: 0.981088\tvalid_0's ndcg@40: 0.981088\tvalid_0's ndcg@50: 0.981088\n","[39]\tvalid_0's ndcg@10: 0.981086\tvalid_0's ndcg@20: 0.981086\tvalid_0's ndcg@30: 0.981086\tvalid_0's ndcg@40: 0.981086\tvalid_0's ndcg@50: 0.981086\tvalid_0's ndcg@10: 0.981086\tvalid_0's ndcg@20: 0.981086\tvalid_0's ndcg@30: 0.981086\tvalid_0's ndcg@40: 0.981086\tvalid_0's ndcg@50: 0.981086\n","[40]\tvalid_0's ndcg@10: 0.981042\tvalid_0's ndcg@20: 0.981042\tvalid_0's ndcg@30: 0.981042\tvalid_0's ndcg@40: 0.981042\tvalid_0's ndcg@50: 0.981042\tvalid_0's ndcg@10: 0.981042\tvalid_0's ndcg@20: 0.981042\tvalid_0's ndcg@30: 0.981042\tvalid_0's ndcg@40: 0.981042\tvalid_0's ndcg@50: 0.981042\n","[41]\tvalid_0's ndcg@10: 0.981106\tvalid_0's ndcg@20: 0.981106\tvalid_0's ndcg@30: 0.981106\tvalid_0's ndcg@40: 0.981106\tvalid_0's ndcg@50: 0.981106\tvalid_0's ndcg@10: 0.981106\tvalid_0's ndcg@20: 0.981106\tvalid_0's ndcg@30: 0.981106\tvalid_0's ndcg@40: 0.981106\tvalid_0's ndcg@50: 0.981106\n","[42]\tvalid_0's ndcg@10: 0.98128\tvalid_0's ndcg@20: 0.98128\tvalid_0's ndcg@30: 0.98128\tvalid_0's ndcg@40: 0.98128\tvalid_0's ndcg@50: 0.98128\tvalid_0's ndcg@10: 0.98128\tvalid_0's ndcg@20: 0.98128\tvalid_0's ndcg@30: 0.98128\tvalid_0's ndcg@40: 0.98128\tvalid_0's ndcg@50: 0.98128\n","[43]\tvalid_0's ndcg@10: 0.981397\tvalid_0's ndcg@20: 0.981397\tvalid_0's ndcg@30: 0.981397\tvalid_0's ndcg@40: 0.981397\tvalid_0's ndcg@50: 0.981397\tvalid_0's ndcg@10: 0.981397\tvalid_0's ndcg@20: 0.981397\tvalid_0's ndcg@30: 0.981397\tvalid_0's ndcg@40: 0.981397\tvalid_0's ndcg@50: 0.981397\n","[44]\tvalid_0's ndcg@10: 0.981382\tvalid_0's ndcg@20: 0.981382\tvalid_0's ndcg@30: 0.981382\tvalid_0's ndcg@40: 0.981382\tvalid_0's ndcg@50: 0.981382\tvalid_0's ndcg@10: 0.981382\tvalid_0's ndcg@20: 0.981382\tvalid_0's ndcg@30: 0.981382\tvalid_0's ndcg@40: 0.981382\tvalid_0's ndcg@50: 0.981382\n","[45]\tvalid_0's ndcg@10: 0.981302\tvalid_0's ndcg@20: 0.981302\tvalid_0's ndcg@30: 0.981302\tvalid_0's ndcg@40: 0.981302\tvalid_0's ndcg@50: 0.981302\tvalid_0's ndcg@10: 0.981302\tvalid_0's ndcg@20: 0.981302\tvalid_0's ndcg@30: 0.981302\tvalid_0's ndcg@40: 0.981302\tvalid_0's ndcg@50: 0.981302\n","[46]\tvalid_0's ndcg@10: 0.981308\tvalid_0's ndcg@20: 0.981308\tvalid_0's ndcg@30: 0.981308\tvalid_0's ndcg@40: 0.981308\tvalid_0's ndcg@50: 0.981308\tvalid_0's ndcg@10: 0.981308\tvalid_0's ndcg@20: 0.981308\tvalid_0's ndcg@30: 0.981308\tvalid_0's ndcg@40: 0.981308\tvalid_0's ndcg@50: 0.981308\n","[47]\tvalid_0's ndcg@10: 0.981382\tvalid_0's ndcg@20: 0.981382\tvalid_0's ndcg@30: 0.981382\tvalid_0's ndcg@40: 0.981382\tvalid_0's ndcg@50: 0.981382\tvalid_0's ndcg@10: 0.981382\tvalid_0's ndcg@20: 0.981382\tvalid_0's ndcg@30: 0.981382\tvalid_0's ndcg@40: 0.981382\tvalid_0's ndcg@50: 0.981382\n","[48]\tvalid_0's ndcg@10: 0.981512\tvalid_0's ndcg@20: 0.981512\tvalid_0's ndcg@30: 0.981512\tvalid_0's ndcg@40: 0.981512\tvalid_0's ndcg@50: 0.981512\tvalid_0's ndcg@10: 0.981512\tvalid_0's ndcg@20: 0.981512\tvalid_0's ndcg@30: 0.981512\tvalid_0's ndcg@40: 0.981512\tvalid_0's ndcg@50: 0.981512\n","[49]\tvalid_0's ndcg@10: 0.981515\tvalid_0's ndcg@20: 0.981515\tvalid_0's ndcg@30: 0.981515\tvalid_0's ndcg@40: 0.981515\tvalid_0's ndcg@50: 0.981515\tvalid_0's ndcg@10: 0.981515\tvalid_0's ndcg@20: 0.981515\tvalid_0's ndcg@30: 0.981515\tvalid_0's ndcg@40: 0.981515\tvalid_0's ndcg@50: 0.981515\n","[50]\tvalid_0's ndcg@10: 0.98139\tvalid_0's ndcg@20: 0.98139\tvalid_0's ndcg@30: 0.98139\tvalid_0's ndcg@40: 0.98139\tvalid_0's ndcg@50: 0.98139\tvalid_0's ndcg@10: 0.98139\tvalid_0's ndcg@20: 0.98139\tvalid_0's ndcg@30: 0.98139\tvalid_0's ndcg@40: 0.98139\tvalid_0's ndcg@50: 0.98139\n","[51]\tvalid_0's ndcg@10: 0.981464\tvalid_0's ndcg@20: 0.981464\tvalid_0's ndcg@30: 0.981464\tvalid_0's ndcg@40: 0.981464\tvalid_0's ndcg@50: 0.981464\tvalid_0's ndcg@10: 0.981464\tvalid_0's ndcg@20: 0.981464\tvalid_0's ndcg@30: 0.981464\tvalid_0's ndcg@40: 0.981464\tvalid_0's ndcg@50: 0.981464\n","[52]\tvalid_0's ndcg@10: 0.981431\tvalid_0's ndcg@20: 0.981431\tvalid_0's ndcg@30: 0.981431\tvalid_0's ndcg@40: 0.981431\tvalid_0's ndcg@50: 0.981431\tvalid_0's ndcg@10: 0.981431\tvalid_0's ndcg@20: 0.981431\tvalid_0's ndcg@30: 0.981431\tvalid_0's ndcg@40: 0.981431\tvalid_0's ndcg@50: 0.981431\n","[53]\tvalid_0's ndcg@10: 0.981346\tvalid_0's ndcg@20: 0.981346\tvalid_0's ndcg@30: 0.981346\tvalid_0's ndcg@40: 0.981346\tvalid_0's ndcg@50: 0.981346\tvalid_0's ndcg@10: 0.981346\tvalid_0's ndcg@20: 0.981346\tvalid_0's ndcg@30: 0.981346\tvalid_0's ndcg@40: 0.981346\tvalid_0's ndcg@50: 0.981346\n","[54]\tvalid_0's ndcg@10: 0.98139\tvalid_0's ndcg@20: 0.98139\tvalid_0's ndcg@30: 0.98139\tvalid_0's ndcg@40: 0.98139\tvalid_0's ndcg@50: 0.98139\tvalid_0's ndcg@10: 0.98139\tvalid_0's ndcg@20: 0.98139\tvalid_0's ndcg@30: 0.98139\tvalid_0's ndcg@40: 0.98139\tvalid_0's ndcg@50: 0.98139\n","[55]\tvalid_0's ndcg@10: 0.981537\tvalid_0's ndcg@20: 0.981537\tvalid_0's ndcg@30: 0.981537\tvalid_0's ndcg@40: 0.981537\tvalid_0's ndcg@50: 0.981537\tvalid_0's ndcg@10: 0.981537\tvalid_0's ndcg@20: 0.981537\tvalid_0's ndcg@30: 0.981537\tvalid_0's ndcg@40: 0.981537\tvalid_0's ndcg@50: 0.981537\n","[56]\tvalid_0's ndcg@10: 0.981397\tvalid_0's ndcg@20: 0.981397\tvalid_0's ndcg@30: 0.981397\tvalid_0's ndcg@40: 0.981397\tvalid_0's ndcg@50: 0.981397\tvalid_0's ndcg@10: 0.981397\tvalid_0's ndcg@20: 0.981397\tvalid_0's ndcg@30: 0.981397\tvalid_0's ndcg@40: 0.981397\tvalid_0's ndcg@50: 0.981397\n","[57]\tvalid_0's ndcg@10: 0.981363\tvalid_0's ndcg@20: 0.981363\tvalid_0's ndcg@30: 0.981363\tvalid_0's ndcg@40: 0.981363\tvalid_0's ndcg@50: 0.981363\tvalid_0's ndcg@10: 0.981363\tvalid_0's ndcg@20: 0.981363\tvalid_0's ndcg@30: 0.981363\tvalid_0's ndcg@40: 0.981363\tvalid_0's ndcg@50: 0.981363\n","[58]\tvalid_0's ndcg@10: 0.981557\tvalid_0's ndcg@20: 0.981557\tvalid_0's ndcg@30: 0.981557\tvalid_0's ndcg@40: 0.981557\tvalid_0's ndcg@50: 0.981557\tvalid_0's ndcg@10: 0.981557\tvalid_0's ndcg@20: 0.981557\tvalid_0's ndcg@30: 0.981557\tvalid_0's ndcg@40: 0.981557\tvalid_0's ndcg@50: 0.981557\n","[59]\tvalid_0's ndcg@10: 0.98163\tvalid_0's ndcg@20: 0.98163\tvalid_0's ndcg@30: 0.98163\tvalid_0's ndcg@40: 0.98163\tvalid_0's ndcg@50: 0.98163\tvalid_0's ndcg@10: 0.98163\tvalid_0's ndcg@20: 0.98163\tvalid_0's ndcg@30: 0.98163\tvalid_0's ndcg@40: 0.98163\tvalid_0's ndcg@50: 0.98163\n","[60]\tvalid_0's ndcg@10: 0.98152\tvalid_0's ndcg@20: 0.98152\tvalid_0's ndcg@30: 0.98152\tvalid_0's ndcg@40: 0.98152\tvalid_0's ndcg@50: 0.98152\tvalid_0's ndcg@10: 0.98152\tvalid_0's ndcg@20: 0.98152\tvalid_0's ndcg@30: 0.98152\tvalid_0's ndcg@40: 0.98152\tvalid_0's ndcg@50: 0.98152\n","[61]\tvalid_0's ndcg@10: 0.981606\tvalid_0's ndcg@20: 0.981606\tvalid_0's ndcg@30: 0.981606\tvalid_0's ndcg@40: 0.981606\tvalid_0's ndcg@50: 0.981606\tvalid_0's ndcg@10: 0.981606\tvalid_0's ndcg@20: 0.981606\tvalid_0's ndcg@30: 0.981606\tvalid_0's ndcg@40: 0.981606\tvalid_0's ndcg@50: 0.981606\n","[62]\tvalid_0's ndcg@10: 0.981661\tvalid_0's ndcg@20: 0.981661\tvalid_0's ndcg@30: 0.981661\tvalid_0's ndcg@40: 0.981661\tvalid_0's ndcg@50: 0.981661\tvalid_0's ndcg@10: 0.981661\tvalid_0's ndcg@20: 0.981661\tvalid_0's ndcg@30: 0.981661\tvalid_0's ndcg@40: 0.981661\tvalid_0's ndcg@50: 0.981661\n","[63]\tvalid_0's ndcg@10: 0.981702\tvalid_0's ndcg@20: 0.981702\tvalid_0's ndcg@30: 0.981702\tvalid_0's ndcg@40: 0.981702\tvalid_0's ndcg@50: 0.981702\tvalid_0's ndcg@10: 0.981702\tvalid_0's ndcg@20: 0.981702\tvalid_0's ndcg@30: 0.981702\tvalid_0's ndcg@40: 0.981702\tvalid_0's ndcg@50: 0.981702\n","[64]\tvalid_0's ndcg@10: 0.981709\tvalid_0's ndcg@20: 0.981709\tvalid_0's ndcg@30: 0.981709\tvalid_0's ndcg@40: 0.981709\tvalid_0's ndcg@50: 0.981709\tvalid_0's ndcg@10: 0.981709\tvalid_0's ndcg@20: 0.981709\tvalid_0's ndcg@30: 0.981709\tvalid_0's ndcg@40: 0.981709\tvalid_0's ndcg@50: 0.981709\n","[65]\tvalid_0's ndcg@10: 0.981654\tvalid_0's ndcg@20: 0.981654\tvalid_0's ndcg@30: 0.981654\tvalid_0's ndcg@40: 0.981654\tvalid_0's ndcg@50: 0.981654\tvalid_0's ndcg@10: 0.981654\tvalid_0's ndcg@20: 0.981654\tvalid_0's ndcg@30: 0.981654\tvalid_0's ndcg@40: 0.981654\tvalid_0's ndcg@50: 0.981654\n","[66]\tvalid_0's ndcg@10: 0.981693\tvalid_0's ndcg@20: 0.981693\tvalid_0's ndcg@30: 0.981693\tvalid_0's ndcg@40: 0.981693\tvalid_0's ndcg@50: 0.981693\tvalid_0's ndcg@10: 0.981693\tvalid_0's ndcg@20: 0.981693\tvalid_0's ndcg@30: 0.981693\tvalid_0's ndcg@40: 0.981693\tvalid_0's ndcg@50: 0.981693\n","[67]\tvalid_0's ndcg@10: 0.981524\tvalid_0's ndcg@20: 0.981524\tvalid_0's ndcg@30: 0.981524\tvalid_0's ndcg@40: 0.981524\tvalid_0's ndcg@50: 0.981524\tvalid_0's ndcg@10: 0.981524\tvalid_0's ndcg@20: 0.981524\tvalid_0's ndcg@30: 0.981524\tvalid_0's ndcg@40: 0.981524\tvalid_0's ndcg@50: 0.981524\n","[68]\tvalid_0's ndcg@10: 0.981524\tvalid_0's ndcg@20: 0.981524\tvalid_0's ndcg@30: 0.981524\tvalid_0's ndcg@40: 0.981524\tvalid_0's ndcg@50: 0.981524\tvalid_0's ndcg@10: 0.981524\tvalid_0's ndcg@20: 0.981524\tvalid_0's ndcg@30: 0.981524\tvalid_0's ndcg@40: 0.981524\tvalid_0's ndcg@50: 0.981524\n","[69]\tvalid_0's ndcg@10: 0.981656\tvalid_0's ndcg@20: 0.981656\tvalid_0's ndcg@30: 0.981656\tvalid_0's ndcg@40: 0.981656\tvalid_0's ndcg@50: 0.981656\tvalid_0's ndcg@10: 0.981656\tvalid_0's ndcg@20: 0.981656\tvalid_0's ndcg@30: 0.981656\tvalid_0's ndcg@40: 0.981656\tvalid_0's ndcg@50: 0.981656\n","[70]\tvalid_0's ndcg@10: 0.981746\tvalid_0's ndcg@20: 0.981746\tvalid_0's ndcg@30: 0.981746\tvalid_0's ndcg@40: 0.981746\tvalid_0's ndcg@50: 0.981746\tvalid_0's ndcg@10: 0.981746\tvalid_0's ndcg@20: 0.981746\tvalid_0's ndcg@30: 0.981746\tvalid_0's ndcg@40: 0.981746\tvalid_0's ndcg@50: 0.981746\n","[71]\tvalid_0's ndcg@10: 0.981628\tvalid_0's ndcg@20: 0.981628\tvalid_0's ndcg@30: 0.981628\tvalid_0's ndcg@40: 0.981628\tvalid_0's ndcg@50: 0.981628\tvalid_0's ndcg@10: 0.981628\tvalid_0's ndcg@20: 0.981628\tvalid_0's ndcg@30: 0.981628\tvalid_0's ndcg@40: 0.981628\tvalid_0's ndcg@50: 0.981628\n","[72]\tvalid_0's ndcg@10: 0.981628\tvalid_0's ndcg@20: 0.981628\tvalid_0's ndcg@30: 0.981628\tvalid_0's ndcg@40: 0.981628\tvalid_0's ndcg@50: 0.981628\tvalid_0's ndcg@10: 0.981628\tvalid_0's ndcg@20: 0.981628\tvalid_0's ndcg@30: 0.981628\tvalid_0's ndcg@40: 0.981628\tvalid_0's ndcg@50: 0.981628\n","[73]\tvalid_0's ndcg@10: 0.981713\tvalid_0's ndcg@20: 0.981713\tvalid_0's ndcg@30: 0.981713\tvalid_0's ndcg@40: 0.981713\tvalid_0's ndcg@50: 0.981713\tvalid_0's ndcg@10: 0.981713\tvalid_0's ndcg@20: 0.981713\tvalid_0's ndcg@30: 0.981713\tvalid_0's ndcg@40: 0.981713\tvalid_0's ndcg@50: 0.981713\n","[74]\tvalid_0's ndcg@10: 0.981703\tvalid_0's ndcg@20: 0.981703\tvalid_0's ndcg@30: 0.981703\tvalid_0's ndcg@40: 0.981703\tvalid_0's ndcg@50: 0.981703\tvalid_0's ndcg@10: 0.981703\tvalid_0's ndcg@20: 0.981703\tvalid_0's ndcg@30: 0.981703\tvalid_0's ndcg@40: 0.981703\tvalid_0's ndcg@50: 0.981703\n","[75]\tvalid_0's ndcg@10: 0.981753\tvalid_0's ndcg@20: 0.981753\tvalid_0's ndcg@30: 0.981753\tvalid_0's ndcg@40: 0.981753\tvalid_0's ndcg@50: 0.981753\tvalid_0's ndcg@10: 0.981753\tvalid_0's ndcg@20: 0.981753\tvalid_0's ndcg@30: 0.981753\tvalid_0's ndcg@40: 0.981753\tvalid_0's ndcg@50: 0.981753\n","[76]\tvalid_0's ndcg@10: 0.981784\tvalid_0's ndcg@20: 0.981784\tvalid_0's ndcg@30: 0.981784\tvalid_0's ndcg@40: 0.981784\tvalid_0's ndcg@50: 0.981784\tvalid_0's ndcg@10: 0.981784\tvalid_0's ndcg@20: 0.981784\tvalid_0's ndcg@30: 0.981784\tvalid_0's ndcg@40: 0.981784\tvalid_0's ndcg@50: 0.981784\n","[77]\tvalid_0's ndcg@10: 0.982005\tvalid_0's ndcg@20: 0.982005\tvalid_0's ndcg@30: 0.982005\tvalid_0's ndcg@40: 0.982005\tvalid_0's ndcg@50: 0.982005\tvalid_0's ndcg@10: 0.982005\tvalid_0's ndcg@20: 0.982005\tvalid_0's ndcg@30: 0.982005\tvalid_0's ndcg@40: 0.982005\tvalid_0's ndcg@50: 0.982005\n","[78]\tvalid_0's ndcg@10: 0.981917\tvalid_0's ndcg@20: 0.981917\tvalid_0's ndcg@30: 0.981917\tvalid_0's ndcg@40: 0.981917\tvalid_0's ndcg@50: 0.981917\tvalid_0's ndcg@10: 0.981917\tvalid_0's ndcg@20: 0.981917\tvalid_0's ndcg@30: 0.981917\tvalid_0's ndcg@40: 0.981917\tvalid_0's ndcg@50: 0.981917\n","[79]\tvalid_0's ndcg@10: 0.981929\tvalid_0's ndcg@20: 0.981929\tvalid_0's ndcg@30: 0.981929\tvalid_0's ndcg@40: 0.981929\tvalid_0's ndcg@50: 0.981929\tvalid_0's ndcg@10: 0.981929\tvalid_0's ndcg@20: 0.981929\tvalid_0's ndcg@30: 0.981929\tvalid_0's ndcg@40: 0.981929\tvalid_0's ndcg@50: 0.981929\n","[80]\tvalid_0's ndcg@10: 0.981843\tvalid_0's ndcg@20: 0.981843\tvalid_0's ndcg@30: 0.981843\tvalid_0's ndcg@40: 0.981843\tvalid_0's ndcg@50: 0.981843\tvalid_0's ndcg@10: 0.981843\tvalid_0's ndcg@20: 0.981843\tvalid_0's ndcg@30: 0.981843\tvalid_0's ndcg@40: 0.981843\tvalid_0's ndcg@50: 0.981843\n","[81]\tvalid_0's ndcg@10: 0.981763\tvalid_0's ndcg@20: 0.981763\tvalid_0's ndcg@30: 0.981763\tvalid_0's ndcg@40: 0.981763\tvalid_0's ndcg@50: 0.981763\tvalid_0's ndcg@10: 0.981763\tvalid_0's ndcg@20: 0.981763\tvalid_0's ndcg@30: 0.981763\tvalid_0's ndcg@40: 0.981763\tvalid_0's ndcg@50: 0.981763\n","[82]\tvalid_0's ndcg@10: 0.981741\tvalid_0's ndcg@20: 0.981741\tvalid_0's ndcg@30: 0.981741\tvalid_0's ndcg@40: 0.981741\tvalid_0's ndcg@50: 0.981741\tvalid_0's ndcg@10: 0.981741\tvalid_0's ndcg@20: 0.981741\tvalid_0's ndcg@30: 0.981741\tvalid_0's ndcg@40: 0.981741\tvalid_0's ndcg@50: 0.981741\n","[83]\tvalid_0's ndcg@10: 0.981741\tvalid_0's ndcg@20: 0.981741\tvalid_0's ndcg@30: 0.981741\tvalid_0's ndcg@40: 0.981741\tvalid_0's ndcg@50: 0.981741\tvalid_0's ndcg@10: 0.981741\tvalid_0's ndcg@20: 0.981741\tvalid_0's ndcg@30: 0.981741\tvalid_0's ndcg@40: 0.981741\tvalid_0's ndcg@50: 0.981741\n","[84]\tvalid_0's ndcg@10: 0.981818\tvalid_0's ndcg@20: 0.981818\tvalid_0's ndcg@30: 0.981818\tvalid_0's ndcg@40: 0.981818\tvalid_0's ndcg@50: 0.981818\tvalid_0's ndcg@10: 0.981818\tvalid_0's ndcg@20: 0.981818\tvalid_0's ndcg@30: 0.981818\tvalid_0's ndcg@40: 0.981818\tvalid_0's ndcg@50: 0.981818\n","[85]\tvalid_0's ndcg@10: 0.981929\tvalid_0's ndcg@20: 0.981929\tvalid_0's ndcg@30: 0.981929\tvalid_0's ndcg@40: 0.981929\tvalid_0's ndcg@50: 0.981929\tvalid_0's ndcg@10: 0.981929\tvalid_0's ndcg@20: 0.981929\tvalid_0's ndcg@30: 0.981929\tvalid_0's ndcg@40: 0.981929\tvalid_0's ndcg@50: 0.981929\n","[86]\tvalid_0's ndcg@10: 0.981858\tvalid_0's ndcg@20: 0.981858\tvalid_0's ndcg@30: 0.981858\tvalid_0's ndcg@40: 0.981858\tvalid_0's ndcg@50: 0.981858\tvalid_0's ndcg@10: 0.981858\tvalid_0's ndcg@20: 0.981858\tvalid_0's ndcg@30: 0.981858\tvalid_0's ndcg@40: 0.981858\tvalid_0's ndcg@50: 0.981858\n","[87]\tvalid_0's ndcg@10: 0.981802\tvalid_0's ndcg@20: 0.981802\tvalid_0's ndcg@30: 0.981802\tvalid_0's ndcg@40: 0.981802\tvalid_0's ndcg@50: 0.981802\tvalid_0's ndcg@10: 0.981802\tvalid_0's ndcg@20: 0.981802\tvalid_0's ndcg@30: 0.981802\tvalid_0's ndcg@40: 0.981802\tvalid_0's ndcg@50: 0.981802\n","[88]\tvalid_0's ndcg@10: 0.981853\tvalid_0's ndcg@20: 0.981853\tvalid_0's ndcg@30: 0.981853\tvalid_0's ndcg@40: 0.981853\tvalid_0's ndcg@50: 0.981853\tvalid_0's ndcg@10: 0.981853\tvalid_0's ndcg@20: 0.981853\tvalid_0's ndcg@30: 0.981853\tvalid_0's ndcg@40: 0.981853\tvalid_0's ndcg@50: 0.981853\n","[89]\tvalid_0's ndcg@10: 0.981802\tvalid_0's ndcg@20: 0.981802\tvalid_0's ndcg@30: 0.981802\tvalid_0's ndcg@40: 0.981802\tvalid_0's ndcg@50: 0.981802\tvalid_0's ndcg@10: 0.981802\tvalid_0's ndcg@20: 0.981802\tvalid_0's ndcg@30: 0.981802\tvalid_0's ndcg@40: 0.981802\tvalid_0's ndcg@50: 0.981802\n","[90]\tvalid_0's ndcg@10: 0.981802\tvalid_0's ndcg@20: 0.981802\tvalid_0's ndcg@30: 0.981802\tvalid_0's ndcg@40: 0.981802\tvalid_0's ndcg@50: 0.981802\tvalid_0's ndcg@10: 0.981802\tvalid_0's ndcg@20: 0.981802\tvalid_0's ndcg@30: 0.981802\tvalid_0's ndcg@40: 0.981802\tvalid_0's ndcg@50: 0.981802\n","[91]\tvalid_0's ndcg@10: 0.981909\tvalid_0's ndcg@20: 0.981909\tvalid_0's ndcg@30: 0.981909\tvalid_0's ndcg@40: 0.981909\tvalid_0's ndcg@50: 0.981909\tvalid_0's ndcg@10: 0.981909\tvalid_0's ndcg@20: 0.981909\tvalid_0's ndcg@30: 0.981909\tvalid_0's ndcg@40: 0.981909\tvalid_0's ndcg@50: 0.981909\n","[92]\tvalid_0's ndcg@10: 0.981766\tvalid_0's ndcg@20: 0.981766\tvalid_0's ndcg@30: 0.981766\tvalid_0's ndcg@40: 0.981766\tvalid_0's ndcg@50: 0.981766\tvalid_0's ndcg@10: 0.981766\tvalid_0's ndcg@20: 0.981766\tvalid_0's ndcg@30: 0.981766\tvalid_0's ndcg@40: 0.981766\tvalid_0's ndcg@50: 0.981766\n","[93]\tvalid_0's ndcg@10: 0.98177\tvalid_0's ndcg@20: 0.98177\tvalid_0's ndcg@30: 0.98177\tvalid_0's ndcg@40: 0.98177\tvalid_0's ndcg@50: 0.98177\tvalid_0's ndcg@10: 0.98177\tvalid_0's ndcg@20: 0.98177\tvalid_0's ndcg@30: 0.98177\tvalid_0's ndcg@40: 0.98177\tvalid_0's ndcg@50: 0.98177\n","[94]\tvalid_0's ndcg@10: 0.981774\tvalid_0's ndcg@20: 0.981774\tvalid_0's ndcg@30: 0.981774\tvalid_0's ndcg@40: 0.981774\tvalid_0's ndcg@50: 0.981774\tvalid_0's ndcg@10: 0.981774\tvalid_0's ndcg@20: 0.981774\tvalid_0's ndcg@30: 0.981774\tvalid_0's ndcg@40: 0.981774\tvalid_0's ndcg@50: 0.981774\n","[95]\tvalid_0's ndcg@10: 0.981711\tvalid_0's ndcg@20: 0.981711\tvalid_0's ndcg@30: 0.981711\tvalid_0's ndcg@40: 0.981711\tvalid_0's ndcg@50: 0.981711\tvalid_0's ndcg@10: 0.981711\tvalid_0's ndcg@20: 0.981711\tvalid_0's ndcg@30: 0.981711\tvalid_0's ndcg@40: 0.981711\tvalid_0's ndcg@50: 0.981711\n","[96]\tvalid_0's ndcg@10: 0.981537\tvalid_0's ndcg@20: 0.981537\tvalid_0's ndcg@30: 0.981537\tvalid_0's ndcg@40: 0.981537\tvalid_0's ndcg@50: 0.981537\tvalid_0's ndcg@10: 0.981537\tvalid_0's ndcg@20: 0.981537\tvalid_0's ndcg@30: 0.981537\tvalid_0's ndcg@40: 0.981537\tvalid_0's ndcg@50: 0.981537\n","[97]\tvalid_0's ndcg@10: 0.981715\tvalid_0's ndcg@20: 0.981715\tvalid_0's ndcg@30: 0.981715\tvalid_0's ndcg@40: 0.981715\tvalid_0's ndcg@50: 0.981715\tvalid_0's ndcg@10: 0.981715\tvalid_0's ndcg@20: 0.981715\tvalid_0's ndcg@30: 0.981715\tvalid_0's ndcg@40: 0.981715\tvalid_0's ndcg@50: 0.981715\n","[98]\tvalid_0's ndcg@10: 0.981567\tvalid_0's ndcg@20: 0.981567\tvalid_0's ndcg@30: 0.981567\tvalid_0's ndcg@40: 0.981567\tvalid_0's ndcg@50: 0.981567\tvalid_0's ndcg@10: 0.981567\tvalid_0's ndcg@20: 0.981567\tvalid_0's ndcg@30: 0.981567\tvalid_0's ndcg@40: 0.981567\tvalid_0's ndcg@50: 0.981567\n","[99]\tvalid_0's ndcg@10: 0.98151\tvalid_0's ndcg@20: 0.98151\tvalid_0's ndcg@30: 0.98151\tvalid_0's ndcg@40: 0.98151\tvalid_0's ndcg@50: 0.98151\tvalid_0's ndcg@10: 0.98151\tvalid_0's ndcg@20: 0.98151\tvalid_0's ndcg@30: 0.98151\tvalid_0's ndcg@40: 0.98151\tvalid_0's ndcg@50: 0.98151\n","[100]\tvalid_0's ndcg@10: 0.981585\tvalid_0's ndcg@20: 0.981585\tvalid_0's ndcg@30: 0.981585\tvalid_0's ndcg@40: 0.981585\tvalid_0's ndcg@50: 0.981585\tvalid_0's ndcg@10: 0.981585\tvalid_0's ndcg@20: 0.981585\tvalid_0's ndcg@30: 0.981585\tvalid_0's ndcg@40: 0.981585\tvalid_0's ndcg@50: 0.981585\n","Did not meet early stopping. Best iteration is:\n","[77]\tvalid_0's ndcg@10: 0.982005\tvalid_0's ndcg@20: 0.982005\tvalid_0's ndcg@30: 0.982005\tvalid_0's ndcg@40: 0.982005\tvalid_0's ndcg@50: 0.982005\tvalid_0's ndcg@10: 0.982005\tvalid_0's ndcg@20: 0.982005\tvalid_0's ndcg@30: 0.982005\tvalid_0's ndcg@40: 0.982005\tvalid_0's ndcg@50: 0.982005\n","[1]\tvalid_0's ndcg@10: 0.977696\tvalid_0's ndcg@20: 0.977696\tvalid_0's ndcg@30: 0.977696\tvalid_0's ndcg@40: 0.977696\tvalid_0's ndcg@50: 0.977696\tvalid_0's ndcg@10: 0.977696\tvalid_0's ndcg@20: 0.977696\tvalid_0's ndcg@30: 0.977696\tvalid_0's ndcg@40: 0.977696\tvalid_0's ndcg@50: 0.977696\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.979201\tvalid_0's ndcg@20: 0.979201\tvalid_0's ndcg@30: 0.979201\tvalid_0's ndcg@40: 0.979201\tvalid_0's ndcg@50: 0.979201\tvalid_0's ndcg@10: 0.979201\tvalid_0's ndcg@20: 0.979201\tvalid_0's ndcg@30: 0.979201\tvalid_0's ndcg@40: 0.979201\tvalid_0's ndcg@50: 0.979201\n","[3]\tvalid_0's ndcg@10: 0.980587\tvalid_0's ndcg@20: 0.980587\tvalid_0's ndcg@30: 0.980587\tvalid_0's ndcg@40: 0.980587\tvalid_0's ndcg@50: 0.980587\tvalid_0's ndcg@10: 0.980587\tvalid_0's ndcg@20: 0.980587\tvalid_0's ndcg@30: 0.980587\tvalid_0's ndcg@40: 0.980587\tvalid_0's ndcg@50: 0.980587\n","[4]\tvalid_0's ndcg@10: 0.980956\tvalid_0's ndcg@20: 0.980956\tvalid_0's ndcg@30: 0.980956\tvalid_0's ndcg@40: 0.980956\tvalid_0's ndcg@50: 0.980956\tvalid_0's ndcg@10: 0.980956\tvalid_0's ndcg@20: 0.980956\tvalid_0's ndcg@30: 0.980956\tvalid_0's ndcg@40: 0.980956\tvalid_0's ndcg@50: 0.980956\n","[5]\tvalid_0's ndcg@10: 0.980841\tvalid_0's ndcg@20: 0.980841\tvalid_0's ndcg@30: 0.980841\tvalid_0's ndcg@40: 0.980841\tvalid_0's ndcg@50: 0.980841\tvalid_0's ndcg@10: 0.980841\tvalid_0's ndcg@20: 0.980841\tvalid_0's ndcg@30: 0.980841\tvalid_0's ndcg@40: 0.980841\tvalid_0's ndcg@50: 0.980841\n","[6]\tvalid_0's ndcg@10: 0.981115\tvalid_0's ndcg@20: 0.981115\tvalid_0's ndcg@30: 0.981115\tvalid_0's ndcg@40: 0.981115\tvalid_0's ndcg@50: 0.981115\tvalid_0's ndcg@10: 0.981115\tvalid_0's ndcg@20: 0.981115\tvalid_0's ndcg@30: 0.981115\tvalid_0's ndcg@40: 0.981115\tvalid_0's ndcg@50: 0.981115\n","[7]\tvalid_0's ndcg@10: 0.981205\tvalid_0's ndcg@20: 0.981205\tvalid_0's ndcg@30: 0.981205\tvalid_0's ndcg@40: 0.981205\tvalid_0's ndcg@50: 0.981205\tvalid_0's ndcg@10: 0.981205\tvalid_0's ndcg@20: 0.981205\tvalid_0's ndcg@30: 0.981205\tvalid_0's ndcg@40: 0.981205\tvalid_0's ndcg@50: 0.981205\n","[8]\tvalid_0's ndcg@10: 0.981254\tvalid_0's ndcg@20: 0.981254\tvalid_0's ndcg@30: 0.981254\tvalid_0's ndcg@40: 0.981254\tvalid_0's ndcg@50: 0.981254\tvalid_0's ndcg@10: 0.981254\tvalid_0's ndcg@20: 0.981254\tvalid_0's ndcg@30: 0.981254\tvalid_0's ndcg@40: 0.981254\tvalid_0's ndcg@50: 0.981254\n","[9]\tvalid_0's ndcg@10: 0.98149\tvalid_0's ndcg@20: 0.98149\tvalid_0's ndcg@30: 0.98149\tvalid_0's ndcg@40: 0.98149\tvalid_0's ndcg@50: 0.98149\tvalid_0's ndcg@10: 0.98149\tvalid_0's ndcg@20: 0.98149\tvalid_0's ndcg@30: 0.98149\tvalid_0's ndcg@40: 0.98149\tvalid_0's ndcg@50: 0.98149\n","[10]\tvalid_0's ndcg@10: 0.981722\tvalid_0's ndcg@20: 0.981722\tvalid_0's ndcg@30: 0.981722\tvalid_0's ndcg@40: 0.981722\tvalid_0's ndcg@50: 0.981722\tvalid_0's ndcg@10: 0.981722\tvalid_0's ndcg@20: 0.981722\tvalid_0's ndcg@30: 0.981722\tvalid_0's ndcg@40: 0.981722\tvalid_0's ndcg@50: 0.981722\n","[11]\tvalid_0's ndcg@10: 0.981955\tvalid_0's ndcg@20: 0.981955\tvalid_0's ndcg@30: 0.981955\tvalid_0's ndcg@40: 0.981955\tvalid_0's ndcg@50: 0.981955\tvalid_0's ndcg@10: 0.981955\tvalid_0's ndcg@20: 0.981955\tvalid_0's ndcg@30: 0.981955\tvalid_0's ndcg@40: 0.981955\tvalid_0's ndcg@50: 0.981955\n","[12]\tvalid_0's ndcg@10: 0.981913\tvalid_0's ndcg@20: 0.981913\tvalid_0's ndcg@30: 0.981913\tvalid_0's ndcg@40: 0.981913\tvalid_0's ndcg@50: 0.981913\tvalid_0's ndcg@10: 0.981913\tvalid_0's ndcg@20: 0.981913\tvalid_0's ndcg@30: 0.981913\tvalid_0's ndcg@40: 0.981913\tvalid_0's ndcg@50: 0.981913\n","[13]\tvalid_0's ndcg@10: 0.981958\tvalid_0's ndcg@20: 0.981958\tvalid_0's ndcg@30: 0.981958\tvalid_0's ndcg@40: 0.981958\tvalid_0's ndcg@50: 0.981958\tvalid_0's ndcg@10: 0.981958\tvalid_0's ndcg@20: 0.981958\tvalid_0's ndcg@30: 0.981958\tvalid_0's ndcg@40: 0.981958\tvalid_0's ndcg@50: 0.981958\n","[14]\tvalid_0's ndcg@10: 0.981784\tvalid_0's ndcg@20: 0.981784\tvalid_0's ndcg@30: 0.981784\tvalid_0's ndcg@40: 0.981784\tvalid_0's ndcg@50: 0.981784\tvalid_0's ndcg@10: 0.981784\tvalid_0's ndcg@20: 0.981784\tvalid_0's ndcg@30: 0.981784\tvalid_0's ndcg@40: 0.981784\tvalid_0's ndcg@50: 0.981784\n","[15]\tvalid_0's ndcg@10: 0.981894\tvalid_0's ndcg@20: 0.981894\tvalid_0's ndcg@30: 0.981894\tvalid_0's ndcg@40: 0.981894\tvalid_0's ndcg@50: 0.981894\tvalid_0's ndcg@10: 0.981894\tvalid_0's ndcg@20: 0.981894\tvalid_0's ndcg@30: 0.981894\tvalid_0's ndcg@40: 0.981894\tvalid_0's ndcg@50: 0.981894\n","[16]\tvalid_0's ndcg@10: 0.981914\tvalid_0's ndcg@20: 0.981914\tvalid_0's ndcg@30: 0.981914\tvalid_0's ndcg@40: 0.981914\tvalid_0's ndcg@50: 0.981914\tvalid_0's ndcg@10: 0.981914\tvalid_0's ndcg@20: 0.981914\tvalid_0's ndcg@30: 0.981914\tvalid_0's ndcg@40: 0.981914\tvalid_0's ndcg@50: 0.981914\n","[17]\tvalid_0's ndcg@10: 0.982059\tvalid_0's ndcg@20: 0.982059\tvalid_0's ndcg@30: 0.982059\tvalid_0's ndcg@40: 0.982059\tvalid_0's ndcg@50: 0.982059\tvalid_0's ndcg@10: 0.982059\tvalid_0's ndcg@20: 0.982059\tvalid_0's ndcg@30: 0.982059\tvalid_0's ndcg@40: 0.982059\tvalid_0's ndcg@50: 0.982059\n","[18]\tvalid_0's ndcg@10: 0.981925\tvalid_0's ndcg@20: 0.981925\tvalid_0's ndcg@30: 0.981925\tvalid_0's ndcg@40: 0.981925\tvalid_0's ndcg@50: 0.981925\tvalid_0's ndcg@10: 0.981925\tvalid_0's ndcg@20: 0.981925\tvalid_0's ndcg@30: 0.981925\tvalid_0's ndcg@40: 0.981925\tvalid_0's ndcg@50: 0.981925\n","[19]\tvalid_0's ndcg@10: 0.981787\tvalid_0's ndcg@20: 0.981787\tvalid_0's ndcg@30: 0.981787\tvalid_0's ndcg@40: 0.981787\tvalid_0's ndcg@50: 0.981787\tvalid_0's ndcg@10: 0.981787\tvalid_0's ndcg@20: 0.981787\tvalid_0's ndcg@30: 0.981787\tvalid_0's ndcg@40: 0.981787\tvalid_0's ndcg@50: 0.981787\n","[20]\tvalid_0's ndcg@10: 0.98147\tvalid_0's ndcg@20: 0.98147\tvalid_0's ndcg@30: 0.98147\tvalid_0's ndcg@40: 0.98147\tvalid_0's ndcg@50: 0.98147\tvalid_0's ndcg@10: 0.98147\tvalid_0's ndcg@20: 0.98147\tvalid_0's ndcg@30: 0.98147\tvalid_0's ndcg@40: 0.98147\tvalid_0's ndcg@50: 0.98147\n","[21]\tvalid_0's ndcg@10: 0.981616\tvalid_0's ndcg@20: 0.981616\tvalid_0's ndcg@30: 0.981616\tvalid_0's ndcg@40: 0.981616\tvalid_0's ndcg@50: 0.981616\tvalid_0's ndcg@10: 0.981616\tvalid_0's ndcg@20: 0.981616\tvalid_0's ndcg@30: 0.981616\tvalid_0's ndcg@40: 0.981616\tvalid_0's ndcg@50: 0.981616\n","[22]\tvalid_0's ndcg@10: 0.981464\tvalid_0's ndcg@20: 0.981464\tvalid_0's ndcg@30: 0.981464\tvalid_0's ndcg@40: 0.981464\tvalid_0's ndcg@50: 0.981464\tvalid_0's ndcg@10: 0.981464\tvalid_0's ndcg@20: 0.981464\tvalid_0's ndcg@30: 0.981464\tvalid_0's ndcg@40: 0.981464\tvalid_0's ndcg@50: 0.981464\n","[23]\tvalid_0's ndcg@10: 0.981788\tvalid_0's ndcg@20: 0.981788\tvalid_0's ndcg@30: 0.981788\tvalid_0's ndcg@40: 0.981788\tvalid_0's ndcg@50: 0.981788\tvalid_0's ndcg@10: 0.981788\tvalid_0's ndcg@20: 0.981788\tvalid_0's ndcg@30: 0.981788\tvalid_0's ndcg@40: 0.981788\tvalid_0's ndcg@50: 0.981788\n","[24]\tvalid_0's ndcg@10: 0.981879\tvalid_0's ndcg@20: 0.981879\tvalid_0's ndcg@30: 0.981879\tvalid_0's ndcg@40: 0.981879\tvalid_0's ndcg@50: 0.981879\tvalid_0's ndcg@10: 0.981879\tvalid_0's ndcg@20: 0.981879\tvalid_0's ndcg@30: 0.981879\tvalid_0's ndcg@40: 0.981879\tvalid_0's ndcg@50: 0.981879\n","[25]\tvalid_0's ndcg@10: 0.981866\tvalid_0's ndcg@20: 0.981866\tvalid_0's ndcg@30: 0.981866\tvalid_0's ndcg@40: 0.981866\tvalid_0's ndcg@50: 0.981866\tvalid_0's ndcg@10: 0.981866\tvalid_0's ndcg@20: 0.981866\tvalid_0's ndcg@30: 0.981866\tvalid_0's ndcg@40: 0.981866\tvalid_0's ndcg@50: 0.981866\n","[26]\tvalid_0's ndcg@10: 0.981625\tvalid_0's ndcg@20: 0.981625\tvalid_0's ndcg@30: 0.981625\tvalid_0's ndcg@40: 0.981625\tvalid_0's ndcg@50: 0.981625\tvalid_0's ndcg@10: 0.981625\tvalid_0's ndcg@20: 0.981625\tvalid_0's ndcg@30: 0.981625\tvalid_0's ndcg@40: 0.981625\tvalid_0's ndcg@50: 0.981625\n","[27]\tvalid_0's ndcg@10: 0.981627\tvalid_0's ndcg@20: 0.981627\tvalid_0's ndcg@30: 0.981627\tvalid_0's ndcg@40: 0.981627\tvalid_0's ndcg@50: 0.981627\tvalid_0's ndcg@10: 0.981627\tvalid_0's ndcg@20: 0.981627\tvalid_0's ndcg@30: 0.981627\tvalid_0's ndcg@40: 0.981627\tvalid_0's ndcg@50: 0.981627\n","[28]\tvalid_0's ndcg@10: 0.981779\tvalid_0's ndcg@20: 0.981779\tvalid_0's ndcg@30: 0.981779\tvalid_0's ndcg@40: 0.981779\tvalid_0's ndcg@50: 0.981779\tvalid_0's ndcg@10: 0.981779\tvalid_0's ndcg@20: 0.981779\tvalid_0's ndcg@30: 0.981779\tvalid_0's ndcg@40: 0.981779\tvalid_0's ndcg@50: 0.981779\n","[29]\tvalid_0's ndcg@10: 0.981739\tvalid_0's ndcg@20: 0.981739\tvalid_0's ndcg@30: 0.981739\tvalid_0's ndcg@40: 0.981739\tvalid_0's ndcg@50: 0.981739\tvalid_0's ndcg@10: 0.981739\tvalid_0's ndcg@20: 0.981739\tvalid_0's ndcg@30: 0.981739\tvalid_0's ndcg@40: 0.981739\tvalid_0's ndcg@50: 0.981739\n","[30]\tvalid_0's ndcg@10: 0.981587\tvalid_0's ndcg@20: 0.981587\tvalid_0's ndcg@30: 0.981587\tvalid_0's ndcg@40: 0.981587\tvalid_0's ndcg@50: 0.981587\tvalid_0's ndcg@10: 0.981587\tvalid_0's ndcg@20: 0.981587\tvalid_0's ndcg@30: 0.981587\tvalid_0's ndcg@40: 0.981587\tvalid_0's ndcg@50: 0.981587\n","[31]\tvalid_0's ndcg@10: 0.981657\tvalid_0's ndcg@20: 0.981657\tvalid_0's ndcg@30: 0.981657\tvalid_0's ndcg@40: 0.981657\tvalid_0's ndcg@50: 0.981657\tvalid_0's ndcg@10: 0.981657\tvalid_0's ndcg@20: 0.981657\tvalid_0's ndcg@30: 0.981657\tvalid_0's ndcg@40: 0.981657\tvalid_0's ndcg@50: 0.981657\n","[32]\tvalid_0's ndcg@10: 0.981577\tvalid_0's ndcg@20: 0.981577\tvalid_0's ndcg@30: 0.981577\tvalid_0's ndcg@40: 0.981577\tvalid_0's ndcg@50: 0.981577\tvalid_0's ndcg@10: 0.981577\tvalid_0's ndcg@20: 0.981577\tvalid_0's ndcg@30: 0.981577\tvalid_0's ndcg@40: 0.981577\tvalid_0's ndcg@50: 0.981577\n","[33]\tvalid_0's ndcg@10: 0.981681\tvalid_0's ndcg@20: 0.981681\tvalid_0's ndcg@30: 0.981681\tvalid_0's ndcg@40: 0.981681\tvalid_0's ndcg@50: 0.981681\tvalid_0's ndcg@10: 0.981681\tvalid_0's ndcg@20: 0.981681\tvalid_0's ndcg@30: 0.981681\tvalid_0's ndcg@40: 0.981681\tvalid_0's ndcg@50: 0.981681\n","[34]\tvalid_0's ndcg@10: 0.981875\tvalid_0's ndcg@20: 0.981875\tvalid_0's ndcg@30: 0.981875\tvalid_0's ndcg@40: 0.981875\tvalid_0's ndcg@50: 0.981875\tvalid_0's ndcg@10: 0.981875\tvalid_0's ndcg@20: 0.981875\tvalid_0's ndcg@30: 0.981875\tvalid_0's ndcg@40: 0.981875\tvalid_0's ndcg@50: 0.981875\n","[35]\tvalid_0's ndcg@10: 0.981973\tvalid_0's ndcg@20: 0.981973\tvalid_0's ndcg@30: 0.981973\tvalid_0's ndcg@40: 0.981973\tvalid_0's ndcg@50: 0.981973\tvalid_0's ndcg@10: 0.981973\tvalid_0's ndcg@20: 0.981973\tvalid_0's ndcg@30: 0.981973\tvalid_0's ndcg@40: 0.981973\tvalid_0's ndcg@50: 0.981973\n","[36]\tvalid_0's ndcg@10: 0.981813\tvalid_0's ndcg@20: 0.981813\tvalid_0's ndcg@30: 0.981813\tvalid_0's ndcg@40: 0.981813\tvalid_0's ndcg@50: 0.981813\tvalid_0's ndcg@10: 0.981813\tvalid_0's ndcg@20: 0.981813\tvalid_0's ndcg@30: 0.981813\tvalid_0's ndcg@40: 0.981813\tvalid_0's ndcg@50: 0.981813\n","[37]\tvalid_0's ndcg@10: 0.981985\tvalid_0's ndcg@20: 0.981985\tvalid_0's ndcg@30: 0.981985\tvalid_0's ndcg@40: 0.981985\tvalid_0's ndcg@50: 0.981985\tvalid_0's ndcg@10: 0.981985\tvalid_0's ndcg@20: 0.981985\tvalid_0's ndcg@30: 0.981985\tvalid_0's ndcg@40: 0.981985\tvalid_0's ndcg@50: 0.981985\n","[38]\tvalid_0's ndcg@10: 0.981861\tvalid_0's ndcg@20: 0.981861\tvalid_0's ndcg@30: 0.981861\tvalid_0's ndcg@40: 0.981861\tvalid_0's ndcg@50: 0.981861\tvalid_0's ndcg@10: 0.981861\tvalid_0's ndcg@20: 0.981861\tvalid_0's ndcg@30: 0.981861\tvalid_0's ndcg@40: 0.981861\tvalid_0's ndcg@50: 0.981861\n","[39]\tvalid_0's ndcg@10: 0.982008\tvalid_0's ndcg@20: 0.982008\tvalid_0's ndcg@30: 0.982008\tvalid_0's ndcg@40: 0.982008\tvalid_0's ndcg@50: 0.982008\tvalid_0's ndcg@10: 0.982008\tvalid_0's ndcg@20: 0.982008\tvalid_0's ndcg@30: 0.982008\tvalid_0's ndcg@40: 0.982008\tvalid_0's ndcg@50: 0.982008\n","[40]\tvalid_0's ndcg@10: 0.981998\tvalid_0's ndcg@20: 0.981998\tvalid_0's ndcg@30: 0.981998\tvalid_0's ndcg@40: 0.981998\tvalid_0's ndcg@50: 0.981998\tvalid_0's ndcg@10: 0.981998\tvalid_0's ndcg@20: 0.981998\tvalid_0's ndcg@30: 0.981998\tvalid_0's ndcg@40: 0.981998\tvalid_0's ndcg@50: 0.981998\n","[41]\tvalid_0's ndcg@10: 0.981884\tvalid_0's ndcg@20: 0.981884\tvalid_0's ndcg@30: 0.981884\tvalid_0's ndcg@40: 0.981884\tvalid_0's ndcg@50: 0.981884\tvalid_0's ndcg@10: 0.981884\tvalid_0's ndcg@20: 0.981884\tvalid_0's ndcg@30: 0.981884\tvalid_0's ndcg@40: 0.981884\tvalid_0's ndcg@50: 0.981884\n","[42]\tvalid_0's ndcg@10: 0.981974\tvalid_0's ndcg@20: 0.981974\tvalid_0's ndcg@30: 0.981974\tvalid_0's ndcg@40: 0.981974\tvalid_0's ndcg@50: 0.981974\tvalid_0's ndcg@10: 0.981974\tvalid_0's ndcg@20: 0.981974\tvalid_0's ndcg@30: 0.981974\tvalid_0's ndcg@40: 0.981974\tvalid_0's ndcg@50: 0.981974\n","[43]\tvalid_0's ndcg@10: 0.981981\tvalid_0's ndcg@20: 0.981981\tvalid_0's ndcg@30: 0.981981\tvalid_0's ndcg@40: 0.981981\tvalid_0's ndcg@50: 0.981981\tvalid_0's ndcg@10: 0.981981\tvalid_0's ndcg@20: 0.981981\tvalid_0's ndcg@30: 0.981981\tvalid_0's ndcg@40: 0.981981\tvalid_0's ndcg@50: 0.981981\n","[44]\tvalid_0's ndcg@10: 0.981895\tvalid_0's ndcg@20: 0.981895\tvalid_0's ndcg@30: 0.981895\tvalid_0's ndcg@40: 0.981895\tvalid_0's ndcg@50: 0.981895\tvalid_0's ndcg@10: 0.981895\tvalid_0's ndcg@20: 0.981895\tvalid_0's ndcg@30: 0.981895\tvalid_0's ndcg@40: 0.981895\tvalid_0's ndcg@50: 0.981895\n","[45]\tvalid_0's ndcg@10: 0.981816\tvalid_0's ndcg@20: 0.981816\tvalid_0's ndcg@30: 0.981816\tvalid_0's ndcg@40: 0.981816\tvalid_0's ndcg@50: 0.981816\tvalid_0's ndcg@10: 0.981816\tvalid_0's ndcg@20: 0.981816\tvalid_0's ndcg@30: 0.981816\tvalid_0's ndcg@40: 0.981816\tvalid_0's ndcg@50: 0.981816\n","[46]\tvalid_0's ndcg@10: 0.981609\tvalid_0's ndcg@20: 0.981609\tvalid_0's ndcg@30: 0.981609\tvalid_0's ndcg@40: 0.981609\tvalid_0's ndcg@50: 0.981609\tvalid_0's ndcg@10: 0.981609\tvalid_0's ndcg@20: 0.981609\tvalid_0's ndcg@30: 0.981609\tvalid_0's ndcg@40: 0.981609\tvalid_0's ndcg@50: 0.981609\n","[47]\tvalid_0's ndcg@10: 0.981653\tvalid_0's ndcg@20: 0.981653\tvalid_0's ndcg@30: 0.981653\tvalid_0's ndcg@40: 0.981653\tvalid_0's ndcg@50: 0.981653\tvalid_0's ndcg@10: 0.981653\tvalid_0's ndcg@20: 0.981653\tvalid_0's ndcg@30: 0.981653\tvalid_0's ndcg@40: 0.981653\tvalid_0's ndcg@50: 0.981653\n","[48]\tvalid_0's ndcg@10: 0.981787\tvalid_0's ndcg@20: 0.981787\tvalid_0's ndcg@30: 0.981787\tvalid_0's ndcg@40: 0.981787\tvalid_0's ndcg@50: 0.981787\tvalid_0's ndcg@10: 0.981787\tvalid_0's ndcg@20: 0.981787\tvalid_0's ndcg@30: 0.981787\tvalid_0's ndcg@40: 0.981787\tvalid_0's ndcg@50: 0.981787\n","[49]\tvalid_0's ndcg@10: 0.981887\tvalid_0's ndcg@20: 0.981887\tvalid_0's ndcg@30: 0.981887\tvalid_0's ndcg@40: 0.981887\tvalid_0's ndcg@50: 0.981887\tvalid_0's ndcg@10: 0.981887\tvalid_0's ndcg@20: 0.981887\tvalid_0's ndcg@30: 0.981887\tvalid_0's ndcg@40: 0.981887\tvalid_0's ndcg@50: 0.981887\n","[50]\tvalid_0's ndcg@10: 0.981682\tvalid_0's ndcg@20: 0.981682\tvalid_0's ndcg@30: 0.981682\tvalid_0's ndcg@40: 0.981682\tvalid_0's ndcg@50: 0.981682\tvalid_0's ndcg@10: 0.981682\tvalid_0's ndcg@20: 0.981682\tvalid_0's ndcg@30: 0.981682\tvalid_0's ndcg@40: 0.981682\tvalid_0's ndcg@50: 0.981682\n","[51]\tvalid_0's ndcg@10: 0.981779\tvalid_0's ndcg@20: 0.981779\tvalid_0's ndcg@30: 0.981779\tvalid_0's ndcg@40: 0.981779\tvalid_0's ndcg@50: 0.981779\tvalid_0's ndcg@10: 0.981779\tvalid_0's ndcg@20: 0.981779\tvalid_0's ndcg@30: 0.981779\tvalid_0's ndcg@40: 0.981779\tvalid_0's ndcg@50: 0.981779\n","[52]\tvalid_0's ndcg@10: 0.981689\tvalid_0's ndcg@20: 0.981689\tvalid_0's ndcg@30: 0.981689\tvalid_0's ndcg@40: 0.981689\tvalid_0's ndcg@50: 0.981689\tvalid_0's ndcg@10: 0.981689\tvalid_0's ndcg@20: 0.981689\tvalid_0's ndcg@30: 0.981689\tvalid_0's ndcg@40: 0.981689\tvalid_0's ndcg@50: 0.981689\n","[53]\tvalid_0's ndcg@10: 0.981701\tvalid_0's ndcg@20: 0.981701\tvalid_0's ndcg@30: 0.981701\tvalid_0's ndcg@40: 0.981701\tvalid_0's ndcg@50: 0.981701\tvalid_0's ndcg@10: 0.981701\tvalid_0's ndcg@20: 0.981701\tvalid_0's ndcg@30: 0.981701\tvalid_0's ndcg@40: 0.981701\tvalid_0's ndcg@50: 0.981701\n","[54]\tvalid_0's ndcg@10: 0.981653\tvalid_0's ndcg@20: 0.981653\tvalid_0's ndcg@30: 0.981653\tvalid_0's ndcg@40: 0.981653\tvalid_0's ndcg@50: 0.981653\tvalid_0's ndcg@10: 0.981653\tvalid_0's ndcg@20: 0.981653\tvalid_0's ndcg@30: 0.981653\tvalid_0's ndcg@40: 0.981653\tvalid_0's ndcg@50: 0.981653\n","[55]\tvalid_0's ndcg@10: 0.981613\tvalid_0's ndcg@20: 0.981613\tvalid_0's ndcg@30: 0.981613\tvalid_0's ndcg@40: 0.981613\tvalid_0's ndcg@50: 0.981613\tvalid_0's ndcg@10: 0.981613\tvalid_0's ndcg@20: 0.981613\tvalid_0's ndcg@30: 0.981613\tvalid_0's ndcg@40: 0.981613\tvalid_0's ndcg@50: 0.981613\n","[56]\tvalid_0's ndcg@10: 0.981562\tvalid_0's ndcg@20: 0.981562\tvalid_0's ndcg@30: 0.981562\tvalid_0's ndcg@40: 0.981562\tvalid_0's ndcg@50: 0.981562\tvalid_0's ndcg@10: 0.981562\tvalid_0's ndcg@20: 0.981562\tvalid_0's ndcg@30: 0.981562\tvalid_0's ndcg@40: 0.981562\tvalid_0's ndcg@50: 0.981562\n","[57]\tvalid_0's ndcg@10: 0.98172\tvalid_0's ndcg@20: 0.98172\tvalid_0's ndcg@30: 0.98172\tvalid_0's ndcg@40: 0.98172\tvalid_0's ndcg@50: 0.98172\tvalid_0's ndcg@10: 0.98172\tvalid_0's ndcg@20: 0.98172\tvalid_0's ndcg@30: 0.98172\tvalid_0's ndcg@40: 0.98172\tvalid_0's ndcg@50: 0.98172\n","[58]\tvalid_0's ndcg@10: 0.981739\tvalid_0's ndcg@20: 0.981739\tvalid_0's ndcg@30: 0.981739\tvalid_0's ndcg@40: 0.981739\tvalid_0's ndcg@50: 0.981739\tvalid_0's ndcg@10: 0.981739\tvalid_0's ndcg@20: 0.981739\tvalid_0's ndcg@30: 0.981739\tvalid_0's ndcg@40: 0.981739\tvalid_0's ndcg@50: 0.981739\n","[59]\tvalid_0's ndcg@10: 0.981722\tvalid_0's ndcg@20: 0.981722\tvalid_0's ndcg@30: 0.981722\tvalid_0's ndcg@40: 0.981722\tvalid_0's ndcg@50: 0.981722\tvalid_0's ndcg@10: 0.981722\tvalid_0's ndcg@20: 0.981722\tvalid_0's ndcg@30: 0.981722\tvalid_0's ndcg@40: 0.981722\tvalid_0's ndcg@50: 0.981722\n","[60]\tvalid_0's ndcg@10: 0.981807\tvalid_0's ndcg@20: 0.981807\tvalid_0's ndcg@30: 0.981807\tvalid_0's ndcg@40: 0.981807\tvalid_0's ndcg@50: 0.981807\tvalid_0's ndcg@10: 0.981807\tvalid_0's ndcg@20: 0.981807\tvalid_0's ndcg@30: 0.981807\tvalid_0's ndcg@40: 0.981807\tvalid_0's ndcg@50: 0.981807\n","[61]\tvalid_0's ndcg@10: 0.98183\tvalid_0's ndcg@20: 0.98183\tvalid_0's ndcg@30: 0.98183\tvalid_0's ndcg@40: 0.98183\tvalid_0's ndcg@50: 0.98183\tvalid_0's ndcg@10: 0.98183\tvalid_0's ndcg@20: 0.98183\tvalid_0's ndcg@30: 0.98183\tvalid_0's ndcg@40: 0.98183\tvalid_0's ndcg@50: 0.98183\n","[62]\tvalid_0's ndcg@10: 0.981785\tvalid_0's ndcg@20: 0.981785\tvalid_0's ndcg@30: 0.981785\tvalid_0's ndcg@40: 0.981785\tvalid_0's ndcg@50: 0.981785\tvalid_0's ndcg@10: 0.981785\tvalid_0's ndcg@20: 0.981785\tvalid_0's ndcg@30: 0.981785\tvalid_0's ndcg@40: 0.981785\tvalid_0's ndcg@50: 0.981785\n","[63]\tvalid_0's ndcg@10: 0.981937\tvalid_0's ndcg@20: 0.981937\tvalid_0's ndcg@30: 0.981937\tvalid_0's ndcg@40: 0.981937\tvalid_0's ndcg@50: 0.981937\tvalid_0's ndcg@10: 0.981937\tvalid_0's ndcg@20: 0.981937\tvalid_0's ndcg@30: 0.981937\tvalid_0's ndcg@40: 0.981937\tvalid_0's ndcg@50: 0.981937\n","[64]\tvalid_0's ndcg@10: 0.98186\tvalid_0's ndcg@20: 0.98186\tvalid_0's ndcg@30: 0.98186\tvalid_0's ndcg@40: 0.98186\tvalid_0's ndcg@50: 0.98186\tvalid_0's ndcg@10: 0.98186\tvalid_0's ndcg@20: 0.98186\tvalid_0's ndcg@30: 0.98186\tvalid_0's ndcg@40: 0.98186\tvalid_0's ndcg@50: 0.98186\n","[65]\tvalid_0's ndcg@10: 0.9819\tvalid_0's ndcg@20: 0.9819\tvalid_0's ndcg@30: 0.9819\tvalid_0's ndcg@40: 0.9819\tvalid_0's ndcg@50: 0.9819\tvalid_0's ndcg@10: 0.9819\tvalid_0's ndcg@20: 0.9819\tvalid_0's ndcg@30: 0.9819\tvalid_0's ndcg@40: 0.9819\tvalid_0's ndcg@50: 0.9819\n","[66]\tvalid_0's ndcg@10: 0.981995\tvalid_0's ndcg@20: 0.981995\tvalid_0's ndcg@30: 0.981995\tvalid_0's ndcg@40: 0.981995\tvalid_0's ndcg@50: 0.981995\tvalid_0's ndcg@10: 0.981995\tvalid_0's ndcg@20: 0.981995\tvalid_0's ndcg@30: 0.981995\tvalid_0's ndcg@40: 0.981995\tvalid_0's ndcg@50: 0.981995\n","[67]\tvalid_0's ndcg@10: 0.981951\tvalid_0's ndcg@20: 0.981951\tvalid_0's ndcg@30: 0.981951\tvalid_0's ndcg@40: 0.981951\tvalid_0's ndcg@50: 0.981951\tvalid_0's ndcg@10: 0.981951\tvalid_0's ndcg@20: 0.981951\tvalid_0's ndcg@30: 0.981951\tvalid_0's ndcg@40: 0.981951\tvalid_0's ndcg@50: 0.981951\n","Early stopping, best iteration is:\n","[17]\tvalid_0's ndcg@10: 0.982059\tvalid_0's ndcg@20: 0.982059\tvalid_0's ndcg@30: 0.982059\tvalid_0's ndcg@40: 0.982059\tvalid_0's ndcg@50: 0.982059\tvalid_0's ndcg@10: 0.982059\tvalid_0's ndcg@20: 0.982059\tvalid_0's ndcg@30: 0.982059\tvalid_0's ndcg@40: 0.982059\tvalid_0's ndcg@50: 0.982059\n","[1]\tvalid_0's ndcg@10: 0.977499\tvalid_0's ndcg@20: 0.977546\tvalid_0's ndcg@30: 0.977546\tvalid_0's ndcg@40: 0.977546\tvalid_0's ndcg@50: 0.977546\tvalid_0's ndcg@10: 0.977499\tvalid_0's ndcg@20: 0.977546\tvalid_0's ndcg@30: 0.977546\tvalid_0's ndcg@40: 0.977546\tvalid_0's ndcg@50: 0.977546\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.978631\tvalid_0's ndcg@20: 0.978631\tvalid_0's ndcg@30: 0.978631\tvalid_0's ndcg@40: 0.978631\tvalid_0's ndcg@50: 0.978631\tvalid_0's ndcg@10: 0.978631\tvalid_0's ndcg@20: 0.978631\tvalid_0's ndcg@30: 0.978631\tvalid_0's ndcg@40: 0.978631\tvalid_0's ndcg@50: 0.978631\n","[3]\tvalid_0's ndcg@10: 0.980602\tvalid_0's ndcg@20: 0.980602\tvalid_0's ndcg@30: 0.980602\tvalid_0's ndcg@40: 0.980602\tvalid_0's ndcg@50: 0.980602\tvalid_0's ndcg@10: 0.980602\tvalid_0's ndcg@20: 0.980602\tvalid_0's ndcg@30: 0.980602\tvalid_0's ndcg@40: 0.980602\tvalid_0's ndcg@50: 0.980602\n","[4]\tvalid_0's ndcg@10: 0.980612\tvalid_0's ndcg@20: 0.980612\tvalid_0's ndcg@30: 0.980612\tvalid_0's ndcg@40: 0.980612\tvalid_0's ndcg@50: 0.980612\tvalid_0's ndcg@10: 0.980612\tvalid_0's ndcg@20: 0.980612\tvalid_0's ndcg@30: 0.980612\tvalid_0's ndcg@40: 0.980612\tvalid_0's ndcg@50: 0.980612\n","[5]\tvalid_0's ndcg@10: 0.980429\tvalid_0's ndcg@20: 0.980429\tvalid_0's ndcg@30: 0.980429\tvalid_0's ndcg@40: 0.980429\tvalid_0's ndcg@50: 0.980429\tvalid_0's ndcg@10: 0.980429\tvalid_0's ndcg@20: 0.980429\tvalid_0's ndcg@30: 0.980429\tvalid_0's ndcg@40: 0.980429\tvalid_0's ndcg@50: 0.980429\n","[6]\tvalid_0's ndcg@10: 0.980417\tvalid_0's ndcg@20: 0.980417\tvalid_0's ndcg@30: 0.980417\tvalid_0's ndcg@40: 0.980417\tvalid_0's ndcg@50: 0.980417\tvalid_0's ndcg@10: 0.980417\tvalid_0's ndcg@20: 0.980417\tvalid_0's ndcg@30: 0.980417\tvalid_0's ndcg@40: 0.980417\tvalid_0's ndcg@50: 0.980417\n","[7]\tvalid_0's ndcg@10: 0.980837\tvalid_0's ndcg@20: 0.980837\tvalid_0's ndcg@30: 0.980837\tvalid_0's ndcg@40: 0.980837\tvalid_0's ndcg@50: 0.980837\tvalid_0's ndcg@10: 0.980837\tvalid_0's ndcg@20: 0.980837\tvalid_0's ndcg@30: 0.980837\tvalid_0's ndcg@40: 0.980837\tvalid_0's ndcg@50: 0.980837\n","[8]\tvalid_0's ndcg@10: 0.980864\tvalid_0's ndcg@20: 0.980864\tvalid_0's ndcg@30: 0.980864\tvalid_0's ndcg@40: 0.980864\tvalid_0's ndcg@50: 0.980864\tvalid_0's ndcg@10: 0.980864\tvalid_0's ndcg@20: 0.980864\tvalid_0's ndcg@30: 0.980864\tvalid_0's ndcg@40: 0.980864\tvalid_0's ndcg@50: 0.980864\n","[9]\tvalid_0's ndcg@10: 0.980764\tvalid_0's ndcg@20: 0.980764\tvalid_0's ndcg@30: 0.980764\tvalid_0's ndcg@40: 0.980764\tvalid_0's ndcg@50: 0.980764\tvalid_0's ndcg@10: 0.980764\tvalid_0's ndcg@20: 0.980764\tvalid_0's ndcg@30: 0.980764\tvalid_0's ndcg@40: 0.980764\tvalid_0's ndcg@50: 0.980764\n","[10]\tvalid_0's ndcg@10: 0.98102\tvalid_0's ndcg@20: 0.98102\tvalid_0's ndcg@30: 0.98102\tvalid_0's ndcg@40: 0.98102\tvalid_0's ndcg@50: 0.98102\tvalid_0's ndcg@10: 0.98102\tvalid_0's ndcg@20: 0.98102\tvalid_0's ndcg@30: 0.98102\tvalid_0's ndcg@40: 0.98102\tvalid_0's ndcg@50: 0.98102\n","[11]\tvalid_0's ndcg@10: 0.981276\tvalid_0's ndcg@20: 0.981276\tvalid_0's ndcg@30: 0.981276\tvalid_0's ndcg@40: 0.981276\tvalid_0's ndcg@50: 0.981276\tvalid_0's ndcg@10: 0.981276\tvalid_0's ndcg@20: 0.981276\tvalid_0's ndcg@30: 0.981276\tvalid_0's ndcg@40: 0.981276\tvalid_0's ndcg@50: 0.981276\n","[12]\tvalid_0's ndcg@10: 0.981341\tvalid_0's ndcg@20: 0.981341\tvalid_0's ndcg@30: 0.981341\tvalid_0's ndcg@40: 0.981341\tvalid_0's ndcg@50: 0.981341\tvalid_0's ndcg@10: 0.981341\tvalid_0's ndcg@20: 0.981341\tvalid_0's ndcg@30: 0.981341\tvalid_0's ndcg@40: 0.981341\tvalid_0's ndcg@50: 0.981341\n","[13]\tvalid_0's ndcg@10: 0.981391\tvalid_0's ndcg@20: 0.981391\tvalid_0's ndcg@30: 0.981391\tvalid_0's ndcg@40: 0.981391\tvalid_0's ndcg@50: 0.981391\tvalid_0's ndcg@10: 0.981391\tvalid_0's ndcg@20: 0.981391\tvalid_0's ndcg@30: 0.981391\tvalid_0's ndcg@40: 0.981391\tvalid_0's ndcg@50: 0.981391\n","[14]\tvalid_0's ndcg@10: 0.981366\tvalid_0's ndcg@20: 0.981366\tvalid_0's ndcg@30: 0.981366\tvalid_0's ndcg@40: 0.981366\tvalid_0's ndcg@50: 0.981366\tvalid_0's ndcg@10: 0.981366\tvalid_0's ndcg@20: 0.981366\tvalid_0's ndcg@30: 0.981366\tvalid_0's ndcg@40: 0.981366\tvalid_0's ndcg@50: 0.981366\n","[15]\tvalid_0's ndcg@10: 0.981211\tvalid_0's ndcg@20: 0.981211\tvalid_0's ndcg@30: 0.981211\tvalid_0's ndcg@40: 0.981211\tvalid_0's ndcg@50: 0.981211\tvalid_0's ndcg@10: 0.981211\tvalid_0's ndcg@20: 0.981211\tvalid_0's ndcg@30: 0.981211\tvalid_0's ndcg@40: 0.981211\tvalid_0's ndcg@50: 0.981211\n","[16]\tvalid_0's ndcg@10: 0.981346\tvalid_0's ndcg@20: 0.981346\tvalid_0's ndcg@30: 0.981346\tvalid_0's ndcg@40: 0.981346\tvalid_0's ndcg@50: 0.981346\tvalid_0's ndcg@10: 0.981346\tvalid_0's ndcg@20: 0.981346\tvalid_0's ndcg@30: 0.981346\tvalid_0's ndcg@40: 0.981346\tvalid_0's ndcg@50: 0.981346\n","[17]\tvalid_0's ndcg@10: 0.98122\tvalid_0's ndcg@20: 0.98122\tvalid_0's ndcg@30: 0.98122\tvalid_0's ndcg@40: 0.98122\tvalid_0's ndcg@50: 0.98122\tvalid_0's ndcg@10: 0.98122\tvalid_0's ndcg@20: 0.98122\tvalid_0's ndcg@30: 0.98122\tvalid_0's ndcg@40: 0.98122\tvalid_0's ndcg@50: 0.98122\n","[18]\tvalid_0's ndcg@10: 0.981684\tvalid_0's ndcg@20: 0.981684\tvalid_0's ndcg@30: 0.981684\tvalid_0's ndcg@40: 0.981684\tvalid_0's ndcg@50: 0.981684\tvalid_0's ndcg@10: 0.981684\tvalid_0's ndcg@20: 0.981684\tvalid_0's ndcg@30: 0.981684\tvalid_0's ndcg@40: 0.981684\tvalid_0's ndcg@50: 0.981684\n","[19]\tvalid_0's ndcg@10: 0.981473\tvalid_0's ndcg@20: 0.981473\tvalid_0's ndcg@30: 0.981473\tvalid_0's ndcg@40: 0.981473\tvalid_0's ndcg@50: 0.981473\tvalid_0's ndcg@10: 0.981473\tvalid_0's ndcg@20: 0.981473\tvalid_0's ndcg@30: 0.981473\tvalid_0's ndcg@40: 0.981473\tvalid_0's ndcg@50: 0.981473\n","[20]\tvalid_0's ndcg@10: 0.981369\tvalid_0's ndcg@20: 0.981369\tvalid_0's ndcg@30: 0.981369\tvalid_0's ndcg@40: 0.981369\tvalid_0's ndcg@50: 0.981369\tvalid_0's ndcg@10: 0.981369\tvalid_0's ndcg@20: 0.981369\tvalid_0's ndcg@30: 0.981369\tvalid_0's ndcg@40: 0.981369\tvalid_0's ndcg@50: 0.981369\n","[21]\tvalid_0's ndcg@10: 0.981335\tvalid_0's ndcg@20: 0.981335\tvalid_0's ndcg@30: 0.981335\tvalid_0's ndcg@40: 0.981335\tvalid_0's ndcg@50: 0.981335\tvalid_0's ndcg@10: 0.981335\tvalid_0's ndcg@20: 0.981335\tvalid_0's ndcg@30: 0.981335\tvalid_0's ndcg@40: 0.981335\tvalid_0's ndcg@50: 0.981335\n","[22]\tvalid_0's ndcg@10: 0.981362\tvalid_0's ndcg@20: 0.981362\tvalid_0's ndcg@30: 0.981362\tvalid_0's ndcg@40: 0.981362\tvalid_0's ndcg@50: 0.981362\tvalid_0's ndcg@10: 0.981362\tvalid_0's ndcg@20: 0.981362\tvalid_0's ndcg@30: 0.981362\tvalid_0's ndcg@40: 0.981362\tvalid_0's ndcg@50: 0.981362\n","[23]\tvalid_0's ndcg@10: 0.981634\tvalid_0's ndcg@20: 0.981634\tvalid_0's ndcg@30: 0.981634\tvalid_0's ndcg@40: 0.981634\tvalid_0's ndcg@50: 0.981634\tvalid_0's ndcg@10: 0.981634\tvalid_0's ndcg@20: 0.981634\tvalid_0's ndcg@30: 0.981634\tvalid_0's ndcg@40: 0.981634\tvalid_0's ndcg@50: 0.981634\n","[24]\tvalid_0's ndcg@10: 0.981434\tvalid_0's ndcg@20: 0.981434\tvalid_0's ndcg@30: 0.981434\tvalid_0's ndcg@40: 0.981434\tvalid_0's ndcg@50: 0.981434\tvalid_0's ndcg@10: 0.981434\tvalid_0's ndcg@20: 0.981434\tvalid_0's ndcg@30: 0.981434\tvalid_0's ndcg@40: 0.981434\tvalid_0's ndcg@50: 0.981434\n","[25]\tvalid_0's ndcg@10: 0.981405\tvalid_0's ndcg@20: 0.981405\tvalid_0's ndcg@30: 0.981405\tvalid_0's ndcg@40: 0.981405\tvalid_0's ndcg@50: 0.981405\tvalid_0's ndcg@10: 0.981405\tvalid_0's ndcg@20: 0.981405\tvalid_0's ndcg@30: 0.981405\tvalid_0's ndcg@40: 0.981405\tvalid_0's ndcg@50: 0.981405\n","[26]\tvalid_0's ndcg@10: 0.981713\tvalid_0's ndcg@20: 0.981713\tvalid_0's ndcg@30: 0.981713\tvalid_0's ndcg@40: 0.981713\tvalid_0's ndcg@50: 0.981713\tvalid_0's ndcg@10: 0.981713\tvalid_0's ndcg@20: 0.981713\tvalid_0's ndcg@30: 0.981713\tvalid_0's ndcg@40: 0.981713\tvalid_0's ndcg@50: 0.981713\n","[27]\tvalid_0's ndcg@10: 0.981655\tvalid_0's ndcg@20: 0.981655\tvalid_0's ndcg@30: 0.981655\tvalid_0's ndcg@40: 0.981655\tvalid_0's ndcg@50: 0.981655\tvalid_0's ndcg@10: 0.981655\tvalid_0's ndcg@20: 0.981655\tvalid_0's ndcg@30: 0.981655\tvalid_0's ndcg@40: 0.981655\tvalid_0's ndcg@50: 0.981655\n","[28]\tvalid_0's ndcg@10: 0.981671\tvalid_0's ndcg@20: 0.981671\tvalid_0's ndcg@30: 0.981671\tvalid_0's ndcg@40: 0.981671\tvalid_0's ndcg@50: 0.981671\tvalid_0's ndcg@10: 0.981671\tvalid_0's ndcg@20: 0.981671\tvalid_0's ndcg@30: 0.981671\tvalid_0's ndcg@40: 0.981671\tvalid_0's ndcg@50: 0.981671\n","[29]\tvalid_0's ndcg@10: 0.981527\tvalid_0's ndcg@20: 0.981527\tvalid_0's ndcg@30: 0.981527\tvalid_0's ndcg@40: 0.981527\tvalid_0's ndcg@50: 0.981527\tvalid_0's ndcg@10: 0.981527\tvalid_0's ndcg@20: 0.981527\tvalid_0's ndcg@30: 0.981527\tvalid_0's ndcg@40: 0.981527\tvalid_0's ndcg@50: 0.981527\n","[30]\tvalid_0's ndcg@10: 0.981509\tvalid_0's ndcg@20: 0.981509\tvalid_0's ndcg@30: 0.981509\tvalid_0's ndcg@40: 0.981509\tvalid_0's ndcg@50: 0.981509\tvalid_0's ndcg@10: 0.981509\tvalid_0's ndcg@20: 0.981509\tvalid_0's ndcg@30: 0.981509\tvalid_0's ndcg@40: 0.981509\tvalid_0's ndcg@50: 0.981509\n","[31]\tvalid_0's ndcg@10: 0.981638\tvalid_0's ndcg@20: 0.981638\tvalid_0's ndcg@30: 0.981638\tvalid_0's ndcg@40: 0.981638\tvalid_0's ndcg@50: 0.981638\tvalid_0's ndcg@10: 0.981638\tvalid_0's ndcg@20: 0.981638\tvalid_0's ndcg@30: 0.981638\tvalid_0's ndcg@40: 0.981638\tvalid_0's ndcg@50: 0.981638\n","[32]\tvalid_0's ndcg@10: 0.981702\tvalid_0's ndcg@20: 0.981702\tvalid_0's ndcg@30: 0.981702\tvalid_0's ndcg@40: 0.981702\tvalid_0's ndcg@50: 0.981702\tvalid_0's ndcg@10: 0.981702\tvalid_0's ndcg@20: 0.981702\tvalid_0's ndcg@30: 0.981702\tvalid_0's ndcg@40: 0.981702\tvalid_0's ndcg@50: 0.981702\n","[33]\tvalid_0's ndcg@10: 0.981758\tvalid_0's ndcg@20: 0.981758\tvalid_0's ndcg@30: 0.981758\tvalid_0's ndcg@40: 0.981758\tvalid_0's ndcg@50: 0.981758\tvalid_0's ndcg@10: 0.981758\tvalid_0's ndcg@20: 0.981758\tvalid_0's ndcg@30: 0.981758\tvalid_0's ndcg@40: 0.981758\tvalid_0's ndcg@50: 0.981758\n","[34]\tvalid_0's ndcg@10: 0.981749\tvalid_0's ndcg@20: 0.981749\tvalid_0's ndcg@30: 0.981749\tvalid_0's ndcg@40: 0.981749\tvalid_0's ndcg@50: 0.981749\tvalid_0's ndcg@10: 0.981749\tvalid_0's ndcg@20: 0.981749\tvalid_0's ndcg@30: 0.981749\tvalid_0's ndcg@40: 0.981749\tvalid_0's ndcg@50: 0.981749\n","[35]\tvalid_0's ndcg@10: 0.9817\tvalid_0's ndcg@20: 0.9817\tvalid_0's ndcg@30: 0.9817\tvalid_0's ndcg@40: 0.9817\tvalid_0's ndcg@50: 0.9817\tvalid_0's ndcg@10: 0.9817\tvalid_0's ndcg@20: 0.9817\tvalid_0's ndcg@30: 0.9817\tvalid_0's ndcg@40: 0.9817\tvalid_0's ndcg@50: 0.9817\n","[36]\tvalid_0's ndcg@10: 0.98158\tvalid_0's ndcg@20: 0.98158\tvalid_0's ndcg@30: 0.98158\tvalid_0's ndcg@40: 0.98158\tvalid_0's ndcg@50: 0.98158\tvalid_0's ndcg@10: 0.98158\tvalid_0's ndcg@20: 0.98158\tvalid_0's ndcg@30: 0.98158\tvalid_0's ndcg@40: 0.98158\tvalid_0's ndcg@50: 0.98158\n","[37]\tvalid_0's ndcg@10: 0.981638\tvalid_0's ndcg@20: 0.981638\tvalid_0's ndcg@30: 0.981638\tvalid_0's ndcg@40: 0.981638\tvalid_0's ndcg@50: 0.981638\tvalid_0's ndcg@10: 0.981638\tvalid_0's ndcg@20: 0.981638\tvalid_0's ndcg@30: 0.981638\tvalid_0's ndcg@40: 0.981638\tvalid_0's ndcg@50: 0.981638\n","[38]\tvalid_0's ndcg@10: 0.981444\tvalid_0's ndcg@20: 0.981444\tvalid_0's ndcg@30: 0.981444\tvalid_0's ndcg@40: 0.981444\tvalid_0's ndcg@50: 0.981444\tvalid_0's ndcg@10: 0.981444\tvalid_0's ndcg@20: 0.981444\tvalid_0's ndcg@30: 0.981444\tvalid_0's ndcg@40: 0.981444\tvalid_0's ndcg@50: 0.981444\n","[39]\tvalid_0's ndcg@10: 0.981387\tvalid_0's ndcg@20: 0.981387\tvalid_0's ndcg@30: 0.981387\tvalid_0's ndcg@40: 0.981387\tvalid_0's ndcg@50: 0.981387\tvalid_0's ndcg@10: 0.981387\tvalid_0's ndcg@20: 0.981387\tvalid_0's ndcg@30: 0.981387\tvalid_0's ndcg@40: 0.981387\tvalid_0's ndcg@50: 0.981387\n","[40]\tvalid_0's ndcg@10: 0.981445\tvalid_0's ndcg@20: 0.981445\tvalid_0's ndcg@30: 0.981445\tvalid_0's ndcg@40: 0.981445\tvalid_0's ndcg@50: 0.981445\tvalid_0's ndcg@10: 0.981445\tvalid_0's ndcg@20: 0.981445\tvalid_0's ndcg@30: 0.981445\tvalid_0's ndcg@40: 0.981445\tvalid_0's ndcg@50: 0.981445\n","[41]\tvalid_0's ndcg@10: 0.981239\tvalid_0's ndcg@20: 0.981239\tvalid_0's ndcg@30: 0.981239\tvalid_0's ndcg@40: 0.981239\tvalid_0's ndcg@50: 0.981239\tvalid_0's ndcg@10: 0.981239\tvalid_0's ndcg@20: 0.981239\tvalid_0's ndcg@30: 0.981239\tvalid_0's ndcg@40: 0.981239\tvalid_0's ndcg@50: 0.981239\n","[42]\tvalid_0's ndcg@10: 0.981465\tvalid_0's ndcg@20: 0.981465\tvalid_0's ndcg@30: 0.981465\tvalid_0's ndcg@40: 0.981465\tvalid_0's ndcg@50: 0.981465\tvalid_0's ndcg@10: 0.981465\tvalid_0's ndcg@20: 0.981465\tvalid_0's ndcg@30: 0.981465\tvalid_0's ndcg@40: 0.981465\tvalid_0's ndcg@50: 0.981465\n","[43]\tvalid_0's ndcg@10: 0.981624\tvalid_0's ndcg@20: 0.981624\tvalid_0's ndcg@30: 0.981624\tvalid_0's ndcg@40: 0.981624\tvalid_0's ndcg@50: 0.981624\tvalid_0's ndcg@10: 0.981624\tvalid_0's ndcg@20: 0.981624\tvalid_0's ndcg@30: 0.981624\tvalid_0's ndcg@40: 0.981624\tvalid_0's ndcg@50: 0.981624\n","[44]\tvalid_0's ndcg@10: 0.981568\tvalid_0's ndcg@20: 0.981568\tvalid_0's ndcg@30: 0.981568\tvalid_0's ndcg@40: 0.981568\tvalid_0's ndcg@50: 0.981568\tvalid_0's ndcg@10: 0.981568\tvalid_0's ndcg@20: 0.981568\tvalid_0's ndcg@30: 0.981568\tvalid_0's ndcg@40: 0.981568\tvalid_0's ndcg@50: 0.981568\n","[45]\tvalid_0's ndcg@10: 0.981347\tvalid_0's ndcg@20: 0.981347\tvalid_0's ndcg@30: 0.981347\tvalid_0's ndcg@40: 0.981347\tvalid_0's ndcg@50: 0.981347\tvalid_0's ndcg@10: 0.981347\tvalid_0's ndcg@20: 0.981347\tvalid_0's ndcg@30: 0.981347\tvalid_0's ndcg@40: 0.981347\tvalid_0's ndcg@50: 0.981347\n","[46]\tvalid_0's ndcg@10: 0.981458\tvalid_0's ndcg@20: 0.981458\tvalid_0's ndcg@30: 0.981458\tvalid_0's ndcg@40: 0.981458\tvalid_0's ndcg@50: 0.981458\tvalid_0's ndcg@10: 0.981458\tvalid_0's ndcg@20: 0.981458\tvalid_0's ndcg@30: 0.981458\tvalid_0's ndcg@40: 0.981458\tvalid_0's ndcg@50: 0.981458\n","[47]\tvalid_0's ndcg@10: 0.981331\tvalid_0's ndcg@20: 0.981331\tvalid_0's ndcg@30: 0.981331\tvalid_0's ndcg@40: 0.981331\tvalid_0's ndcg@50: 0.981331\tvalid_0's ndcg@10: 0.981331\tvalid_0's ndcg@20: 0.981331\tvalid_0's ndcg@30: 0.981331\tvalid_0's ndcg@40: 0.981331\tvalid_0's ndcg@50: 0.981331\n","[48]\tvalid_0's ndcg@10: 0.981458\tvalid_0's ndcg@20: 0.981458\tvalid_0's ndcg@30: 0.981458\tvalid_0's ndcg@40: 0.981458\tvalid_0's ndcg@50: 0.981458\tvalid_0's ndcg@10: 0.981458\tvalid_0's ndcg@20: 0.981458\tvalid_0's ndcg@30: 0.981458\tvalid_0's ndcg@40: 0.981458\tvalid_0's ndcg@50: 0.981458\n","[49]\tvalid_0's ndcg@10: 0.981475\tvalid_0's ndcg@20: 0.981475\tvalid_0's ndcg@30: 0.981475\tvalid_0's ndcg@40: 0.981475\tvalid_0's ndcg@50: 0.981475\tvalid_0's ndcg@10: 0.981475\tvalid_0's ndcg@20: 0.981475\tvalid_0's ndcg@30: 0.981475\tvalid_0's ndcg@40: 0.981475\tvalid_0's ndcg@50: 0.981475\n","[50]\tvalid_0's ndcg@10: 0.981499\tvalid_0's ndcg@20: 0.981499\tvalid_0's ndcg@30: 0.981499\tvalid_0's ndcg@40: 0.981499\tvalid_0's ndcg@50: 0.981499\tvalid_0's ndcg@10: 0.981499\tvalid_0's ndcg@20: 0.981499\tvalid_0's ndcg@30: 0.981499\tvalid_0's ndcg@40: 0.981499\tvalid_0's ndcg@50: 0.981499\n","[51]\tvalid_0's ndcg@10: 0.981504\tvalid_0's ndcg@20: 0.981504\tvalid_0's ndcg@30: 0.981504\tvalid_0's ndcg@40: 0.981504\tvalid_0's ndcg@50: 0.981504\tvalid_0's ndcg@10: 0.981504\tvalid_0's ndcg@20: 0.981504\tvalid_0's ndcg@30: 0.981504\tvalid_0's ndcg@40: 0.981504\tvalid_0's ndcg@50: 0.981504\n","[52]\tvalid_0's ndcg@10: 0.981416\tvalid_0's ndcg@20: 0.981416\tvalid_0's ndcg@30: 0.981416\tvalid_0's ndcg@40: 0.981416\tvalid_0's ndcg@50: 0.981416\tvalid_0's ndcg@10: 0.981416\tvalid_0's ndcg@20: 0.981416\tvalid_0's ndcg@30: 0.981416\tvalid_0's ndcg@40: 0.981416\tvalid_0's ndcg@50: 0.981416\n","[53]\tvalid_0's ndcg@10: 0.98148\tvalid_0's ndcg@20: 0.98148\tvalid_0's ndcg@30: 0.98148\tvalid_0's ndcg@40: 0.98148\tvalid_0's ndcg@50: 0.98148\tvalid_0's ndcg@10: 0.98148\tvalid_0's ndcg@20: 0.98148\tvalid_0's ndcg@30: 0.98148\tvalid_0's ndcg@40: 0.98148\tvalid_0's ndcg@50: 0.98148\n","[54]\tvalid_0's ndcg@10: 0.981481\tvalid_0's ndcg@20: 0.981481\tvalid_0's ndcg@30: 0.981481\tvalid_0's ndcg@40: 0.981481\tvalid_0's ndcg@50: 0.981481\tvalid_0's ndcg@10: 0.981481\tvalid_0's ndcg@20: 0.981481\tvalid_0's ndcg@30: 0.981481\tvalid_0's ndcg@40: 0.981481\tvalid_0's ndcg@50: 0.981481\n","[55]\tvalid_0's ndcg@10: 0.981493\tvalid_0's ndcg@20: 0.981493\tvalid_0's ndcg@30: 0.981493\tvalid_0's ndcg@40: 0.981493\tvalid_0's ndcg@50: 0.981493\tvalid_0's ndcg@10: 0.981493\tvalid_0's ndcg@20: 0.981493\tvalid_0's ndcg@30: 0.981493\tvalid_0's ndcg@40: 0.981493\tvalid_0's ndcg@50: 0.981493\n","[56]\tvalid_0's ndcg@10: 0.981546\tvalid_0's ndcg@20: 0.981546\tvalid_0's ndcg@30: 0.981546\tvalid_0's ndcg@40: 0.981546\tvalid_0's ndcg@50: 0.981546\tvalid_0's ndcg@10: 0.981546\tvalid_0's ndcg@20: 0.981546\tvalid_0's ndcg@30: 0.981546\tvalid_0's ndcg@40: 0.981546\tvalid_0's ndcg@50: 0.981546\n","[57]\tvalid_0's ndcg@10: 0.981526\tvalid_0's ndcg@20: 0.981526\tvalid_0's ndcg@30: 0.981526\tvalid_0's ndcg@40: 0.981526\tvalid_0's ndcg@50: 0.981526\tvalid_0's ndcg@10: 0.981526\tvalid_0's ndcg@20: 0.981526\tvalid_0's ndcg@30: 0.981526\tvalid_0's ndcg@40: 0.981526\tvalid_0's ndcg@50: 0.981526\n","[58]\tvalid_0's ndcg@10: 0.981597\tvalid_0's ndcg@20: 0.981597\tvalid_0's ndcg@30: 0.981597\tvalid_0's ndcg@40: 0.981597\tvalid_0's ndcg@50: 0.981597\tvalid_0's ndcg@10: 0.981597\tvalid_0's ndcg@20: 0.981597\tvalid_0's ndcg@30: 0.981597\tvalid_0's ndcg@40: 0.981597\tvalid_0's ndcg@50: 0.981597\n","[59]\tvalid_0's ndcg@10: 0.981626\tvalid_0's ndcg@20: 0.981626\tvalid_0's ndcg@30: 0.981626\tvalid_0's ndcg@40: 0.981626\tvalid_0's ndcg@50: 0.981626\tvalid_0's ndcg@10: 0.981626\tvalid_0's ndcg@20: 0.981626\tvalid_0's ndcg@30: 0.981626\tvalid_0's ndcg@40: 0.981626\tvalid_0's ndcg@50: 0.981626\n","[60]\tvalid_0's ndcg@10: 0.981616\tvalid_0's ndcg@20: 0.981616\tvalid_0's ndcg@30: 0.981616\tvalid_0's ndcg@40: 0.981616\tvalid_0's ndcg@50: 0.981616\tvalid_0's ndcg@10: 0.981616\tvalid_0's ndcg@20: 0.981616\tvalid_0's ndcg@30: 0.981616\tvalid_0's ndcg@40: 0.981616\tvalid_0's ndcg@50: 0.981616\n","[61]\tvalid_0's ndcg@10: 0.981576\tvalid_0's ndcg@20: 0.981576\tvalid_0's ndcg@30: 0.981576\tvalid_0's ndcg@40: 0.981576\tvalid_0's ndcg@50: 0.981576\tvalid_0's ndcg@10: 0.981576\tvalid_0's ndcg@20: 0.981576\tvalid_0's ndcg@30: 0.981576\tvalid_0's ndcg@40: 0.981576\tvalid_0's ndcg@50: 0.981576\n","[62]\tvalid_0's ndcg@10: 0.981526\tvalid_0's ndcg@20: 0.981526\tvalid_0's ndcg@30: 0.981526\tvalid_0's ndcg@40: 0.981526\tvalid_0's ndcg@50: 0.981526\tvalid_0's ndcg@10: 0.981526\tvalid_0's ndcg@20: 0.981526\tvalid_0's ndcg@30: 0.981526\tvalid_0's ndcg@40: 0.981526\tvalid_0's ndcg@50: 0.981526\n","[63]\tvalid_0's ndcg@10: 0.981719\tvalid_0's ndcg@20: 0.981719\tvalid_0's ndcg@30: 0.981719\tvalid_0's ndcg@40: 0.981719\tvalid_0's ndcg@50: 0.981719\tvalid_0's ndcg@10: 0.981719\tvalid_0's ndcg@20: 0.981719\tvalid_0's ndcg@30: 0.981719\tvalid_0's ndcg@40: 0.981719\tvalid_0's ndcg@50: 0.981719\n","[64]\tvalid_0's ndcg@10: 0.981826\tvalid_0's ndcg@20: 0.981826\tvalid_0's ndcg@30: 0.981826\tvalid_0's ndcg@40: 0.981826\tvalid_0's ndcg@50: 0.981826\tvalid_0's ndcg@10: 0.981826\tvalid_0's ndcg@20: 0.981826\tvalid_0's ndcg@30: 0.981826\tvalid_0's ndcg@40: 0.981826\tvalid_0's ndcg@50: 0.981826\n","[65]\tvalid_0's ndcg@10: 0.981847\tvalid_0's ndcg@20: 0.981847\tvalid_0's ndcg@30: 0.981847\tvalid_0's ndcg@40: 0.981847\tvalid_0's ndcg@50: 0.981847\tvalid_0's ndcg@10: 0.981847\tvalid_0's ndcg@20: 0.981847\tvalid_0's ndcg@30: 0.981847\tvalid_0's ndcg@40: 0.981847\tvalid_0's ndcg@50: 0.981847\n","[66]\tvalid_0's ndcg@10: 0.981792\tvalid_0's ndcg@20: 0.981792\tvalid_0's ndcg@30: 0.981792\tvalid_0's ndcg@40: 0.981792\tvalid_0's ndcg@50: 0.981792\tvalid_0's ndcg@10: 0.981792\tvalid_0's ndcg@20: 0.981792\tvalid_0's ndcg@30: 0.981792\tvalid_0's ndcg@40: 0.981792\tvalid_0's ndcg@50: 0.981792\n","[67]\tvalid_0's ndcg@10: 0.981943\tvalid_0's ndcg@20: 0.981943\tvalid_0's ndcg@30: 0.981943\tvalid_0's ndcg@40: 0.981943\tvalid_0's ndcg@50: 0.981943\tvalid_0's ndcg@10: 0.981943\tvalid_0's ndcg@20: 0.981943\tvalid_0's ndcg@30: 0.981943\tvalid_0's ndcg@40: 0.981943\tvalid_0's ndcg@50: 0.981943\n","[68]\tvalid_0's ndcg@10: 0.981842\tvalid_0's ndcg@20: 0.981842\tvalid_0's ndcg@30: 0.981842\tvalid_0's ndcg@40: 0.981842\tvalid_0's ndcg@50: 0.981842\tvalid_0's ndcg@10: 0.981842\tvalid_0's ndcg@20: 0.981842\tvalid_0's ndcg@30: 0.981842\tvalid_0's ndcg@40: 0.981842\tvalid_0's ndcg@50: 0.981842\n","[69]\tvalid_0's ndcg@10: 0.981842\tvalid_0's ndcg@20: 0.981842\tvalid_0's ndcg@30: 0.981842\tvalid_0's ndcg@40: 0.981842\tvalid_0's ndcg@50: 0.981842\tvalid_0's ndcg@10: 0.981842\tvalid_0's ndcg@20: 0.981842\tvalid_0's ndcg@30: 0.981842\tvalid_0's ndcg@40: 0.981842\tvalid_0's ndcg@50: 0.981842\n","[70]\tvalid_0's ndcg@10: 0.98196\tvalid_0's ndcg@20: 0.98196\tvalid_0's ndcg@30: 0.98196\tvalid_0's ndcg@40: 0.98196\tvalid_0's ndcg@50: 0.98196\tvalid_0's ndcg@10: 0.98196\tvalid_0's ndcg@20: 0.98196\tvalid_0's ndcg@30: 0.98196\tvalid_0's ndcg@40: 0.98196\tvalid_0's ndcg@50: 0.98196\n","[71]\tvalid_0's ndcg@10: 0.981916\tvalid_0's ndcg@20: 0.981916\tvalid_0's ndcg@30: 0.981916\tvalid_0's ndcg@40: 0.981916\tvalid_0's ndcg@50: 0.981916\tvalid_0's ndcg@10: 0.981916\tvalid_0's ndcg@20: 0.981916\tvalid_0's ndcg@30: 0.981916\tvalid_0's ndcg@40: 0.981916\tvalid_0's ndcg@50: 0.981916\n","[72]\tvalid_0's ndcg@10: 0.981852\tvalid_0's ndcg@20: 0.981852\tvalid_0's ndcg@30: 0.981852\tvalid_0's ndcg@40: 0.981852\tvalid_0's ndcg@50: 0.981852\tvalid_0's ndcg@10: 0.981852\tvalid_0's ndcg@20: 0.981852\tvalid_0's ndcg@30: 0.981852\tvalid_0's ndcg@40: 0.981852\tvalid_0's ndcg@50: 0.981852\n","[73]\tvalid_0's ndcg@10: 0.981845\tvalid_0's ndcg@20: 0.981845\tvalid_0's ndcg@30: 0.981845\tvalid_0's ndcg@40: 0.981845\tvalid_0's ndcg@50: 0.981845\tvalid_0's ndcg@10: 0.981845\tvalid_0's ndcg@20: 0.981845\tvalid_0's ndcg@30: 0.981845\tvalid_0's ndcg@40: 0.981845\tvalid_0's ndcg@50: 0.981845\n","[74]\tvalid_0's ndcg@10: 0.981782\tvalid_0's ndcg@20: 0.981782\tvalid_0's ndcg@30: 0.981782\tvalid_0's ndcg@40: 0.981782\tvalid_0's ndcg@50: 0.981782\tvalid_0's ndcg@10: 0.981782\tvalid_0's ndcg@20: 0.981782\tvalid_0's ndcg@30: 0.981782\tvalid_0's ndcg@40: 0.981782\tvalid_0's ndcg@50: 0.981782\n","[75]\tvalid_0's ndcg@10: 0.98185\tvalid_0's ndcg@20: 0.98185\tvalid_0's ndcg@30: 0.98185\tvalid_0's ndcg@40: 0.98185\tvalid_0's ndcg@50: 0.98185\tvalid_0's ndcg@10: 0.98185\tvalid_0's ndcg@20: 0.98185\tvalid_0's ndcg@30: 0.98185\tvalid_0's ndcg@40: 0.98185\tvalid_0's ndcg@50: 0.98185\n","[76]\tvalid_0's ndcg@10: 0.98196\tvalid_0's ndcg@20: 0.98196\tvalid_0's ndcg@30: 0.98196\tvalid_0's ndcg@40: 0.98196\tvalid_0's ndcg@50: 0.98196\tvalid_0's ndcg@10: 0.98196\tvalid_0's ndcg@20: 0.98196\tvalid_0's ndcg@30: 0.98196\tvalid_0's ndcg@40: 0.98196\tvalid_0's ndcg@50: 0.98196\n","[77]\tvalid_0's ndcg@10: 0.982049\tvalid_0's ndcg@20: 0.982049\tvalid_0's ndcg@30: 0.982049\tvalid_0's ndcg@40: 0.982049\tvalid_0's ndcg@50: 0.982049\tvalid_0's ndcg@10: 0.982049\tvalid_0's ndcg@20: 0.982049\tvalid_0's ndcg@30: 0.982049\tvalid_0's ndcg@40: 0.982049\tvalid_0's ndcg@50: 0.982049\n","[78]\tvalid_0's ndcg@10: 0.981957\tvalid_0's ndcg@20: 0.981957\tvalid_0's ndcg@30: 0.981957\tvalid_0's ndcg@40: 0.981957\tvalid_0's ndcg@50: 0.981957\tvalid_0's ndcg@10: 0.981957\tvalid_0's ndcg@20: 0.981957\tvalid_0's ndcg@30: 0.981957\tvalid_0's ndcg@40: 0.981957\tvalid_0's ndcg@50: 0.981957\n","[79]\tvalid_0's ndcg@10: 0.982031\tvalid_0's ndcg@20: 0.982031\tvalid_0's ndcg@30: 0.982031\tvalid_0's ndcg@40: 0.982031\tvalid_0's ndcg@50: 0.982031\tvalid_0's ndcg@10: 0.982031\tvalid_0's ndcg@20: 0.982031\tvalid_0's ndcg@30: 0.982031\tvalid_0's ndcg@40: 0.982031\tvalid_0's ndcg@50: 0.982031\n","[80]\tvalid_0's ndcg@10: 0.982016\tvalid_0's ndcg@20: 0.982016\tvalid_0's ndcg@30: 0.982016\tvalid_0's ndcg@40: 0.982016\tvalid_0's ndcg@50: 0.982016\tvalid_0's ndcg@10: 0.982016\tvalid_0's ndcg@20: 0.982016\tvalid_0's ndcg@30: 0.982016\tvalid_0's ndcg@40: 0.982016\tvalid_0's ndcg@50: 0.982016\n","[81]\tvalid_0's ndcg@10: 0.982006\tvalid_0's ndcg@20: 0.982006\tvalid_0's ndcg@30: 0.982006\tvalid_0's ndcg@40: 0.982006\tvalid_0's ndcg@50: 0.982006\tvalid_0's ndcg@10: 0.982006\tvalid_0's ndcg@20: 0.982006\tvalid_0's ndcg@30: 0.982006\tvalid_0's ndcg@40: 0.982006\tvalid_0's ndcg@50: 0.982006\n","[82]\tvalid_0's ndcg@10: 0.982016\tvalid_0's ndcg@20: 0.982016\tvalid_0's ndcg@30: 0.982016\tvalid_0's ndcg@40: 0.982016\tvalid_0's ndcg@50: 0.982016\tvalid_0's ndcg@10: 0.982016\tvalid_0's ndcg@20: 0.982016\tvalid_0's ndcg@30: 0.982016\tvalid_0's ndcg@40: 0.982016\tvalid_0's ndcg@50: 0.982016\n","[83]\tvalid_0's ndcg@10: 0.981982\tvalid_0's ndcg@20: 0.981982\tvalid_0's ndcg@30: 0.981982\tvalid_0's ndcg@40: 0.981982\tvalid_0's ndcg@50: 0.981982\tvalid_0's ndcg@10: 0.981982\tvalid_0's ndcg@20: 0.981982\tvalid_0's ndcg@30: 0.981982\tvalid_0's ndcg@40: 0.981982\tvalid_0's ndcg@50: 0.981982\n","[84]\tvalid_0's ndcg@10: 0.982039\tvalid_0's ndcg@20: 0.982039\tvalid_0's ndcg@30: 0.982039\tvalid_0's ndcg@40: 0.982039\tvalid_0's ndcg@50: 0.982039\tvalid_0's ndcg@10: 0.982039\tvalid_0's ndcg@20: 0.982039\tvalid_0's ndcg@30: 0.982039\tvalid_0's ndcg@40: 0.982039\tvalid_0's ndcg@50: 0.982039\n","[85]\tvalid_0's ndcg@10: 0.982015\tvalid_0's ndcg@20: 0.982015\tvalid_0's ndcg@30: 0.982015\tvalid_0's ndcg@40: 0.982015\tvalid_0's ndcg@50: 0.982015\tvalid_0's ndcg@10: 0.982015\tvalid_0's ndcg@20: 0.982015\tvalid_0's ndcg@30: 0.982015\tvalid_0's ndcg@40: 0.982015\tvalid_0's ndcg@50: 0.982015\n","[86]\tvalid_0's ndcg@10: 0.98201\tvalid_0's ndcg@20: 0.98201\tvalid_0's ndcg@30: 0.98201\tvalid_0's ndcg@40: 0.98201\tvalid_0's ndcg@50: 0.98201\tvalid_0's ndcg@10: 0.98201\tvalid_0's ndcg@20: 0.98201\tvalid_0's ndcg@30: 0.98201\tvalid_0's ndcg@40: 0.98201\tvalid_0's ndcg@50: 0.98201\n","[87]\tvalid_0's ndcg@10: 0.982051\tvalid_0's ndcg@20: 0.982051\tvalid_0's ndcg@30: 0.982051\tvalid_0's ndcg@40: 0.982051\tvalid_0's ndcg@50: 0.982051\tvalid_0's ndcg@10: 0.982051\tvalid_0's ndcg@20: 0.982051\tvalid_0's ndcg@30: 0.982051\tvalid_0's ndcg@40: 0.982051\tvalid_0's ndcg@50: 0.982051\n","[88]\tvalid_0's ndcg@10: 0.982033\tvalid_0's ndcg@20: 0.982033\tvalid_0's ndcg@30: 0.982033\tvalid_0's ndcg@40: 0.982033\tvalid_0's ndcg@50: 0.982033\tvalid_0's ndcg@10: 0.982033\tvalid_0's ndcg@20: 0.982033\tvalid_0's ndcg@30: 0.982033\tvalid_0's ndcg@40: 0.982033\tvalid_0's ndcg@50: 0.982033\n","[89]\tvalid_0's ndcg@10: 0.982156\tvalid_0's ndcg@20: 0.982156\tvalid_0's ndcg@30: 0.982156\tvalid_0's ndcg@40: 0.982156\tvalid_0's ndcg@50: 0.982156\tvalid_0's ndcg@10: 0.982156\tvalid_0's ndcg@20: 0.982156\tvalid_0's ndcg@30: 0.982156\tvalid_0's ndcg@40: 0.982156\tvalid_0's ndcg@50: 0.982156\n","[90]\tvalid_0's ndcg@10: 0.982041\tvalid_0's ndcg@20: 0.982041\tvalid_0's ndcg@30: 0.982041\tvalid_0's ndcg@40: 0.982041\tvalid_0's ndcg@50: 0.982041\tvalid_0's ndcg@10: 0.982041\tvalid_0's ndcg@20: 0.982041\tvalid_0's ndcg@30: 0.982041\tvalid_0's ndcg@40: 0.982041\tvalid_0's ndcg@50: 0.982041\n","[91]\tvalid_0's ndcg@10: 0.982041\tvalid_0's ndcg@20: 0.982041\tvalid_0's ndcg@30: 0.982041\tvalid_0's ndcg@40: 0.982041\tvalid_0's ndcg@50: 0.982041\tvalid_0's ndcg@10: 0.982041\tvalid_0's ndcg@20: 0.982041\tvalid_0's ndcg@30: 0.982041\tvalid_0's ndcg@40: 0.982041\tvalid_0's ndcg@50: 0.982041\n","[92]\tvalid_0's ndcg@10: 0.982075\tvalid_0's ndcg@20: 0.982075\tvalid_0's ndcg@30: 0.982075\tvalid_0's ndcg@40: 0.982075\tvalid_0's ndcg@50: 0.982075\tvalid_0's ndcg@10: 0.982075\tvalid_0's ndcg@20: 0.982075\tvalid_0's ndcg@30: 0.982075\tvalid_0's ndcg@40: 0.982075\tvalid_0's ndcg@50: 0.982075\n","[93]\tvalid_0's ndcg@10: 0.982272\tvalid_0's ndcg@20: 0.982272\tvalid_0's ndcg@30: 0.982272\tvalid_0's ndcg@40: 0.982272\tvalid_0's ndcg@50: 0.982272\tvalid_0's ndcg@10: 0.982272\tvalid_0's ndcg@20: 0.982272\tvalid_0's ndcg@30: 0.982272\tvalid_0's ndcg@40: 0.982272\tvalid_0's ndcg@50: 0.982272\n","[94]\tvalid_0's ndcg@10: 0.982182\tvalid_0's ndcg@20: 0.982182\tvalid_0's ndcg@30: 0.982182\tvalid_0's ndcg@40: 0.982182\tvalid_0's ndcg@50: 0.982182\tvalid_0's ndcg@10: 0.982182\tvalid_0's ndcg@20: 0.982182\tvalid_0's ndcg@30: 0.982182\tvalid_0's ndcg@40: 0.982182\tvalid_0's ndcg@50: 0.982182\n","[95]\tvalid_0's ndcg@10: 0.982063\tvalid_0's ndcg@20: 0.982063\tvalid_0's ndcg@30: 0.982063\tvalid_0's ndcg@40: 0.982063\tvalid_0's ndcg@50: 0.982063\tvalid_0's ndcg@10: 0.982063\tvalid_0's ndcg@20: 0.982063\tvalid_0's ndcg@30: 0.982063\tvalid_0's ndcg@40: 0.982063\tvalid_0's ndcg@50: 0.982063\n","[96]\tvalid_0's ndcg@10: 0.9822\tvalid_0's ndcg@20: 0.9822\tvalid_0's ndcg@30: 0.9822\tvalid_0's ndcg@40: 0.9822\tvalid_0's ndcg@50: 0.9822\tvalid_0's ndcg@10: 0.9822\tvalid_0's ndcg@20: 0.9822\tvalid_0's ndcg@30: 0.9822\tvalid_0's ndcg@40: 0.9822\tvalid_0's ndcg@50: 0.9822\n","[97]\tvalid_0's ndcg@10: 0.982105\tvalid_0's ndcg@20: 0.982105\tvalid_0's ndcg@30: 0.982105\tvalid_0's ndcg@40: 0.982105\tvalid_0's ndcg@50: 0.982105\tvalid_0's ndcg@10: 0.982105\tvalid_0's ndcg@20: 0.982105\tvalid_0's ndcg@30: 0.982105\tvalid_0's ndcg@40: 0.982105\tvalid_0's ndcg@50: 0.982105\n","[98]\tvalid_0's ndcg@10: 0.982144\tvalid_0's ndcg@20: 0.982144\tvalid_0's ndcg@30: 0.982144\tvalid_0's ndcg@40: 0.982144\tvalid_0's ndcg@50: 0.982144\tvalid_0's ndcg@10: 0.982144\tvalid_0's ndcg@20: 0.982144\tvalid_0's ndcg@30: 0.982144\tvalid_0's ndcg@40: 0.982144\tvalid_0's ndcg@50: 0.982144\n","[99]\tvalid_0's ndcg@10: 0.981963\tvalid_0's ndcg@20: 0.981963\tvalid_0's ndcg@30: 0.981963\tvalid_0's ndcg@40: 0.981963\tvalid_0's ndcg@50: 0.981963\tvalid_0's ndcg@10: 0.981963\tvalid_0's ndcg@20: 0.981963\tvalid_0's ndcg@30: 0.981963\tvalid_0's ndcg@40: 0.981963\tvalid_0's ndcg@50: 0.981963\n","[100]\tvalid_0's ndcg@10: 0.981984\tvalid_0's ndcg@20: 0.981984\tvalid_0's ndcg@30: 0.981984\tvalid_0's ndcg@40: 0.981984\tvalid_0's ndcg@50: 0.981984\tvalid_0's ndcg@10: 0.981984\tvalid_0's ndcg@20: 0.981984\tvalid_0's ndcg@30: 0.981984\tvalid_0's ndcg@40: 0.981984\tvalid_0's ndcg@50: 0.981984\n","Did not meet early stopping. Best iteration is:\n","[93]\tvalid_0's ndcg@10: 0.982272\tvalid_0's ndcg@20: 0.982272\tvalid_0's ndcg@30: 0.982272\tvalid_0's ndcg@40: 0.982272\tvalid_0's ndcg@50: 0.982272\tvalid_0's ndcg@10: 0.982272\tvalid_0's ndcg@20: 0.982272\tvalid_0's ndcg@30: 0.982272\tvalid_0's ndcg@40: 0.982272\tvalid_0's ndcg@50: 0.982272\n","[1]\tvalid_0's ndcg@10: 0.976005\tvalid_0's ndcg@20: 0.976052\tvalid_0's ndcg@30: 0.976052\tvalid_0's ndcg@40: 0.976052\tvalid_0's ndcg@50: 0.976052\tvalid_0's ndcg@10: 0.976005\tvalid_0's ndcg@20: 0.976052\tvalid_0's ndcg@30: 0.976052\tvalid_0's ndcg@40: 0.976052\tvalid_0's ndcg@50: 0.976052\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.977342\tvalid_0's ndcg@20: 0.977342\tvalid_0's ndcg@30: 0.977342\tvalid_0's ndcg@40: 0.977342\tvalid_0's ndcg@50: 0.977342\tvalid_0's ndcg@10: 0.977342\tvalid_0's ndcg@20: 0.977342\tvalid_0's ndcg@30: 0.977342\tvalid_0's ndcg@40: 0.977342\tvalid_0's ndcg@50: 0.977342\n","[3]\tvalid_0's ndcg@10: 0.979466\tvalid_0's ndcg@20: 0.979511\tvalid_0's ndcg@30: 0.979511\tvalid_0's ndcg@40: 0.979511\tvalid_0's ndcg@50: 0.979511\tvalid_0's ndcg@10: 0.979466\tvalid_0's ndcg@20: 0.979511\tvalid_0's ndcg@30: 0.979511\tvalid_0's ndcg@40: 0.979511\tvalid_0's ndcg@50: 0.979511\n","[4]\tvalid_0's ndcg@10: 0.98012\tvalid_0's ndcg@20: 0.98012\tvalid_0's ndcg@30: 0.98012\tvalid_0's ndcg@40: 0.98012\tvalid_0's ndcg@50: 0.98012\tvalid_0's ndcg@10: 0.98012\tvalid_0's ndcg@20: 0.98012\tvalid_0's ndcg@30: 0.98012\tvalid_0's ndcg@40: 0.98012\tvalid_0's ndcg@50: 0.98012\n","[5]\tvalid_0's ndcg@10: 0.980492\tvalid_0's ndcg@20: 0.980492\tvalid_0's ndcg@30: 0.980492\tvalid_0's ndcg@40: 0.980492\tvalid_0's ndcg@50: 0.980492\tvalid_0's ndcg@10: 0.980492\tvalid_0's ndcg@20: 0.980492\tvalid_0's ndcg@30: 0.980492\tvalid_0's ndcg@40: 0.980492\tvalid_0's ndcg@50: 0.980492\n","[6]\tvalid_0's ndcg@10: 0.980129\tvalid_0's ndcg@20: 0.980129\tvalid_0's ndcg@30: 0.980129\tvalid_0's ndcg@40: 0.980129\tvalid_0's ndcg@50: 0.980129\tvalid_0's ndcg@10: 0.980129\tvalid_0's ndcg@20: 0.980129\tvalid_0's ndcg@30: 0.980129\tvalid_0's ndcg@40: 0.980129\tvalid_0's ndcg@50: 0.980129\n","[7]\tvalid_0's ndcg@10: 0.980374\tvalid_0's ndcg@20: 0.980374\tvalid_0's ndcg@30: 0.980374\tvalid_0's ndcg@40: 0.980374\tvalid_0's ndcg@50: 0.980374\tvalid_0's ndcg@10: 0.980374\tvalid_0's ndcg@20: 0.980374\tvalid_0's ndcg@30: 0.980374\tvalid_0's ndcg@40: 0.980374\tvalid_0's ndcg@50: 0.980374\n","[8]\tvalid_0's ndcg@10: 0.980623\tvalid_0's ndcg@20: 0.980623\tvalid_0's ndcg@30: 0.980623\tvalid_0's ndcg@40: 0.980623\tvalid_0's ndcg@50: 0.980623\tvalid_0's ndcg@10: 0.980623\tvalid_0's ndcg@20: 0.980623\tvalid_0's ndcg@30: 0.980623\tvalid_0's ndcg@40: 0.980623\tvalid_0's ndcg@50: 0.980623\n","[9]\tvalid_0's ndcg@10: 0.98065\tvalid_0's ndcg@20: 0.98065\tvalid_0's ndcg@30: 0.98065\tvalid_0's ndcg@40: 0.98065\tvalid_0's ndcg@50: 0.98065\tvalid_0's ndcg@10: 0.98065\tvalid_0's ndcg@20: 0.98065\tvalid_0's ndcg@30: 0.98065\tvalid_0's ndcg@40: 0.98065\tvalid_0's ndcg@50: 0.98065\n","[10]\tvalid_0's ndcg@10: 0.980548\tvalid_0's ndcg@20: 0.980548\tvalid_0's ndcg@30: 0.980548\tvalid_0's ndcg@40: 0.980548\tvalid_0's ndcg@50: 0.980548\tvalid_0's ndcg@10: 0.980548\tvalid_0's ndcg@20: 0.980548\tvalid_0's ndcg@30: 0.980548\tvalid_0's ndcg@40: 0.980548\tvalid_0's ndcg@50: 0.980548\n","[11]\tvalid_0's ndcg@10: 0.980238\tvalid_0's ndcg@20: 0.980238\tvalid_0's ndcg@30: 0.980238\tvalid_0's ndcg@40: 0.980238\tvalid_0's ndcg@50: 0.980238\tvalid_0's ndcg@10: 0.980238\tvalid_0's ndcg@20: 0.980238\tvalid_0's ndcg@30: 0.980238\tvalid_0's ndcg@40: 0.980238\tvalid_0's ndcg@50: 0.980238\n","[12]\tvalid_0's ndcg@10: 0.98059\tvalid_0's ndcg@20: 0.98059\tvalid_0's ndcg@30: 0.98059\tvalid_0's ndcg@40: 0.98059\tvalid_0's ndcg@50: 0.98059\tvalid_0's ndcg@10: 0.98059\tvalid_0's ndcg@20: 0.98059\tvalid_0's ndcg@30: 0.98059\tvalid_0's ndcg@40: 0.98059\tvalid_0's ndcg@50: 0.98059\n","[13]\tvalid_0's ndcg@10: 0.98065\tvalid_0's ndcg@20: 0.98065\tvalid_0's ndcg@30: 0.98065\tvalid_0's ndcg@40: 0.98065\tvalid_0's ndcg@50: 0.98065\tvalid_0's ndcg@10: 0.98065\tvalid_0's ndcg@20: 0.98065\tvalid_0's ndcg@30: 0.98065\tvalid_0's ndcg@40: 0.98065\tvalid_0's ndcg@50: 0.98065\n","[14]\tvalid_0's ndcg@10: 0.980476\tvalid_0's ndcg@20: 0.980476\tvalid_0's ndcg@30: 0.980476\tvalid_0's ndcg@40: 0.980476\tvalid_0's ndcg@50: 0.980476\tvalid_0's ndcg@10: 0.980476\tvalid_0's ndcg@20: 0.980476\tvalid_0's ndcg@30: 0.980476\tvalid_0's ndcg@40: 0.980476\tvalid_0's ndcg@50: 0.980476\n","[15]\tvalid_0's ndcg@10: 0.980657\tvalid_0's ndcg@20: 0.980657\tvalid_0's ndcg@30: 0.980657\tvalid_0's ndcg@40: 0.980657\tvalid_0's ndcg@50: 0.980657\tvalid_0's ndcg@10: 0.980657\tvalid_0's ndcg@20: 0.980657\tvalid_0's ndcg@30: 0.980657\tvalid_0's ndcg@40: 0.980657\tvalid_0's ndcg@50: 0.980657\n","[16]\tvalid_0's ndcg@10: 0.980759\tvalid_0's ndcg@20: 0.980759\tvalid_0's ndcg@30: 0.980759\tvalid_0's ndcg@40: 0.980759\tvalid_0's ndcg@50: 0.980759\tvalid_0's ndcg@10: 0.980759\tvalid_0's ndcg@20: 0.980759\tvalid_0's ndcg@30: 0.980759\tvalid_0's ndcg@40: 0.980759\tvalid_0's ndcg@50: 0.980759\n","[17]\tvalid_0's ndcg@10: 0.98084\tvalid_0's ndcg@20: 0.98084\tvalid_0's ndcg@30: 0.98084\tvalid_0's ndcg@40: 0.98084\tvalid_0's ndcg@50: 0.98084\tvalid_0's ndcg@10: 0.98084\tvalid_0's ndcg@20: 0.98084\tvalid_0's ndcg@30: 0.98084\tvalid_0's ndcg@40: 0.98084\tvalid_0's ndcg@50: 0.98084\n","[18]\tvalid_0's ndcg@10: 0.980748\tvalid_0's ndcg@20: 0.980748\tvalid_0's ndcg@30: 0.980748\tvalid_0's ndcg@40: 0.980748\tvalid_0's ndcg@50: 0.980748\tvalid_0's ndcg@10: 0.980748\tvalid_0's ndcg@20: 0.980748\tvalid_0's ndcg@30: 0.980748\tvalid_0's ndcg@40: 0.980748\tvalid_0's ndcg@50: 0.980748\n","[19]\tvalid_0's ndcg@10: 0.980585\tvalid_0's ndcg@20: 0.980585\tvalid_0's ndcg@30: 0.980585\tvalid_0's ndcg@40: 0.980585\tvalid_0's ndcg@50: 0.980585\tvalid_0's ndcg@10: 0.980585\tvalid_0's ndcg@20: 0.980585\tvalid_0's ndcg@30: 0.980585\tvalid_0's ndcg@40: 0.980585\tvalid_0's ndcg@50: 0.980585\n","[20]\tvalid_0's ndcg@10: 0.980633\tvalid_0's ndcg@20: 0.980633\tvalid_0's ndcg@30: 0.980633\tvalid_0's ndcg@40: 0.980633\tvalid_0's ndcg@50: 0.980633\tvalid_0's ndcg@10: 0.980633\tvalid_0's ndcg@20: 0.980633\tvalid_0's ndcg@30: 0.980633\tvalid_0's ndcg@40: 0.980633\tvalid_0's ndcg@50: 0.980633\n","[21]\tvalid_0's ndcg@10: 0.980618\tvalid_0's ndcg@20: 0.980618\tvalid_0's ndcg@30: 0.980618\tvalid_0's ndcg@40: 0.980618\tvalid_0's ndcg@50: 0.980618\tvalid_0's ndcg@10: 0.980618\tvalid_0's ndcg@20: 0.980618\tvalid_0's ndcg@30: 0.980618\tvalid_0's ndcg@40: 0.980618\tvalid_0's ndcg@50: 0.980618\n","[22]\tvalid_0's ndcg@10: 0.980577\tvalid_0's ndcg@20: 0.980577\tvalid_0's ndcg@30: 0.980577\tvalid_0's ndcg@40: 0.980577\tvalid_0's ndcg@50: 0.980577\tvalid_0's ndcg@10: 0.980577\tvalid_0's ndcg@20: 0.980577\tvalid_0's ndcg@30: 0.980577\tvalid_0's ndcg@40: 0.980577\tvalid_0's ndcg@50: 0.980577\n","[23]\tvalid_0's ndcg@10: 0.980655\tvalid_0's ndcg@20: 0.980655\tvalid_0's ndcg@30: 0.980655\tvalid_0's ndcg@40: 0.980655\tvalid_0's ndcg@50: 0.980655\tvalid_0's ndcg@10: 0.980655\tvalid_0's ndcg@20: 0.980655\tvalid_0's ndcg@30: 0.980655\tvalid_0's ndcg@40: 0.980655\tvalid_0's ndcg@50: 0.980655\n","[24]\tvalid_0's ndcg@10: 0.980736\tvalid_0's ndcg@20: 0.980736\tvalid_0's ndcg@30: 0.980736\tvalid_0's ndcg@40: 0.980736\tvalid_0's ndcg@50: 0.980736\tvalid_0's ndcg@10: 0.980736\tvalid_0's ndcg@20: 0.980736\tvalid_0's ndcg@30: 0.980736\tvalid_0's ndcg@40: 0.980736\tvalid_0's ndcg@50: 0.980736\n","[25]\tvalid_0's ndcg@10: 0.980683\tvalid_0's ndcg@20: 0.980683\tvalid_0's ndcg@30: 0.980683\tvalid_0's ndcg@40: 0.980683\tvalid_0's ndcg@50: 0.980683\tvalid_0's ndcg@10: 0.980683\tvalid_0's ndcg@20: 0.980683\tvalid_0's ndcg@30: 0.980683\tvalid_0's ndcg@40: 0.980683\tvalid_0's ndcg@50: 0.980683\n","[26]\tvalid_0's ndcg@10: 0.980622\tvalid_0's ndcg@20: 0.980622\tvalid_0's ndcg@30: 0.980622\tvalid_0's ndcg@40: 0.980622\tvalid_0's ndcg@50: 0.980622\tvalid_0's ndcg@10: 0.980622\tvalid_0's ndcg@20: 0.980622\tvalid_0's ndcg@30: 0.980622\tvalid_0's ndcg@40: 0.980622\tvalid_0's ndcg@50: 0.980622\n","[27]\tvalid_0's ndcg@10: 0.980725\tvalid_0's ndcg@20: 0.980725\tvalid_0's ndcg@30: 0.980725\tvalid_0's ndcg@40: 0.980725\tvalid_0's ndcg@50: 0.980725\tvalid_0's ndcg@10: 0.980725\tvalid_0's ndcg@20: 0.980725\tvalid_0's ndcg@30: 0.980725\tvalid_0's ndcg@40: 0.980725\tvalid_0's ndcg@50: 0.980725\n","[28]\tvalid_0's ndcg@10: 0.98088\tvalid_0's ndcg@20: 0.98088\tvalid_0's ndcg@30: 0.98088\tvalid_0's ndcg@40: 0.98088\tvalid_0's ndcg@50: 0.98088\tvalid_0's ndcg@10: 0.98088\tvalid_0's ndcg@20: 0.98088\tvalid_0's ndcg@30: 0.98088\tvalid_0's ndcg@40: 0.98088\tvalid_0's ndcg@50: 0.98088\n","[29]\tvalid_0's ndcg@10: 0.980957\tvalid_0's ndcg@20: 0.980957\tvalid_0's ndcg@30: 0.980957\tvalid_0's ndcg@40: 0.980957\tvalid_0's ndcg@50: 0.980957\tvalid_0's ndcg@10: 0.980957\tvalid_0's ndcg@20: 0.980957\tvalid_0's ndcg@30: 0.980957\tvalid_0's ndcg@40: 0.980957\tvalid_0's ndcg@50: 0.980957\n","[30]\tvalid_0's ndcg@10: 0.980856\tvalid_0's ndcg@20: 0.980856\tvalid_0's ndcg@30: 0.980856\tvalid_0's ndcg@40: 0.980856\tvalid_0's ndcg@50: 0.980856\tvalid_0's ndcg@10: 0.980856\tvalid_0's ndcg@20: 0.980856\tvalid_0's ndcg@30: 0.980856\tvalid_0's ndcg@40: 0.980856\tvalid_0's ndcg@50: 0.980856\n","[31]\tvalid_0's ndcg@10: 0.980912\tvalid_0's ndcg@20: 0.980912\tvalid_0's ndcg@30: 0.980912\tvalid_0's ndcg@40: 0.980912\tvalid_0's ndcg@50: 0.980912\tvalid_0's ndcg@10: 0.980912\tvalid_0's ndcg@20: 0.980912\tvalid_0's ndcg@30: 0.980912\tvalid_0's ndcg@40: 0.980912\tvalid_0's ndcg@50: 0.980912\n","[32]\tvalid_0's ndcg@10: 0.98111\tvalid_0's ndcg@20: 0.98111\tvalid_0's ndcg@30: 0.98111\tvalid_0's ndcg@40: 0.98111\tvalid_0's ndcg@50: 0.98111\tvalid_0's ndcg@10: 0.98111\tvalid_0's ndcg@20: 0.98111\tvalid_0's ndcg@30: 0.98111\tvalid_0's ndcg@40: 0.98111\tvalid_0's ndcg@50: 0.98111\n","[33]\tvalid_0's ndcg@10: 0.981121\tvalid_0's ndcg@20: 0.981121\tvalid_0's ndcg@30: 0.981121\tvalid_0's ndcg@40: 0.981121\tvalid_0's ndcg@50: 0.981121\tvalid_0's ndcg@10: 0.981121\tvalid_0's ndcg@20: 0.981121\tvalid_0's ndcg@30: 0.981121\tvalid_0's ndcg@40: 0.981121\tvalid_0's ndcg@50: 0.981121\n","[34]\tvalid_0's ndcg@10: 0.981219\tvalid_0's ndcg@20: 0.981219\tvalid_0's ndcg@30: 0.981219\tvalid_0's ndcg@40: 0.981219\tvalid_0's ndcg@50: 0.981219\tvalid_0's ndcg@10: 0.981219\tvalid_0's ndcg@20: 0.981219\tvalid_0's ndcg@30: 0.981219\tvalid_0's ndcg@40: 0.981219\tvalid_0's ndcg@50: 0.981219\n","[35]\tvalid_0's ndcg@10: 0.981285\tvalid_0's ndcg@20: 0.981285\tvalid_0's ndcg@30: 0.981285\tvalid_0's ndcg@40: 0.981285\tvalid_0's ndcg@50: 0.981285\tvalid_0's ndcg@10: 0.981285\tvalid_0's ndcg@20: 0.981285\tvalid_0's ndcg@30: 0.981285\tvalid_0's ndcg@40: 0.981285\tvalid_0's ndcg@50: 0.981285\n","[36]\tvalid_0's ndcg@10: 0.98134\tvalid_0's ndcg@20: 0.98134\tvalid_0's ndcg@30: 0.98134\tvalid_0's ndcg@40: 0.98134\tvalid_0's ndcg@50: 0.98134\tvalid_0's ndcg@10: 0.98134\tvalid_0's ndcg@20: 0.98134\tvalid_0's ndcg@30: 0.98134\tvalid_0's ndcg@40: 0.98134\tvalid_0's ndcg@50: 0.98134\n","[37]\tvalid_0's ndcg@10: 0.981267\tvalid_0's ndcg@20: 0.981267\tvalid_0's ndcg@30: 0.981267\tvalid_0's ndcg@40: 0.981267\tvalid_0's ndcg@50: 0.981267\tvalid_0's ndcg@10: 0.981267\tvalid_0's ndcg@20: 0.981267\tvalid_0's ndcg@30: 0.981267\tvalid_0's ndcg@40: 0.981267\tvalid_0's ndcg@50: 0.981267\n","[38]\tvalid_0's ndcg@10: 0.98138\tvalid_0's ndcg@20: 0.98138\tvalid_0's ndcg@30: 0.98138\tvalid_0's ndcg@40: 0.98138\tvalid_0's ndcg@50: 0.98138\tvalid_0's ndcg@10: 0.98138\tvalid_0's ndcg@20: 0.98138\tvalid_0's ndcg@30: 0.98138\tvalid_0's ndcg@40: 0.98138\tvalid_0's ndcg@50: 0.98138\n","[39]\tvalid_0's ndcg@10: 0.981333\tvalid_0's ndcg@20: 0.981333\tvalid_0's ndcg@30: 0.981333\tvalid_0's ndcg@40: 0.981333\tvalid_0's ndcg@50: 0.981333\tvalid_0's ndcg@10: 0.981333\tvalid_0's ndcg@20: 0.981333\tvalid_0's ndcg@30: 0.981333\tvalid_0's ndcg@40: 0.981333\tvalid_0's ndcg@50: 0.981333\n","[40]\tvalid_0's ndcg@10: 0.981417\tvalid_0's ndcg@20: 0.981417\tvalid_0's ndcg@30: 0.981417\tvalid_0's ndcg@40: 0.981417\tvalid_0's ndcg@50: 0.981417\tvalid_0's ndcg@10: 0.981417\tvalid_0's ndcg@20: 0.981417\tvalid_0's ndcg@30: 0.981417\tvalid_0's ndcg@40: 0.981417\tvalid_0's ndcg@50: 0.981417\n","[41]\tvalid_0's ndcg@10: 0.981489\tvalid_0's ndcg@20: 0.981489\tvalid_0's ndcg@30: 0.981489\tvalid_0's ndcg@40: 0.981489\tvalid_0's ndcg@50: 0.981489\tvalid_0's ndcg@10: 0.981489\tvalid_0's ndcg@20: 0.981489\tvalid_0's ndcg@30: 0.981489\tvalid_0's ndcg@40: 0.981489\tvalid_0's ndcg@50: 0.981489\n","[42]\tvalid_0's ndcg@10: 0.981523\tvalid_0's ndcg@20: 0.981523\tvalid_0's ndcg@30: 0.981523\tvalid_0's ndcg@40: 0.981523\tvalid_0's ndcg@50: 0.981523\tvalid_0's ndcg@10: 0.981523\tvalid_0's ndcg@20: 0.981523\tvalid_0's ndcg@30: 0.981523\tvalid_0's ndcg@40: 0.981523\tvalid_0's ndcg@50: 0.981523\n","[43]\tvalid_0's ndcg@10: 0.981587\tvalid_0's ndcg@20: 0.981587\tvalid_0's ndcg@30: 0.981587\tvalid_0's ndcg@40: 0.981587\tvalid_0's ndcg@50: 0.981587\tvalid_0's ndcg@10: 0.981587\tvalid_0's ndcg@20: 0.981587\tvalid_0's ndcg@30: 0.981587\tvalid_0's ndcg@40: 0.981587\tvalid_0's ndcg@50: 0.981587\n","[44]\tvalid_0's ndcg@10: 0.981476\tvalid_0's ndcg@20: 0.981476\tvalid_0's ndcg@30: 0.981476\tvalid_0's ndcg@40: 0.981476\tvalid_0's ndcg@50: 0.981476\tvalid_0's ndcg@10: 0.981476\tvalid_0's ndcg@20: 0.981476\tvalid_0's ndcg@30: 0.981476\tvalid_0's ndcg@40: 0.981476\tvalid_0's ndcg@50: 0.981476\n","[45]\tvalid_0's ndcg@10: 0.981316\tvalid_0's ndcg@20: 0.981316\tvalid_0's ndcg@30: 0.981316\tvalid_0's ndcg@40: 0.981316\tvalid_0's ndcg@50: 0.981316\tvalid_0's ndcg@10: 0.981316\tvalid_0's ndcg@20: 0.981316\tvalid_0's ndcg@30: 0.981316\tvalid_0's ndcg@40: 0.981316\tvalid_0's ndcg@50: 0.981316\n","[46]\tvalid_0's ndcg@10: 0.981457\tvalid_0's ndcg@20: 0.981457\tvalid_0's ndcg@30: 0.981457\tvalid_0's ndcg@40: 0.981457\tvalid_0's ndcg@50: 0.981457\tvalid_0's ndcg@10: 0.981457\tvalid_0's ndcg@20: 0.981457\tvalid_0's ndcg@30: 0.981457\tvalid_0's ndcg@40: 0.981457\tvalid_0's ndcg@50: 0.981457\n","[47]\tvalid_0's ndcg@10: 0.981435\tvalid_0's ndcg@20: 0.981435\tvalid_0's ndcg@30: 0.981435\tvalid_0's ndcg@40: 0.981435\tvalid_0's ndcg@50: 0.981435\tvalid_0's ndcg@10: 0.981435\tvalid_0's ndcg@20: 0.981435\tvalid_0's ndcg@30: 0.981435\tvalid_0's ndcg@40: 0.981435\tvalid_0's ndcg@50: 0.981435\n","[48]\tvalid_0's ndcg@10: 0.981432\tvalid_0's ndcg@20: 0.981432\tvalid_0's ndcg@30: 0.981432\tvalid_0's ndcg@40: 0.981432\tvalid_0's ndcg@50: 0.981432\tvalid_0's ndcg@10: 0.981432\tvalid_0's ndcg@20: 0.981432\tvalid_0's ndcg@30: 0.981432\tvalid_0's ndcg@40: 0.981432\tvalid_0's ndcg@50: 0.981432\n","[49]\tvalid_0's ndcg@10: 0.981381\tvalid_0's ndcg@20: 0.981381\tvalid_0's ndcg@30: 0.981381\tvalid_0's ndcg@40: 0.981381\tvalid_0's ndcg@50: 0.981381\tvalid_0's ndcg@10: 0.981381\tvalid_0's ndcg@20: 0.981381\tvalid_0's ndcg@30: 0.981381\tvalid_0's ndcg@40: 0.981381\tvalid_0's ndcg@50: 0.981381\n","[50]\tvalid_0's ndcg@10: 0.981298\tvalid_0's ndcg@20: 0.981298\tvalid_0's ndcg@30: 0.981298\tvalid_0's ndcg@40: 0.981298\tvalid_0's ndcg@50: 0.981298\tvalid_0's ndcg@10: 0.981298\tvalid_0's ndcg@20: 0.981298\tvalid_0's ndcg@30: 0.981298\tvalid_0's ndcg@40: 0.981298\tvalid_0's ndcg@50: 0.981298\n","[51]\tvalid_0's ndcg@10: 0.981349\tvalid_0's ndcg@20: 0.981349\tvalid_0's ndcg@30: 0.981349\tvalid_0's ndcg@40: 0.981349\tvalid_0's ndcg@50: 0.981349\tvalid_0's ndcg@10: 0.981349\tvalid_0's ndcg@20: 0.981349\tvalid_0's ndcg@30: 0.981349\tvalid_0's ndcg@40: 0.981349\tvalid_0's ndcg@50: 0.981349\n","[52]\tvalid_0's ndcg@10: 0.981377\tvalid_0's ndcg@20: 0.981377\tvalid_0's ndcg@30: 0.981377\tvalid_0's ndcg@40: 0.981377\tvalid_0's ndcg@50: 0.981377\tvalid_0's ndcg@10: 0.981377\tvalid_0's ndcg@20: 0.981377\tvalid_0's ndcg@30: 0.981377\tvalid_0's ndcg@40: 0.981377\tvalid_0's ndcg@50: 0.981377\n","[53]\tvalid_0's ndcg@10: 0.981295\tvalid_0's ndcg@20: 0.981295\tvalid_0's ndcg@30: 0.981295\tvalid_0's ndcg@40: 0.981295\tvalid_0's ndcg@50: 0.981295\tvalid_0's ndcg@10: 0.981295\tvalid_0's ndcg@20: 0.981295\tvalid_0's ndcg@30: 0.981295\tvalid_0's ndcg@40: 0.981295\tvalid_0's ndcg@50: 0.981295\n","[54]\tvalid_0's ndcg@10: 0.981395\tvalid_0's ndcg@20: 0.981395\tvalid_0's ndcg@30: 0.981395\tvalid_0's ndcg@40: 0.981395\tvalid_0's ndcg@50: 0.981395\tvalid_0's ndcg@10: 0.981395\tvalid_0's ndcg@20: 0.981395\tvalid_0's ndcg@30: 0.981395\tvalid_0's ndcg@40: 0.981395\tvalid_0's ndcg@50: 0.981395\n","[55]\tvalid_0's ndcg@10: 0.981487\tvalid_0's ndcg@20: 0.981487\tvalid_0's ndcg@30: 0.981487\tvalid_0's ndcg@40: 0.981487\tvalid_0's ndcg@50: 0.981487\tvalid_0's ndcg@10: 0.981487\tvalid_0's ndcg@20: 0.981487\tvalid_0's ndcg@30: 0.981487\tvalid_0's ndcg@40: 0.981487\tvalid_0's ndcg@50: 0.981487\n","[56]\tvalid_0's ndcg@10: 0.981567\tvalid_0's ndcg@20: 0.981567\tvalid_0's ndcg@30: 0.981567\tvalid_0's ndcg@40: 0.981567\tvalid_0's ndcg@50: 0.981567\tvalid_0's ndcg@10: 0.981567\tvalid_0's ndcg@20: 0.981567\tvalid_0's ndcg@30: 0.981567\tvalid_0's ndcg@40: 0.981567\tvalid_0's ndcg@50: 0.981567\n","[57]\tvalid_0's ndcg@10: 0.981521\tvalid_0's ndcg@20: 0.981521\tvalid_0's ndcg@30: 0.981521\tvalid_0's ndcg@40: 0.981521\tvalid_0's ndcg@50: 0.981521\tvalid_0's ndcg@10: 0.981521\tvalid_0's ndcg@20: 0.981521\tvalid_0's ndcg@30: 0.981521\tvalid_0's ndcg@40: 0.981521\tvalid_0's ndcg@50: 0.981521\n","[58]\tvalid_0's ndcg@10: 0.981589\tvalid_0's ndcg@20: 0.981589\tvalid_0's ndcg@30: 0.981589\tvalid_0's ndcg@40: 0.981589\tvalid_0's ndcg@50: 0.981589\tvalid_0's ndcg@10: 0.981589\tvalid_0's ndcg@20: 0.981589\tvalid_0's ndcg@30: 0.981589\tvalid_0's ndcg@40: 0.981589\tvalid_0's ndcg@50: 0.981589\n","[59]\tvalid_0's ndcg@10: 0.981538\tvalid_0's ndcg@20: 0.981538\tvalid_0's ndcg@30: 0.981538\tvalid_0's ndcg@40: 0.981538\tvalid_0's ndcg@50: 0.981538\tvalid_0's ndcg@10: 0.981538\tvalid_0's ndcg@20: 0.981538\tvalid_0's ndcg@30: 0.981538\tvalid_0's ndcg@40: 0.981538\tvalid_0's ndcg@50: 0.981538\n","[60]\tvalid_0's ndcg@10: 0.981639\tvalid_0's ndcg@20: 0.981639\tvalid_0's ndcg@30: 0.981639\tvalid_0's ndcg@40: 0.981639\tvalid_0's ndcg@50: 0.981639\tvalid_0's ndcg@10: 0.981639\tvalid_0's ndcg@20: 0.981639\tvalid_0's ndcg@30: 0.981639\tvalid_0's ndcg@40: 0.981639\tvalid_0's ndcg@50: 0.981639\n","[61]\tvalid_0's ndcg@10: 0.98157\tvalid_0's ndcg@20: 0.98157\tvalid_0's ndcg@30: 0.98157\tvalid_0's ndcg@40: 0.98157\tvalid_0's ndcg@50: 0.98157\tvalid_0's ndcg@10: 0.98157\tvalid_0's ndcg@20: 0.98157\tvalid_0's ndcg@30: 0.98157\tvalid_0's ndcg@40: 0.98157\tvalid_0's ndcg@50: 0.98157\n","[62]\tvalid_0's ndcg@10: 0.98159\tvalid_0's ndcg@20: 0.98159\tvalid_0's ndcg@30: 0.98159\tvalid_0's ndcg@40: 0.98159\tvalid_0's ndcg@50: 0.98159\tvalid_0's ndcg@10: 0.98159\tvalid_0's ndcg@20: 0.98159\tvalid_0's ndcg@30: 0.98159\tvalid_0's ndcg@40: 0.98159\tvalid_0's ndcg@50: 0.98159\n","[63]\tvalid_0's ndcg@10: 0.981472\tvalid_0's ndcg@20: 0.981472\tvalid_0's ndcg@30: 0.981472\tvalid_0's ndcg@40: 0.981472\tvalid_0's ndcg@50: 0.981472\tvalid_0's ndcg@10: 0.981472\tvalid_0's ndcg@20: 0.981472\tvalid_0's ndcg@30: 0.981472\tvalid_0's ndcg@40: 0.981472\tvalid_0's ndcg@50: 0.981472\n","[64]\tvalid_0's ndcg@10: 0.981446\tvalid_0's ndcg@20: 0.981446\tvalid_0's ndcg@30: 0.981446\tvalid_0's ndcg@40: 0.981446\tvalid_0's ndcg@50: 0.981446\tvalid_0's ndcg@10: 0.981446\tvalid_0's ndcg@20: 0.981446\tvalid_0's ndcg@30: 0.981446\tvalid_0's ndcg@40: 0.981446\tvalid_0's ndcg@50: 0.981446\n","[65]\tvalid_0's ndcg@10: 0.981434\tvalid_0's ndcg@20: 0.981434\tvalid_0's ndcg@30: 0.981434\tvalid_0's ndcg@40: 0.981434\tvalid_0's ndcg@50: 0.981434\tvalid_0's ndcg@10: 0.981434\tvalid_0's ndcg@20: 0.981434\tvalid_0's ndcg@30: 0.981434\tvalid_0's ndcg@40: 0.981434\tvalid_0's ndcg@50: 0.981434\n","[66]\tvalid_0's ndcg@10: 0.981385\tvalid_0's ndcg@20: 0.981385\tvalid_0's ndcg@30: 0.981385\tvalid_0's ndcg@40: 0.981385\tvalid_0's ndcg@50: 0.981385\tvalid_0's ndcg@10: 0.981385\tvalid_0's ndcg@20: 0.981385\tvalid_0's ndcg@30: 0.981385\tvalid_0's ndcg@40: 0.981385\tvalid_0's ndcg@50: 0.981385\n","[67]\tvalid_0's ndcg@10: 0.981433\tvalid_0's ndcg@20: 0.981433\tvalid_0's ndcg@30: 0.981433\tvalid_0's ndcg@40: 0.981433\tvalid_0's ndcg@50: 0.981433\tvalid_0's ndcg@10: 0.981433\tvalid_0's ndcg@20: 0.981433\tvalid_0's ndcg@30: 0.981433\tvalid_0's ndcg@40: 0.981433\tvalid_0's ndcg@50: 0.981433\n","[68]\tvalid_0's ndcg@10: 0.98142\tvalid_0's ndcg@20: 0.98142\tvalid_0's ndcg@30: 0.98142\tvalid_0's ndcg@40: 0.98142\tvalid_0's ndcg@50: 0.98142\tvalid_0's ndcg@10: 0.98142\tvalid_0's ndcg@20: 0.98142\tvalid_0's ndcg@30: 0.98142\tvalid_0's ndcg@40: 0.98142\tvalid_0's ndcg@50: 0.98142\n","[69]\tvalid_0's ndcg@10: 0.98142\tvalid_0's ndcg@20: 0.98142\tvalid_0's ndcg@30: 0.98142\tvalid_0's ndcg@40: 0.98142\tvalid_0's ndcg@50: 0.98142\tvalid_0's ndcg@10: 0.98142\tvalid_0's ndcg@20: 0.98142\tvalid_0's ndcg@30: 0.98142\tvalid_0's ndcg@40: 0.98142\tvalid_0's ndcg@50: 0.98142\n","[70]\tvalid_0's ndcg@10: 0.981375\tvalid_0's ndcg@20: 0.981375\tvalid_0's ndcg@30: 0.981375\tvalid_0's ndcg@40: 0.981375\tvalid_0's ndcg@50: 0.981375\tvalid_0's ndcg@10: 0.981375\tvalid_0's ndcg@20: 0.981375\tvalid_0's ndcg@30: 0.981375\tvalid_0's ndcg@40: 0.981375\tvalid_0's ndcg@50: 0.981375\n","[71]\tvalid_0's ndcg@10: 0.981352\tvalid_0's ndcg@20: 0.981352\tvalid_0's ndcg@30: 0.981352\tvalid_0's ndcg@40: 0.981352\tvalid_0's ndcg@50: 0.981352\tvalid_0's ndcg@10: 0.981352\tvalid_0's ndcg@20: 0.981352\tvalid_0's ndcg@30: 0.981352\tvalid_0's ndcg@40: 0.981352\tvalid_0's ndcg@50: 0.981352\n","[72]\tvalid_0's ndcg@10: 0.981338\tvalid_0's ndcg@20: 0.981338\tvalid_0's ndcg@30: 0.981338\tvalid_0's ndcg@40: 0.981338\tvalid_0's ndcg@50: 0.981338\tvalid_0's ndcg@10: 0.981338\tvalid_0's ndcg@20: 0.981338\tvalid_0's ndcg@30: 0.981338\tvalid_0's ndcg@40: 0.981338\tvalid_0's ndcg@50: 0.981338\n","[73]\tvalid_0's ndcg@10: 0.98132\tvalid_0's ndcg@20: 0.98132\tvalid_0's ndcg@30: 0.98132\tvalid_0's ndcg@40: 0.98132\tvalid_0's ndcg@50: 0.98132\tvalid_0's ndcg@10: 0.98132\tvalid_0's ndcg@20: 0.98132\tvalid_0's ndcg@30: 0.98132\tvalid_0's ndcg@40: 0.98132\tvalid_0's ndcg@50: 0.98132\n","[74]\tvalid_0's ndcg@10: 0.981291\tvalid_0's ndcg@20: 0.981291\tvalid_0's ndcg@30: 0.981291\tvalid_0's ndcg@40: 0.981291\tvalid_0's ndcg@50: 0.981291\tvalid_0's ndcg@10: 0.981291\tvalid_0's ndcg@20: 0.981291\tvalid_0's ndcg@30: 0.981291\tvalid_0's ndcg@40: 0.981291\tvalid_0's ndcg@50: 0.981291\n","[75]\tvalid_0's ndcg@10: 0.981268\tvalid_0's ndcg@20: 0.981268\tvalid_0's ndcg@30: 0.981268\tvalid_0's ndcg@40: 0.981268\tvalid_0's ndcg@50: 0.981268\tvalid_0's ndcg@10: 0.981268\tvalid_0's ndcg@20: 0.981268\tvalid_0's ndcg@30: 0.981268\tvalid_0's ndcg@40: 0.981268\tvalid_0's ndcg@50: 0.981268\n","[76]\tvalid_0's ndcg@10: 0.981263\tvalid_0's ndcg@20: 0.981263\tvalid_0's ndcg@30: 0.981263\tvalid_0's ndcg@40: 0.981263\tvalid_0's ndcg@50: 0.981263\tvalid_0's ndcg@10: 0.981263\tvalid_0's ndcg@20: 0.981263\tvalid_0's ndcg@30: 0.981263\tvalid_0's ndcg@40: 0.981263\tvalid_0's ndcg@50: 0.981263\n","[77]\tvalid_0's ndcg@10: 0.981378\tvalid_0's ndcg@20: 0.981378\tvalid_0's ndcg@30: 0.981378\tvalid_0's ndcg@40: 0.981378\tvalid_0's ndcg@50: 0.981378\tvalid_0's ndcg@10: 0.981378\tvalid_0's ndcg@20: 0.981378\tvalid_0's ndcg@30: 0.981378\tvalid_0's ndcg@40: 0.981378\tvalid_0's ndcg@50: 0.981378\n","[78]\tvalid_0's ndcg@10: 0.981378\tvalid_0's ndcg@20: 0.981378\tvalid_0's ndcg@30: 0.981378\tvalid_0's ndcg@40: 0.981378\tvalid_0's ndcg@50: 0.981378\tvalid_0's ndcg@10: 0.981378\tvalid_0's ndcg@20: 0.981378\tvalid_0's ndcg@30: 0.981378\tvalid_0's ndcg@40: 0.981378\tvalid_0's ndcg@50: 0.981378\n","[79]\tvalid_0's ndcg@10: 0.981373\tvalid_0's ndcg@20: 0.981373\tvalid_0's ndcg@30: 0.981373\tvalid_0's ndcg@40: 0.981373\tvalid_0's ndcg@50: 0.981373\tvalid_0's ndcg@10: 0.981373\tvalid_0's ndcg@20: 0.981373\tvalid_0's ndcg@30: 0.981373\tvalid_0's ndcg@40: 0.981373\tvalid_0's ndcg@50: 0.981373\n","[80]\tvalid_0's ndcg@10: 0.981293\tvalid_0's ndcg@20: 0.981293\tvalid_0's ndcg@30: 0.981293\tvalid_0's ndcg@40: 0.981293\tvalid_0's ndcg@50: 0.981293\tvalid_0's ndcg@10: 0.981293\tvalid_0's ndcg@20: 0.981293\tvalid_0's ndcg@30: 0.981293\tvalid_0's ndcg@40: 0.981293\tvalid_0's ndcg@50: 0.981293\n","[81]\tvalid_0's ndcg@10: 0.981373\tvalid_0's ndcg@20: 0.981373\tvalid_0's ndcg@30: 0.981373\tvalid_0's ndcg@40: 0.981373\tvalid_0's ndcg@50: 0.981373\tvalid_0's ndcg@10: 0.981373\tvalid_0's ndcg@20: 0.981373\tvalid_0's ndcg@30: 0.981373\tvalid_0's ndcg@40: 0.981373\tvalid_0's ndcg@50: 0.981373\n","[82]\tvalid_0's ndcg@10: 0.981185\tvalid_0's ndcg@20: 0.981185\tvalid_0's ndcg@30: 0.981185\tvalid_0's ndcg@40: 0.981185\tvalid_0's ndcg@50: 0.981185\tvalid_0's ndcg@10: 0.981185\tvalid_0's ndcg@20: 0.981185\tvalid_0's ndcg@30: 0.981185\tvalid_0's ndcg@40: 0.981185\tvalid_0's ndcg@50: 0.981185\n","[83]\tvalid_0's ndcg@10: 0.981315\tvalid_0's ndcg@20: 0.981315\tvalid_0's ndcg@30: 0.981315\tvalid_0's ndcg@40: 0.981315\tvalid_0's ndcg@50: 0.981315\tvalid_0's ndcg@10: 0.981315\tvalid_0's ndcg@20: 0.981315\tvalid_0's ndcg@30: 0.981315\tvalid_0's ndcg@40: 0.981315\tvalid_0's ndcg@50: 0.981315\n","[84]\tvalid_0's ndcg@10: 0.981225\tvalid_0's ndcg@20: 0.981225\tvalid_0's ndcg@30: 0.981225\tvalid_0's ndcg@40: 0.981225\tvalid_0's ndcg@50: 0.981225\tvalid_0's ndcg@10: 0.981225\tvalid_0's ndcg@20: 0.981225\tvalid_0's ndcg@30: 0.981225\tvalid_0's ndcg@40: 0.981225\tvalid_0's ndcg@50: 0.981225\n","[85]\tvalid_0's ndcg@10: 0.981206\tvalid_0's ndcg@20: 0.981206\tvalid_0's ndcg@30: 0.981206\tvalid_0's ndcg@40: 0.981206\tvalid_0's ndcg@50: 0.981206\tvalid_0's ndcg@10: 0.981206\tvalid_0's ndcg@20: 0.981206\tvalid_0's ndcg@30: 0.981206\tvalid_0's ndcg@40: 0.981206\tvalid_0's ndcg@50: 0.981206\n","[86]\tvalid_0's ndcg@10: 0.981188\tvalid_0's ndcg@20: 0.981188\tvalid_0's ndcg@30: 0.981188\tvalid_0's ndcg@40: 0.981188\tvalid_0's ndcg@50: 0.981188\tvalid_0's ndcg@10: 0.981188\tvalid_0's ndcg@20: 0.981188\tvalid_0's ndcg@30: 0.981188\tvalid_0's ndcg@40: 0.981188\tvalid_0's ndcg@50: 0.981188\n","[87]\tvalid_0's ndcg@10: 0.981124\tvalid_0's ndcg@20: 0.981124\tvalid_0's ndcg@30: 0.981124\tvalid_0's ndcg@40: 0.981124\tvalid_0's ndcg@50: 0.981124\tvalid_0's ndcg@10: 0.981124\tvalid_0's ndcg@20: 0.981124\tvalid_0's ndcg@30: 0.981124\tvalid_0's ndcg@40: 0.981124\tvalid_0's ndcg@50: 0.981124\n","[88]\tvalid_0's ndcg@10: 0.981099\tvalid_0's ndcg@20: 0.981099\tvalid_0's ndcg@30: 0.981099\tvalid_0's ndcg@40: 0.981099\tvalid_0's ndcg@50: 0.981099\tvalid_0's ndcg@10: 0.981099\tvalid_0's ndcg@20: 0.981099\tvalid_0's ndcg@30: 0.981099\tvalid_0's ndcg@40: 0.981099\tvalid_0's ndcg@50: 0.981099\n","[89]\tvalid_0's ndcg@10: 0.981166\tvalid_0's ndcg@20: 0.981166\tvalid_0's ndcg@30: 0.981166\tvalid_0's ndcg@40: 0.981166\tvalid_0's ndcg@50: 0.981166\tvalid_0's ndcg@10: 0.981166\tvalid_0's ndcg@20: 0.981166\tvalid_0's ndcg@30: 0.981166\tvalid_0's ndcg@40: 0.981166\tvalid_0's ndcg@50: 0.981166\n","[90]\tvalid_0's ndcg@10: 0.981227\tvalid_0's ndcg@20: 0.981227\tvalid_0's ndcg@30: 0.981227\tvalid_0's ndcg@40: 0.981227\tvalid_0's ndcg@50: 0.981227\tvalid_0's ndcg@10: 0.981227\tvalid_0's ndcg@20: 0.981227\tvalid_0's ndcg@30: 0.981227\tvalid_0's ndcg@40: 0.981227\tvalid_0's ndcg@50: 0.981227\n","[91]\tvalid_0's ndcg@10: 0.981195\tvalid_0's ndcg@20: 0.981195\tvalid_0's ndcg@30: 0.981195\tvalid_0's ndcg@40: 0.981195\tvalid_0's ndcg@50: 0.981195\tvalid_0's ndcg@10: 0.981195\tvalid_0's ndcg@20: 0.981195\tvalid_0's ndcg@30: 0.981195\tvalid_0's ndcg@40: 0.981195\tvalid_0's ndcg@50: 0.981195\n","[92]\tvalid_0's ndcg@10: 0.981142\tvalid_0's ndcg@20: 0.981142\tvalid_0's ndcg@30: 0.981142\tvalid_0's ndcg@40: 0.981142\tvalid_0's ndcg@50: 0.981142\tvalid_0's ndcg@10: 0.981142\tvalid_0's ndcg@20: 0.981142\tvalid_0's ndcg@30: 0.981142\tvalid_0's ndcg@40: 0.981142\tvalid_0's ndcg@50: 0.981142\n","[93]\tvalid_0's ndcg@10: 0.981161\tvalid_0's ndcg@20: 0.981161\tvalid_0's ndcg@30: 0.981161\tvalid_0's ndcg@40: 0.981161\tvalid_0's ndcg@50: 0.981161\tvalid_0's ndcg@10: 0.981161\tvalid_0's ndcg@20: 0.981161\tvalid_0's ndcg@30: 0.981161\tvalid_0's ndcg@40: 0.981161\tvalid_0's ndcg@50: 0.981161\n","[94]\tvalid_0's ndcg@10: 0.981098\tvalid_0's ndcg@20: 0.981098\tvalid_0's ndcg@30: 0.981098\tvalid_0's ndcg@40: 0.981098\tvalid_0's ndcg@50: 0.981098\tvalid_0's ndcg@10: 0.981098\tvalid_0's ndcg@20: 0.981098\tvalid_0's ndcg@30: 0.981098\tvalid_0's ndcg@40: 0.981098\tvalid_0's ndcg@50: 0.981098\n","[95]\tvalid_0's ndcg@10: 0.981172\tvalid_0's ndcg@20: 0.981172\tvalid_0's ndcg@30: 0.981172\tvalid_0's ndcg@40: 0.981172\tvalid_0's ndcg@50: 0.981172\tvalid_0's ndcg@10: 0.981172\tvalid_0's ndcg@20: 0.981172\tvalid_0's ndcg@30: 0.981172\tvalid_0's ndcg@40: 0.981172\tvalid_0's ndcg@50: 0.981172\n","[96]\tvalid_0's ndcg@10: 0.981235\tvalid_0's ndcg@20: 0.981235\tvalid_0's ndcg@30: 0.981235\tvalid_0's ndcg@40: 0.981235\tvalid_0's ndcg@50: 0.981235\tvalid_0's ndcg@10: 0.981235\tvalid_0's ndcg@20: 0.981235\tvalid_0's ndcg@30: 0.981235\tvalid_0's ndcg@40: 0.981235\tvalid_0's ndcg@50: 0.981235\n","[97]\tvalid_0's ndcg@10: 0.981339\tvalid_0's ndcg@20: 0.981339\tvalid_0's ndcg@30: 0.981339\tvalid_0's ndcg@40: 0.981339\tvalid_0's ndcg@50: 0.981339\tvalid_0's ndcg@10: 0.981339\tvalid_0's ndcg@20: 0.981339\tvalid_0's ndcg@30: 0.981339\tvalid_0's ndcg@40: 0.981339\tvalid_0's ndcg@50: 0.981339\n","[98]\tvalid_0's ndcg@10: 0.981414\tvalid_0's ndcg@20: 0.981414\tvalid_0's ndcg@30: 0.981414\tvalid_0's ndcg@40: 0.981414\tvalid_0's ndcg@50: 0.981414\tvalid_0's ndcg@10: 0.981414\tvalid_0's ndcg@20: 0.981414\tvalid_0's ndcg@30: 0.981414\tvalid_0's ndcg@40: 0.981414\tvalid_0's ndcg@50: 0.981414\n","[99]\tvalid_0's ndcg@10: 0.98124\tvalid_0's ndcg@20: 0.98124\tvalid_0's ndcg@30: 0.98124\tvalid_0's ndcg@40: 0.98124\tvalid_0's ndcg@50: 0.98124\tvalid_0's ndcg@10: 0.98124\tvalid_0's ndcg@20: 0.98124\tvalid_0's ndcg@30: 0.98124\tvalid_0's ndcg@40: 0.98124\tvalid_0's ndcg@50: 0.98124\n","[100]\tvalid_0's ndcg@10: 0.981259\tvalid_0's ndcg@20: 0.981259\tvalid_0's ndcg@30: 0.981259\tvalid_0's ndcg@40: 0.981259\tvalid_0's ndcg@50: 0.981259\tvalid_0's ndcg@10: 0.981259\tvalid_0's ndcg@20: 0.981259\tvalid_0's ndcg@30: 0.981259\tvalid_0's ndcg@40: 0.981259\tvalid_0's ndcg@50: 0.981259\n","Did not meet early stopping. Best iteration is:\n","[60]\tvalid_0's ndcg@10: 0.981639\tvalid_0's ndcg@20: 0.981639\tvalid_0's ndcg@30: 0.981639\tvalid_0's ndcg@40: 0.981639\tvalid_0's ndcg@50: 0.981639\tvalid_0's ndcg@10: 0.981639\tvalid_0's ndcg@20: 0.981639\tvalid_0's ndcg@30: 0.981639\tvalid_0's ndcg@40: 0.981639\tvalid_0's ndcg@50: 0.981639\n"]}]},{"cell_type":"code","source":["!python src/rank/GBDT_rank.py --task usercf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t7ovm_eW3-Yj","executionInfo":{"status":"ok","timestamp":1639076272937,"user_tz":-480,"elapsed":37375,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"7a1bd3c1-0489-48ef-eafb-9c2c99679be2"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["[1]\tvalid_0's ndcg@10: 0.990359\tvalid_0's ndcg@20: 0.990359\tvalid_0's ndcg@30: 0.990359\tvalid_0's ndcg@40: 0.990359\tvalid_0's ndcg@50: 0.990359\tvalid_0's ndcg@10: 0.990359\tvalid_0's ndcg@20: 0.990359\tvalid_0's ndcg@30: 0.990359\tvalid_0's ndcg@40: 0.990359\tvalid_0's ndcg@50: 0.990359\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.989929\tvalid_0's ndcg@20: 0.989929\tvalid_0's ndcg@30: 0.989929\tvalid_0's ndcg@40: 0.989929\tvalid_0's ndcg@50: 0.989929\tvalid_0's ndcg@10: 0.989929\tvalid_0's ndcg@20: 0.989929\tvalid_0's ndcg@30: 0.989929\tvalid_0's ndcg@40: 0.989929\tvalid_0's ndcg@50: 0.989929\n","[3]\tvalid_0's ndcg@10: 0.991255\tvalid_0's ndcg@20: 0.991255\tvalid_0's ndcg@30: 0.991255\tvalid_0's ndcg@40: 0.991255\tvalid_0's ndcg@50: 0.991255\tvalid_0's ndcg@10: 0.991255\tvalid_0's ndcg@20: 0.991255\tvalid_0's ndcg@30: 0.991255\tvalid_0's ndcg@40: 0.991255\tvalid_0's ndcg@50: 0.991255\n","[4]\tvalid_0's ndcg@10: 0.99128\tvalid_0's ndcg@20: 0.99128\tvalid_0's ndcg@30: 0.99128\tvalid_0's ndcg@40: 0.99128\tvalid_0's ndcg@50: 0.99128\tvalid_0's ndcg@10: 0.99128\tvalid_0's ndcg@20: 0.99128\tvalid_0's ndcg@30: 0.99128\tvalid_0's ndcg@40: 0.99128\tvalid_0's ndcg@50: 0.99128\n","[5]\tvalid_0's ndcg@10: 0.991542\tvalid_0's ndcg@20: 0.991542\tvalid_0's ndcg@30: 0.991542\tvalid_0's ndcg@40: 0.991542\tvalid_0's ndcg@50: 0.991542\tvalid_0's ndcg@10: 0.991542\tvalid_0's ndcg@20: 0.991542\tvalid_0's ndcg@30: 0.991542\tvalid_0's ndcg@40: 0.991542\tvalid_0's ndcg@50: 0.991542\n","[6]\tvalid_0's ndcg@10: 0.991805\tvalid_0's ndcg@20: 0.991805\tvalid_0's ndcg@30: 0.991805\tvalid_0's ndcg@40: 0.991805\tvalid_0's ndcg@50: 0.991805\tvalid_0's ndcg@10: 0.991805\tvalid_0's ndcg@20: 0.991805\tvalid_0's ndcg@30: 0.991805\tvalid_0's ndcg@40: 0.991805\tvalid_0's ndcg@50: 0.991805\n","[7]\tvalid_0's ndcg@10: 0.991623\tvalid_0's ndcg@20: 0.991623\tvalid_0's ndcg@30: 0.991623\tvalid_0's ndcg@40: 0.991623\tvalid_0's ndcg@50: 0.991623\tvalid_0's ndcg@10: 0.991623\tvalid_0's ndcg@20: 0.991623\tvalid_0's ndcg@30: 0.991623\tvalid_0's ndcg@40: 0.991623\tvalid_0's ndcg@50: 0.991623\n","[8]\tvalid_0's ndcg@10: 0.991746\tvalid_0's ndcg@20: 0.991746\tvalid_0's ndcg@30: 0.991746\tvalid_0's ndcg@40: 0.991746\tvalid_0's ndcg@50: 0.991746\tvalid_0's ndcg@10: 0.991746\tvalid_0's ndcg@20: 0.991746\tvalid_0's ndcg@30: 0.991746\tvalid_0's ndcg@40: 0.991746\tvalid_0's ndcg@50: 0.991746\n","[9]\tvalid_0's ndcg@10: 0.991987\tvalid_0's ndcg@20: 0.991987\tvalid_0's ndcg@30: 0.991987\tvalid_0's ndcg@40: 0.991987\tvalid_0's ndcg@50: 0.991987\tvalid_0's ndcg@10: 0.991987\tvalid_0's ndcg@20: 0.991987\tvalid_0's ndcg@30: 0.991987\tvalid_0's ndcg@40: 0.991987\tvalid_0's ndcg@50: 0.991987\n","[10]\tvalid_0's ndcg@10: 0.992153\tvalid_0's ndcg@20: 0.992153\tvalid_0's ndcg@30: 0.992153\tvalid_0's ndcg@40: 0.992153\tvalid_0's ndcg@50: 0.992153\tvalid_0's ndcg@10: 0.992153\tvalid_0's ndcg@20: 0.992153\tvalid_0's ndcg@30: 0.992153\tvalid_0's ndcg@40: 0.992153\tvalid_0's ndcg@50: 0.992153\n","[11]\tvalid_0's ndcg@10: 0.991839\tvalid_0's ndcg@20: 0.991839\tvalid_0's ndcg@30: 0.991839\tvalid_0's ndcg@40: 0.991839\tvalid_0's ndcg@50: 0.991839\tvalid_0's ndcg@10: 0.991839\tvalid_0's ndcg@20: 0.991839\tvalid_0's ndcg@30: 0.991839\tvalid_0's ndcg@40: 0.991839\tvalid_0's ndcg@50: 0.991839\n","[12]\tvalid_0's ndcg@10: 0.991993\tvalid_0's ndcg@20: 0.991993\tvalid_0's ndcg@30: 0.991993\tvalid_0's ndcg@40: 0.991993\tvalid_0's ndcg@50: 0.991993\tvalid_0's ndcg@10: 0.991993\tvalid_0's ndcg@20: 0.991993\tvalid_0's ndcg@30: 0.991993\tvalid_0's ndcg@40: 0.991993\tvalid_0's ndcg@50: 0.991993\n","[13]\tvalid_0's ndcg@10: 0.992001\tvalid_0's ndcg@20: 0.992001\tvalid_0's ndcg@30: 0.992001\tvalid_0's ndcg@40: 0.992001\tvalid_0's ndcg@50: 0.992001\tvalid_0's ndcg@10: 0.992001\tvalid_0's ndcg@20: 0.992001\tvalid_0's ndcg@30: 0.992001\tvalid_0's ndcg@40: 0.992001\tvalid_0's ndcg@50: 0.992001\n","[14]\tvalid_0's ndcg@10: 0.992082\tvalid_0's ndcg@20: 0.992082\tvalid_0's ndcg@30: 0.992082\tvalid_0's ndcg@40: 0.992082\tvalid_0's ndcg@50: 0.992082\tvalid_0's ndcg@10: 0.992082\tvalid_0's ndcg@20: 0.992082\tvalid_0's ndcg@30: 0.992082\tvalid_0's ndcg@40: 0.992082\tvalid_0's ndcg@50: 0.992082\n","[15]\tvalid_0's ndcg@10: 0.992136\tvalid_0's ndcg@20: 0.992136\tvalid_0's ndcg@30: 0.992136\tvalid_0's ndcg@40: 0.992136\tvalid_0's ndcg@50: 0.992136\tvalid_0's ndcg@10: 0.992136\tvalid_0's ndcg@20: 0.992136\tvalid_0's ndcg@30: 0.992136\tvalid_0's ndcg@40: 0.992136\tvalid_0's ndcg@50: 0.992136\n","[16]\tvalid_0's ndcg@10: 0.992145\tvalid_0's ndcg@20: 0.992145\tvalid_0's ndcg@30: 0.992145\tvalid_0's ndcg@40: 0.992145\tvalid_0's ndcg@50: 0.992145\tvalid_0's ndcg@10: 0.992145\tvalid_0's ndcg@20: 0.992145\tvalid_0's ndcg@30: 0.992145\tvalid_0's ndcg@40: 0.992145\tvalid_0's ndcg@50: 0.992145\n","[17]\tvalid_0's ndcg@10: 0.992155\tvalid_0's ndcg@20: 0.992155\tvalid_0's ndcg@30: 0.992155\tvalid_0's ndcg@40: 0.992155\tvalid_0's ndcg@50: 0.992155\tvalid_0's ndcg@10: 0.992155\tvalid_0's ndcg@20: 0.992155\tvalid_0's ndcg@30: 0.992155\tvalid_0's ndcg@40: 0.992155\tvalid_0's ndcg@50: 0.992155\n","[18]\tvalid_0's ndcg@10: 0.992137\tvalid_0's ndcg@20: 0.992137\tvalid_0's ndcg@30: 0.992137\tvalid_0's ndcg@40: 0.992137\tvalid_0's ndcg@50: 0.992137\tvalid_0's ndcg@10: 0.992137\tvalid_0's ndcg@20: 0.992137\tvalid_0's ndcg@30: 0.992137\tvalid_0's ndcg@40: 0.992137\tvalid_0's ndcg@50: 0.992137\n","[19]\tvalid_0's ndcg@10: 0.992081\tvalid_0's ndcg@20: 0.992081\tvalid_0's ndcg@30: 0.992081\tvalid_0's ndcg@40: 0.992081\tvalid_0's ndcg@50: 0.992081\tvalid_0's ndcg@10: 0.992081\tvalid_0's ndcg@20: 0.992081\tvalid_0's ndcg@30: 0.992081\tvalid_0's ndcg@40: 0.992081\tvalid_0's ndcg@50: 0.992081\n","[20]\tvalid_0's ndcg@10: 0.992185\tvalid_0's ndcg@20: 0.992185\tvalid_0's ndcg@30: 0.992185\tvalid_0's ndcg@40: 0.992185\tvalid_0's ndcg@50: 0.992185\tvalid_0's ndcg@10: 0.992185\tvalid_0's ndcg@20: 0.992185\tvalid_0's ndcg@30: 0.992185\tvalid_0's ndcg@40: 0.992185\tvalid_0's ndcg@50: 0.992185\n","[21]\tvalid_0's ndcg@10: 0.992208\tvalid_0's ndcg@20: 0.992208\tvalid_0's ndcg@30: 0.992208\tvalid_0's ndcg@40: 0.992208\tvalid_0's ndcg@50: 0.992208\tvalid_0's ndcg@10: 0.992208\tvalid_0's ndcg@20: 0.992208\tvalid_0's ndcg@30: 0.992208\tvalid_0's ndcg@40: 0.992208\tvalid_0's ndcg@50: 0.992208\n","[22]\tvalid_0's ndcg@10: 0.992104\tvalid_0's ndcg@20: 0.992104\tvalid_0's ndcg@30: 0.992104\tvalid_0's ndcg@40: 0.992104\tvalid_0's ndcg@50: 0.992104\tvalid_0's ndcg@10: 0.992104\tvalid_0's ndcg@20: 0.992104\tvalid_0's ndcg@30: 0.992104\tvalid_0's ndcg@40: 0.992104\tvalid_0's ndcg@50: 0.992104\n","[23]\tvalid_0's ndcg@10: 0.992224\tvalid_0's ndcg@20: 0.992224\tvalid_0's ndcg@30: 0.992224\tvalid_0's ndcg@40: 0.992224\tvalid_0's ndcg@50: 0.992224\tvalid_0's ndcg@10: 0.992224\tvalid_0's ndcg@20: 0.992224\tvalid_0's ndcg@30: 0.992224\tvalid_0's ndcg@40: 0.992224\tvalid_0's ndcg@50: 0.992224\n","[24]\tvalid_0's ndcg@10: 0.992183\tvalid_0's ndcg@20: 0.992183\tvalid_0's ndcg@30: 0.992183\tvalid_0's ndcg@40: 0.992183\tvalid_0's ndcg@50: 0.992183\tvalid_0's ndcg@10: 0.992183\tvalid_0's ndcg@20: 0.992183\tvalid_0's ndcg@30: 0.992183\tvalid_0's ndcg@40: 0.992183\tvalid_0's ndcg@50: 0.992183\n","[25]\tvalid_0's ndcg@10: 0.992243\tvalid_0's ndcg@20: 0.992243\tvalid_0's ndcg@30: 0.992243\tvalid_0's ndcg@40: 0.992243\tvalid_0's ndcg@50: 0.992243\tvalid_0's ndcg@10: 0.992243\tvalid_0's ndcg@20: 0.992243\tvalid_0's ndcg@30: 0.992243\tvalid_0's ndcg@40: 0.992243\tvalid_0's ndcg@50: 0.992243\n","[26]\tvalid_0's ndcg@10: 0.99232\tvalid_0's ndcg@20: 0.99232\tvalid_0's ndcg@30: 0.99232\tvalid_0's ndcg@40: 0.99232\tvalid_0's ndcg@50: 0.99232\tvalid_0's ndcg@10: 0.99232\tvalid_0's ndcg@20: 0.99232\tvalid_0's ndcg@30: 0.99232\tvalid_0's ndcg@40: 0.99232\tvalid_0's ndcg@50: 0.99232\n","[27]\tvalid_0's ndcg@10: 0.99227\tvalid_0's ndcg@20: 0.99227\tvalid_0's ndcg@30: 0.99227\tvalid_0's ndcg@40: 0.99227\tvalid_0's ndcg@50: 0.99227\tvalid_0's ndcg@10: 0.99227\tvalid_0's ndcg@20: 0.99227\tvalid_0's ndcg@30: 0.99227\tvalid_0's ndcg@40: 0.99227\tvalid_0's ndcg@50: 0.99227\n","[28]\tvalid_0's ndcg@10: 0.99229\tvalid_0's ndcg@20: 0.99229\tvalid_0's ndcg@30: 0.99229\tvalid_0's ndcg@40: 0.99229\tvalid_0's ndcg@50: 0.99229\tvalid_0's ndcg@10: 0.99229\tvalid_0's ndcg@20: 0.99229\tvalid_0's ndcg@30: 0.99229\tvalid_0's ndcg@40: 0.99229\tvalid_0's ndcg@50: 0.99229\n","[29]\tvalid_0's ndcg@10: 0.99235\tvalid_0's ndcg@20: 0.99235\tvalid_0's ndcg@30: 0.99235\tvalid_0's ndcg@40: 0.99235\tvalid_0's ndcg@50: 0.99235\tvalid_0's ndcg@10: 0.99235\tvalid_0's ndcg@20: 0.99235\tvalid_0's ndcg@30: 0.99235\tvalid_0's ndcg@40: 0.99235\tvalid_0's ndcg@50: 0.99235\n","[30]\tvalid_0's ndcg@10: 0.99228\tvalid_0's ndcg@20: 0.99228\tvalid_0's ndcg@30: 0.99228\tvalid_0's ndcg@40: 0.99228\tvalid_0's ndcg@50: 0.99228\tvalid_0's ndcg@10: 0.99228\tvalid_0's ndcg@20: 0.99228\tvalid_0's ndcg@30: 0.99228\tvalid_0's ndcg@40: 0.99228\tvalid_0's ndcg@50: 0.99228\n","[31]\tvalid_0's ndcg@10: 0.992353\tvalid_0's ndcg@20: 0.992353\tvalid_0's ndcg@30: 0.992353\tvalid_0's ndcg@40: 0.992353\tvalid_0's ndcg@50: 0.992353\tvalid_0's ndcg@10: 0.992353\tvalid_0's ndcg@20: 0.992353\tvalid_0's ndcg@30: 0.992353\tvalid_0's ndcg@40: 0.992353\tvalid_0's ndcg@50: 0.992353\n","[32]\tvalid_0's ndcg@10: 0.992341\tvalid_0's ndcg@20: 0.992341\tvalid_0's ndcg@30: 0.992341\tvalid_0's ndcg@40: 0.992341\tvalid_0's ndcg@50: 0.992341\tvalid_0's ndcg@10: 0.992341\tvalid_0's ndcg@20: 0.992341\tvalid_0's ndcg@30: 0.992341\tvalid_0's ndcg@40: 0.992341\tvalid_0's ndcg@50: 0.992341\n","[33]\tvalid_0's ndcg@10: 0.992261\tvalid_0's ndcg@20: 0.992261\tvalid_0's ndcg@30: 0.992261\tvalid_0's ndcg@40: 0.992261\tvalid_0's ndcg@50: 0.992261\tvalid_0's ndcg@10: 0.992261\tvalid_0's ndcg@20: 0.992261\tvalid_0's ndcg@30: 0.992261\tvalid_0's ndcg@40: 0.992261\tvalid_0's ndcg@50: 0.992261\n","[34]\tvalid_0's ndcg@10: 0.992265\tvalid_0's ndcg@20: 0.992265\tvalid_0's ndcg@30: 0.992265\tvalid_0's ndcg@40: 0.992265\tvalid_0's ndcg@50: 0.992265\tvalid_0's ndcg@10: 0.992265\tvalid_0's ndcg@20: 0.992265\tvalid_0's ndcg@30: 0.992265\tvalid_0's ndcg@40: 0.992265\tvalid_0's ndcg@50: 0.992265\n","[35]\tvalid_0's ndcg@10: 0.992276\tvalid_0's ndcg@20: 0.992276\tvalid_0's ndcg@30: 0.992276\tvalid_0's ndcg@40: 0.992276\tvalid_0's ndcg@50: 0.992276\tvalid_0's ndcg@10: 0.992276\tvalid_0's ndcg@20: 0.992276\tvalid_0's ndcg@30: 0.992276\tvalid_0's ndcg@40: 0.992276\tvalid_0's ndcg@50: 0.992276\n","[36]\tvalid_0's ndcg@10: 0.992254\tvalid_0's ndcg@20: 0.992254\tvalid_0's ndcg@30: 0.992254\tvalid_0's ndcg@40: 0.992254\tvalid_0's ndcg@50: 0.992254\tvalid_0's ndcg@10: 0.992254\tvalid_0's ndcg@20: 0.992254\tvalid_0's ndcg@30: 0.992254\tvalid_0's ndcg@40: 0.992254\tvalid_0's ndcg@50: 0.992254\n","[37]\tvalid_0's ndcg@10: 0.992312\tvalid_0's ndcg@20: 0.992312\tvalid_0's ndcg@30: 0.992312\tvalid_0's ndcg@40: 0.992312\tvalid_0's ndcg@50: 0.992312\tvalid_0's ndcg@10: 0.992312\tvalid_0's ndcg@20: 0.992312\tvalid_0's ndcg@30: 0.992312\tvalid_0's ndcg@40: 0.992312\tvalid_0's ndcg@50: 0.992312\n","[38]\tvalid_0's ndcg@10: 0.992262\tvalid_0's ndcg@20: 0.992262\tvalid_0's ndcg@30: 0.992262\tvalid_0's ndcg@40: 0.992262\tvalid_0's ndcg@50: 0.992262\tvalid_0's ndcg@10: 0.992262\tvalid_0's ndcg@20: 0.992262\tvalid_0's ndcg@30: 0.992262\tvalid_0's ndcg@40: 0.992262\tvalid_0's ndcg@50: 0.992262\n","[39]\tvalid_0's ndcg@10: 0.992262\tvalid_0's ndcg@20: 0.992262\tvalid_0's ndcg@30: 0.992262\tvalid_0's ndcg@40: 0.992262\tvalid_0's ndcg@50: 0.992262\tvalid_0's ndcg@10: 0.992262\tvalid_0's ndcg@20: 0.992262\tvalid_0's ndcg@30: 0.992262\tvalid_0's ndcg@40: 0.992262\tvalid_0's ndcg@50: 0.992262\n","[40]\tvalid_0's ndcg@10: 0.992306\tvalid_0's ndcg@20: 0.992306\tvalid_0's ndcg@30: 0.992306\tvalid_0's ndcg@40: 0.992306\tvalid_0's ndcg@50: 0.992306\tvalid_0's ndcg@10: 0.992306\tvalid_0's ndcg@20: 0.992306\tvalid_0's ndcg@30: 0.992306\tvalid_0's ndcg@40: 0.992306\tvalid_0's ndcg@50: 0.992306\n","[41]\tvalid_0's ndcg@10: 0.992203\tvalid_0's ndcg@20: 0.992203\tvalid_0's ndcg@30: 0.992203\tvalid_0's ndcg@40: 0.992203\tvalid_0's ndcg@50: 0.992203\tvalid_0's ndcg@10: 0.992203\tvalid_0's ndcg@20: 0.992203\tvalid_0's ndcg@30: 0.992203\tvalid_0's ndcg@40: 0.992203\tvalid_0's ndcg@50: 0.992203\n","[42]\tvalid_0's ndcg@10: 0.992346\tvalid_0's ndcg@20: 0.992346\tvalid_0's ndcg@30: 0.992346\tvalid_0's ndcg@40: 0.992346\tvalid_0's ndcg@50: 0.992346\tvalid_0's ndcg@10: 0.992346\tvalid_0's ndcg@20: 0.992346\tvalid_0's ndcg@30: 0.992346\tvalid_0's ndcg@40: 0.992346\tvalid_0's ndcg@50: 0.992346\n","[43]\tvalid_0's ndcg@10: 0.992271\tvalid_0's ndcg@20: 0.992271\tvalid_0's ndcg@30: 0.992271\tvalid_0's ndcg@40: 0.992271\tvalid_0's ndcg@50: 0.992271\tvalid_0's ndcg@10: 0.992271\tvalid_0's ndcg@20: 0.992271\tvalid_0's ndcg@30: 0.992271\tvalid_0's ndcg@40: 0.992271\tvalid_0's ndcg@50: 0.992271\n","[44]\tvalid_0's ndcg@10: 0.992317\tvalid_0's ndcg@20: 0.992317\tvalid_0's ndcg@30: 0.992317\tvalid_0's ndcg@40: 0.992317\tvalid_0's ndcg@50: 0.992317\tvalid_0's ndcg@10: 0.992317\tvalid_0's ndcg@20: 0.992317\tvalid_0's ndcg@30: 0.992317\tvalid_0's ndcg@40: 0.992317\tvalid_0's ndcg@50: 0.992317\n","[45]\tvalid_0's ndcg@10: 0.992339\tvalid_0's ndcg@20: 0.992339\tvalid_0's ndcg@30: 0.992339\tvalid_0's ndcg@40: 0.992339\tvalid_0's ndcg@50: 0.992339\tvalid_0's ndcg@10: 0.992339\tvalid_0's ndcg@20: 0.992339\tvalid_0's ndcg@30: 0.992339\tvalid_0's ndcg@40: 0.992339\tvalid_0's ndcg@50: 0.992339\n","[46]\tvalid_0's ndcg@10: 0.992276\tvalid_0's ndcg@20: 0.992276\tvalid_0's ndcg@30: 0.992276\tvalid_0's ndcg@40: 0.992276\tvalid_0's ndcg@50: 0.992276\tvalid_0's ndcg@10: 0.992276\tvalid_0's ndcg@20: 0.992276\tvalid_0's ndcg@30: 0.992276\tvalid_0's ndcg@40: 0.992276\tvalid_0's ndcg@50: 0.992276\n","[47]\tvalid_0's ndcg@10: 0.992321\tvalid_0's ndcg@20: 0.992321\tvalid_0's ndcg@30: 0.992321\tvalid_0's ndcg@40: 0.992321\tvalid_0's ndcg@50: 0.992321\tvalid_0's ndcg@10: 0.992321\tvalid_0's ndcg@20: 0.992321\tvalid_0's ndcg@30: 0.992321\tvalid_0's ndcg@40: 0.992321\tvalid_0's ndcg@50: 0.992321\n","[48]\tvalid_0's ndcg@10: 0.992224\tvalid_0's ndcg@20: 0.992224\tvalid_0's ndcg@30: 0.992224\tvalid_0's ndcg@40: 0.992224\tvalid_0's ndcg@50: 0.992224\tvalid_0's ndcg@10: 0.992224\tvalid_0's ndcg@20: 0.992224\tvalid_0's ndcg@30: 0.992224\tvalid_0's ndcg@40: 0.992224\tvalid_0's ndcg@50: 0.992224\n","[49]\tvalid_0's ndcg@10: 0.992246\tvalid_0's ndcg@20: 0.992246\tvalid_0's ndcg@30: 0.992246\tvalid_0's ndcg@40: 0.992246\tvalid_0's ndcg@50: 0.992246\tvalid_0's ndcg@10: 0.992246\tvalid_0's ndcg@20: 0.992246\tvalid_0's ndcg@30: 0.992246\tvalid_0's ndcg@40: 0.992246\tvalid_0's ndcg@50: 0.992246\n","[50]\tvalid_0's ndcg@10: 0.992361\tvalid_0's ndcg@20: 0.992361\tvalid_0's ndcg@30: 0.992361\tvalid_0's ndcg@40: 0.992361\tvalid_0's ndcg@50: 0.992361\tvalid_0's ndcg@10: 0.992361\tvalid_0's ndcg@20: 0.992361\tvalid_0's ndcg@30: 0.992361\tvalid_0's ndcg@40: 0.992361\tvalid_0's ndcg@50: 0.992361\n","[51]\tvalid_0's ndcg@10: 0.992269\tvalid_0's ndcg@20: 0.992269\tvalid_0's ndcg@30: 0.992269\tvalid_0's ndcg@40: 0.992269\tvalid_0's ndcg@50: 0.992269\tvalid_0's ndcg@10: 0.992269\tvalid_0's ndcg@20: 0.992269\tvalid_0's ndcg@30: 0.992269\tvalid_0's ndcg@40: 0.992269\tvalid_0's ndcg@50: 0.992269\n","[52]\tvalid_0's ndcg@10: 0.992279\tvalid_0's ndcg@20: 0.992279\tvalid_0's ndcg@30: 0.992279\tvalid_0's ndcg@40: 0.992279\tvalid_0's ndcg@50: 0.992279\tvalid_0's ndcg@10: 0.992279\tvalid_0's ndcg@20: 0.992279\tvalid_0's ndcg@30: 0.992279\tvalid_0's ndcg@40: 0.992279\tvalid_0's ndcg@50: 0.992279\n","[53]\tvalid_0's ndcg@10: 0.992223\tvalid_0's ndcg@20: 0.992223\tvalid_0's ndcg@30: 0.992223\tvalid_0's ndcg@40: 0.992223\tvalid_0's ndcg@50: 0.992223\tvalid_0's ndcg@10: 0.992223\tvalid_0's ndcg@20: 0.992223\tvalid_0's ndcg@30: 0.992223\tvalid_0's ndcg@40: 0.992223\tvalid_0's ndcg@50: 0.992223\n","[54]\tvalid_0's ndcg@10: 0.992288\tvalid_0's ndcg@20: 0.992288\tvalid_0's ndcg@30: 0.992288\tvalid_0's ndcg@40: 0.992288\tvalid_0's ndcg@50: 0.992288\tvalid_0's ndcg@10: 0.992288\tvalid_0's ndcg@20: 0.992288\tvalid_0's ndcg@30: 0.992288\tvalid_0's ndcg@40: 0.992288\tvalid_0's ndcg@50: 0.992288\n","[55]\tvalid_0's ndcg@10: 0.992307\tvalid_0's ndcg@20: 0.992307\tvalid_0's ndcg@30: 0.992307\tvalid_0's ndcg@40: 0.992307\tvalid_0's ndcg@50: 0.992307\tvalid_0's ndcg@10: 0.992307\tvalid_0's ndcg@20: 0.992307\tvalid_0's ndcg@30: 0.992307\tvalid_0's ndcg@40: 0.992307\tvalid_0's ndcg@50: 0.992307\n","[56]\tvalid_0's ndcg@10: 0.992307\tvalid_0's ndcg@20: 0.992307\tvalid_0's ndcg@30: 0.992307\tvalid_0's ndcg@40: 0.992307\tvalid_0's ndcg@50: 0.992307\tvalid_0's ndcg@10: 0.992307\tvalid_0's ndcg@20: 0.992307\tvalid_0's ndcg@30: 0.992307\tvalid_0's ndcg@40: 0.992307\tvalid_0's ndcg@50: 0.992307\n","[57]\tvalid_0's ndcg@10: 0.992297\tvalid_0's ndcg@20: 0.992297\tvalid_0's ndcg@30: 0.992297\tvalid_0's ndcg@40: 0.992297\tvalid_0's ndcg@50: 0.992297\tvalid_0's ndcg@10: 0.992297\tvalid_0's ndcg@20: 0.992297\tvalid_0's ndcg@30: 0.992297\tvalid_0's ndcg@40: 0.992297\tvalid_0's ndcg@50: 0.992297\n","[58]\tvalid_0's ndcg@10: 0.992264\tvalid_0's ndcg@20: 0.992264\tvalid_0's ndcg@30: 0.992264\tvalid_0's ndcg@40: 0.992264\tvalid_0's ndcg@50: 0.992264\tvalid_0's ndcg@10: 0.992264\tvalid_0's ndcg@20: 0.992264\tvalid_0's ndcg@30: 0.992264\tvalid_0's ndcg@40: 0.992264\tvalid_0's ndcg@50: 0.992264\n","[59]\tvalid_0's ndcg@10: 0.992293\tvalid_0's ndcg@20: 0.992293\tvalid_0's ndcg@30: 0.992293\tvalid_0's ndcg@40: 0.992293\tvalid_0's ndcg@50: 0.992293\tvalid_0's ndcg@10: 0.992293\tvalid_0's ndcg@20: 0.992293\tvalid_0's ndcg@30: 0.992293\tvalid_0's ndcg@40: 0.992293\tvalid_0's ndcg@50: 0.992293\n","[60]\tvalid_0's ndcg@10: 0.992303\tvalid_0's ndcg@20: 0.992303\tvalid_0's ndcg@30: 0.992303\tvalid_0's ndcg@40: 0.992303\tvalid_0's ndcg@50: 0.992303\tvalid_0's ndcg@10: 0.992303\tvalid_0's ndcg@20: 0.992303\tvalid_0's ndcg@30: 0.992303\tvalid_0's ndcg@40: 0.992303\tvalid_0's ndcg@50: 0.992303\n","[61]\tvalid_0's ndcg@10: 0.992291\tvalid_0's ndcg@20: 0.992291\tvalid_0's ndcg@30: 0.992291\tvalid_0's ndcg@40: 0.992291\tvalid_0's ndcg@50: 0.992291\tvalid_0's ndcg@10: 0.992291\tvalid_0's ndcg@20: 0.992291\tvalid_0's ndcg@30: 0.992291\tvalid_0's ndcg@40: 0.992291\tvalid_0's ndcg@50: 0.992291\n","[62]\tvalid_0's ndcg@10: 0.992301\tvalid_0's ndcg@20: 0.992301\tvalid_0's ndcg@30: 0.992301\tvalid_0's ndcg@40: 0.992301\tvalid_0's ndcg@50: 0.992301\tvalid_0's ndcg@10: 0.992301\tvalid_0's ndcg@20: 0.992301\tvalid_0's ndcg@30: 0.992301\tvalid_0's ndcg@40: 0.992301\tvalid_0's ndcg@50: 0.992301\n","[63]\tvalid_0's ndcg@10: 0.992282\tvalid_0's ndcg@20: 0.992282\tvalid_0's ndcg@30: 0.992282\tvalid_0's ndcg@40: 0.992282\tvalid_0's ndcg@50: 0.992282\tvalid_0's ndcg@10: 0.992282\tvalid_0's ndcg@20: 0.992282\tvalid_0's ndcg@30: 0.992282\tvalid_0's ndcg@40: 0.992282\tvalid_0's ndcg@50: 0.992282\n","[64]\tvalid_0's ndcg@10: 0.992227\tvalid_0's ndcg@20: 0.992227\tvalid_0's ndcg@30: 0.992227\tvalid_0's ndcg@40: 0.992227\tvalid_0's ndcg@50: 0.992227\tvalid_0's ndcg@10: 0.992227\tvalid_0's ndcg@20: 0.992227\tvalid_0's ndcg@30: 0.992227\tvalid_0's ndcg@40: 0.992227\tvalid_0's ndcg@50: 0.992227\n","[65]\tvalid_0's ndcg@10: 0.992193\tvalid_0's ndcg@20: 0.992193\tvalid_0's ndcg@30: 0.992193\tvalid_0's ndcg@40: 0.992193\tvalid_0's ndcg@50: 0.992193\tvalid_0's ndcg@10: 0.992193\tvalid_0's ndcg@20: 0.992193\tvalid_0's ndcg@30: 0.992193\tvalid_0's ndcg@40: 0.992193\tvalid_0's ndcg@50: 0.992193\n","[66]\tvalid_0's ndcg@10: 0.992149\tvalid_0's ndcg@20: 0.992149\tvalid_0's ndcg@30: 0.992149\tvalid_0's ndcg@40: 0.992149\tvalid_0's ndcg@50: 0.992149\tvalid_0's ndcg@10: 0.992149\tvalid_0's ndcg@20: 0.992149\tvalid_0's ndcg@30: 0.992149\tvalid_0's ndcg@40: 0.992149\tvalid_0's ndcg@50: 0.992149\n","[67]\tvalid_0's ndcg@10: 0.992186\tvalid_0's ndcg@20: 0.992186\tvalid_0's ndcg@30: 0.992186\tvalid_0's ndcg@40: 0.992186\tvalid_0's ndcg@50: 0.992186\tvalid_0's ndcg@10: 0.992186\tvalid_0's ndcg@20: 0.992186\tvalid_0's ndcg@30: 0.992186\tvalid_0's ndcg@40: 0.992186\tvalid_0's ndcg@50: 0.992186\n","[68]\tvalid_0's ndcg@10: 0.992214\tvalid_0's ndcg@20: 0.992214\tvalid_0's ndcg@30: 0.992214\tvalid_0's ndcg@40: 0.992214\tvalid_0's ndcg@50: 0.992214\tvalid_0's ndcg@10: 0.992214\tvalid_0's ndcg@20: 0.992214\tvalid_0's ndcg@30: 0.992214\tvalid_0's ndcg@40: 0.992214\tvalid_0's ndcg@50: 0.992214\n","[69]\tvalid_0's ndcg@10: 0.992357\tvalid_0's ndcg@20: 0.992357\tvalid_0's ndcg@30: 0.992357\tvalid_0's ndcg@40: 0.992357\tvalid_0's ndcg@50: 0.992357\tvalid_0's ndcg@10: 0.992357\tvalid_0's ndcg@20: 0.992357\tvalid_0's ndcg@30: 0.992357\tvalid_0's ndcg@40: 0.992357\tvalid_0's ndcg@50: 0.992357\n","[70]\tvalid_0's ndcg@10: 0.992152\tvalid_0's ndcg@20: 0.992152\tvalid_0's ndcg@30: 0.992152\tvalid_0's ndcg@40: 0.992152\tvalid_0's ndcg@50: 0.992152\tvalid_0's ndcg@10: 0.992152\tvalid_0's ndcg@20: 0.992152\tvalid_0's ndcg@30: 0.992152\tvalid_0's ndcg@40: 0.992152\tvalid_0's ndcg@50: 0.992152\n","[71]\tvalid_0's ndcg@10: 0.992249\tvalid_0's ndcg@20: 0.992249\tvalid_0's ndcg@30: 0.992249\tvalid_0's ndcg@40: 0.992249\tvalid_0's ndcg@50: 0.992249\tvalid_0's ndcg@10: 0.992249\tvalid_0's ndcg@20: 0.992249\tvalid_0's ndcg@30: 0.992249\tvalid_0's ndcg@40: 0.992249\tvalid_0's ndcg@50: 0.992249\n","[72]\tvalid_0's ndcg@10: 0.992282\tvalid_0's ndcg@20: 0.992282\tvalid_0's ndcg@30: 0.992282\tvalid_0's ndcg@40: 0.992282\tvalid_0's ndcg@50: 0.992282\tvalid_0's ndcg@10: 0.992282\tvalid_0's ndcg@20: 0.992282\tvalid_0's ndcg@30: 0.992282\tvalid_0's ndcg@40: 0.992282\tvalid_0's ndcg@50: 0.992282\n","[73]\tvalid_0's ndcg@10: 0.992156\tvalid_0's ndcg@20: 0.992156\tvalid_0's ndcg@30: 0.992156\tvalid_0's ndcg@40: 0.992156\tvalid_0's ndcg@50: 0.992156\tvalid_0's ndcg@10: 0.992156\tvalid_0's ndcg@20: 0.992156\tvalid_0's ndcg@30: 0.992156\tvalid_0's ndcg@40: 0.992156\tvalid_0's ndcg@50: 0.992156\n","[74]\tvalid_0's ndcg@10: 0.992083\tvalid_0's ndcg@20: 0.992083\tvalid_0's ndcg@30: 0.992083\tvalid_0's ndcg@40: 0.992083\tvalid_0's ndcg@50: 0.992083\tvalid_0's ndcg@10: 0.992083\tvalid_0's ndcg@20: 0.992083\tvalid_0's ndcg@30: 0.992083\tvalid_0's ndcg@40: 0.992083\tvalid_0's ndcg@50: 0.992083\n","[75]\tvalid_0's ndcg@10: 0.992202\tvalid_0's ndcg@20: 0.992202\tvalid_0's ndcg@30: 0.992202\tvalid_0's ndcg@40: 0.992202\tvalid_0's ndcg@50: 0.992202\tvalid_0's ndcg@10: 0.992202\tvalid_0's ndcg@20: 0.992202\tvalid_0's ndcg@30: 0.992202\tvalid_0's ndcg@40: 0.992202\tvalid_0's ndcg@50: 0.992202\n","[76]\tvalid_0's ndcg@10: 0.992372\tvalid_0's ndcg@20: 0.992372\tvalid_0's ndcg@30: 0.992372\tvalid_0's ndcg@40: 0.992372\tvalid_0's ndcg@50: 0.992372\tvalid_0's ndcg@10: 0.992372\tvalid_0's ndcg@20: 0.992372\tvalid_0's ndcg@30: 0.992372\tvalid_0's ndcg@40: 0.992372\tvalid_0's ndcg@50: 0.992372\n","[77]\tvalid_0's ndcg@10: 0.992184\tvalid_0's ndcg@20: 0.992184\tvalid_0's ndcg@30: 0.992184\tvalid_0's ndcg@40: 0.992184\tvalid_0's ndcg@50: 0.992184\tvalid_0's ndcg@10: 0.992184\tvalid_0's ndcg@20: 0.992184\tvalid_0's ndcg@30: 0.992184\tvalid_0's ndcg@40: 0.992184\tvalid_0's ndcg@50: 0.992184\n","[78]\tvalid_0's ndcg@10: 0.992246\tvalid_0's ndcg@20: 0.992246\tvalid_0's ndcg@30: 0.992246\tvalid_0's ndcg@40: 0.992246\tvalid_0's ndcg@50: 0.992246\tvalid_0's ndcg@10: 0.992246\tvalid_0's ndcg@20: 0.992246\tvalid_0's ndcg@30: 0.992246\tvalid_0's ndcg@40: 0.992246\tvalid_0's ndcg@50: 0.992246\n","[79]\tvalid_0's ndcg@10: 0.992327\tvalid_0's ndcg@20: 0.992327\tvalid_0's ndcg@30: 0.992327\tvalid_0's ndcg@40: 0.992327\tvalid_0's ndcg@50: 0.992327\tvalid_0's ndcg@10: 0.992327\tvalid_0's ndcg@20: 0.992327\tvalid_0's ndcg@30: 0.992327\tvalid_0's ndcg@40: 0.992327\tvalid_0's ndcg@50: 0.992327\n","[80]\tvalid_0's ndcg@10: 0.99217\tvalid_0's ndcg@20: 0.99217\tvalid_0's ndcg@30: 0.99217\tvalid_0's ndcg@40: 0.99217\tvalid_0's ndcg@50: 0.99217\tvalid_0's ndcg@10: 0.99217\tvalid_0's ndcg@20: 0.99217\tvalid_0's ndcg@30: 0.99217\tvalid_0's ndcg@40: 0.99217\tvalid_0's ndcg@50: 0.99217\n","[81]\tvalid_0's ndcg@10: 0.992111\tvalid_0's ndcg@20: 0.992111\tvalid_0's ndcg@30: 0.992111\tvalid_0's ndcg@40: 0.992111\tvalid_0's ndcg@50: 0.992111\tvalid_0's ndcg@10: 0.992111\tvalid_0's ndcg@20: 0.992111\tvalid_0's ndcg@30: 0.992111\tvalid_0's ndcg@40: 0.992111\tvalid_0's ndcg@50: 0.992111\n","[82]\tvalid_0's ndcg@10: 0.9921\tvalid_0's ndcg@20: 0.9921\tvalid_0's ndcg@30: 0.9921\tvalid_0's ndcg@40: 0.9921\tvalid_0's ndcg@50: 0.9921\tvalid_0's ndcg@10: 0.9921\tvalid_0's ndcg@20: 0.9921\tvalid_0's ndcg@30: 0.9921\tvalid_0's ndcg@40: 0.9921\tvalid_0's ndcg@50: 0.9921\n","[83]\tvalid_0's ndcg@10: 0.992075\tvalid_0's ndcg@20: 0.992075\tvalid_0's ndcg@30: 0.992075\tvalid_0's ndcg@40: 0.992075\tvalid_0's ndcg@50: 0.992075\tvalid_0's ndcg@10: 0.992075\tvalid_0's ndcg@20: 0.992075\tvalid_0's ndcg@30: 0.992075\tvalid_0's ndcg@40: 0.992075\tvalid_0's ndcg@50: 0.992075\n","[84]\tvalid_0's ndcg@10: 0.992058\tvalid_0's ndcg@20: 0.992058\tvalid_0's ndcg@30: 0.992058\tvalid_0's ndcg@40: 0.992058\tvalid_0's ndcg@50: 0.992058\tvalid_0's ndcg@10: 0.992058\tvalid_0's ndcg@20: 0.992058\tvalid_0's ndcg@30: 0.992058\tvalid_0's ndcg@40: 0.992058\tvalid_0's ndcg@50: 0.992058\n","[85]\tvalid_0's ndcg@10: 0.992143\tvalid_0's ndcg@20: 0.992143\tvalid_0's ndcg@30: 0.992143\tvalid_0's ndcg@40: 0.992143\tvalid_0's ndcg@50: 0.992143\tvalid_0's ndcg@10: 0.992143\tvalid_0's ndcg@20: 0.992143\tvalid_0's ndcg@30: 0.992143\tvalid_0's ndcg@40: 0.992143\tvalid_0's ndcg@50: 0.992143\n","[86]\tvalid_0's ndcg@10: 0.992151\tvalid_0's ndcg@20: 0.992151\tvalid_0's ndcg@30: 0.992151\tvalid_0's ndcg@40: 0.992151\tvalid_0's ndcg@50: 0.992151\tvalid_0's ndcg@10: 0.992151\tvalid_0's ndcg@20: 0.992151\tvalid_0's ndcg@30: 0.992151\tvalid_0's ndcg@40: 0.992151\tvalid_0's ndcg@50: 0.992151\n","[87]\tvalid_0's ndcg@10: 0.992036\tvalid_0's ndcg@20: 0.992036\tvalid_0's ndcg@30: 0.992036\tvalid_0's ndcg@40: 0.992036\tvalid_0's ndcg@50: 0.992036\tvalid_0's ndcg@10: 0.992036\tvalid_0's ndcg@20: 0.992036\tvalid_0's ndcg@30: 0.992036\tvalid_0's ndcg@40: 0.992036\tvalid_0's ndcg@50: 0.992036\n","[88]\tvalid_0's ndcg@10: 0.99211\tvalid_0's ndcg@20: 0.99211\tvalid_0's ndcg@30: 0.99211\tvalid_0's ndcg@40: 0.99211\tvalid_0's ndcg@50: 0.99211\tvalid_0's ndcg@10: 0.99211\tvalid_0's ndcg@20: 0.99211\tvalid_0's ndcg@30: 0.99211\tvalid_0's ndcg@40: 0.99211\tvalid_0's ndcg@50: 0.99211\n","[89]\tvalid_0's ndcg@10: 0.992037\tvalid_0's ndcg@20: 0.992037\tvalid_0's ndcg@30: 0.992037\tvalid_0's ndcg@40: 0.992037\tvalid_0's ndcg@50: 0.992037\tvalid_0's ndcg@10: 0.992037\tvalid_0's ndcg@20: 0.992037\tvalid_0's ndcg@30: 0.992037\tvalid_0's ndcg@40: 0.992037\tvalid_0's ndcg@50: 0.992037\n","[90]\tvalid_0's ndcg@10: 0.992049\tvalid_0's ndcg@20: 0.992049\tvalid_0's ndcg@30: 0.992049\tvalid_0's ndcg@40: 0.992049\tvalid_0's ndcg@50: 0.992049\tvalid_0's ndcg@10: 0.992049\tvalid_0's ndcg@20: 0.992049\tvalid_0's ndcg@30: 0.992049\tvalid_0's ndcg@40: 0.992049\tvalid_0's ndcg@50: 0.992049\n","[91]\tvalid_0's ndcg@10: 0.992112\tvalid_0's ndcg@20: 0.992112\tvalid_0's ndcg@30: 0.992112\tvalid_0's ndcg@40: 0.992112\tvalid_0's ndcg@50: 0.992112\tvalid_0's ndcg@10: 0.992112\tvalid_0's ndcg@20: 0.992112\tvalid_0's ndcg@30: 0.992112\tvalid_0's ndcg@40: 0.992112\tvalid_0's ndcg@50: 0.992112\n","[92]\tvalid_0's ndcg@10: 0.992237\tvalid_0's ndcg@20: 0.992237\tvalid_0's ndcg@30: 0.992237\tvalid_0's ndcg@40: 0.992237\tvalid_0's ndcg@50: 0.992237\tvalid_0's ndcg@10: 0.992237\tvalid_0's ndcg@20: 0.992237\tvalid_0's ndcg@30: 0.992237\tvalid_0's ndcg@40: 0.992237\tvalid_0's ndcg@50: 0.992237\n","[93]\tvalid_0's ndcg@10: 0.992259\tvalid_0's ndcg@20: 0.992259\tvalid_0's ndcg@30: 0.992259\tvalid_0's ndcg@40: 0.992259\tvalid_0's ndcg@50: 0.992259\tvalid_0's ndcg@10: 0.992259\tvalid_0's ndcg@20: 0.992259\tvalid_0's ndcg@30: 0.992259\tvalid_0's ndcg@40: 0.992259\tvalid_0's ndcg@50: 0.992259\n","[94]\tvalid_0's ndcg@10: 0.992213\tvalid_0's ndcg@20: 0.992213\tvalid_0's ndcg@30: 0.992213\tvalid_0's ndcg@40: 0.992213\tvalid_0's ndcg@50: 0.992213\tvalid_0's ndcg@10: 0.992213\tvalid_0's ndcg@20: 0.992213\tvalid_0's ndcg@30: 0.992213\tvalid_0's ndcg@40: 0.992213\tvalid_0's ndcg@50: 0.992213\n","[95]\tvalid_0's ndcg@10: 0.992179\tvalid_0's ndcg@20: 0.992179\tvalid_0's ndcg@30: 0.992179\tvalid_0's ndcg@40: 0.992179\tvalid_0's ndcg@50: 0.992179\tvalid_0's ndcg@10: 0.992179\tvalid_0's ndcg@20: 0.992179\tvalid_0's ndcg@30: 0.992179\tvalid_0's ndcg@40: 0.992179\tvalid_0's ndcg@50: 0.992179\n","[96]\tvalid_0's ndcg@10: 0.992237\tvalid_0's ndcg@20: 0.992237\tvalid_0's ndcg@30: 0.992237\tvalid_0's ndcg@40: 0.992237\tvalid_0's ndcg@50: 0.992237\tvalid_0's ndcg@10: 0.992237\tvalid_0's ndcg@20: 0.992237\tvalid_0's ndcg@30: 0.992237\tvalid_0's ndcg@40: 0.992237\tvalid_0's ndcg@50: 0.992237\n","[97]\tvalid_0's ndcg@10: 0.992237\tvalid_0's ndcg@20: 0.992237\tvalid_0's ndcg@30: 0.992237\tvalid_0's ndcg@40: 0.992237\tvalid_0's ndcg@50: 0.992237\tvalid_0's ndcg@10: 0.992237\tvalid_0's ndcg@20: 0.992237\tvalid_0's ndcg@30: 0.992237\tvalid_0's ndcg@40: 0.992237\tvalid_0's ndcg@50: 0.992237\n","[98]\tvalid_0's ndcg@10: 0.992301\tvalid_0's ndcg@20: 0.992301\tvalid_0's ndcg@30: 0.992301\tvalid_0's ndcg@40: 0.992301\tvalid_0's ndcg@50: 0.992301\tvalid_0's ndcg@10: 0.992301\tvalid_0's ndcg@20: 0.992301\tvalid_0's ndcg@30: 0.992301\tvalid_0's ndcg@40: 0.992301\tvalid_0's ndcg@50: 0.992301\n","[99]\tvalid_0's ndcg@10: 0.992356\tvalid_0's ndcg@20: 0.992356\tvalid_0's ndcg@30: 0.992356\tvalid_0's ndcg@40: 0.992356\tvalid_0's ndcg@50: 0.992356\tvalid_0's ndcg@10: 0.992356\tvalid_0's ndcg@20: 0.992356\tvalid_0's ndcg@30: 0.992356\tvalid_0's ndcg@40: 0.992356\tvalid_0's ndcg@50: 0.992356\n","[100]\tvalid_0's ndcg@10: 0.992348\tvalid_0's ndcg@20: 0.992348\tvalid_0's ndcg@30: 0.992348\tvalid_0's ndcg@40: 0.992348\tvalid_0's ndcg@50: 0.992348\tvalid_0's ndcg@10: 0.992348\tvalid_0's ndcg@20: 0.992348\tvalid_0's ndcg@30: 0.992348\tvalid_0's ndcg@40: 0.992348\tvalid_0's ndcg@50: 0.992348\n","Did not meet early stopping. Best iteration is:\n","[76]\tvalid_0's ndcg@10: 0.992372\tvalid_0's ndcg@20: 0.992372\tvalid_0's ndcg@30: 0.992372\tvalid_0's ndcg@40: 0.992372\tvalid_0's ndcg@50: 0.992372\tvalid_0's ndcg@10: 0.992372\tvalid_0's ndcg@20: 0.992372\tvalid_0's ndcg@30: 0.992372\tvalid_0's ndcg@40: 0.992372\tvalid_0's ndcg@50: 0.992372\n","[1]\tvalid_0's ndcg@10: 0.989609\tvalid_0's ndcg@20: 0.9897\tvalid_0's ndcg@30: 0.9897\tvalid_0's ndcg@40: 0.9897\tvalid_0's ndcg@50: 0.9897\tvalid_0's ndcg@10: 0.989609\tvalid_0's ndcg@20: 0.9897\tvalid_0's ndcg@30: 0.9897\tvalid_0's ndcg@40: 0.9897\tvalid_0's ndcg@50: 0.9897\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.989646\tvalid_0's ndcg@20: 0.989737\tvalid_0's ndcg@30: 0.989737\tvalid_0's ndcg@40: 0.989737\tvalid_0's ndcg@50: 0.989737\tvalid_0's ndcg@10: 0.989646\tvalid_0's ndcg@20: 0.989737\tvalid_0's ndcg@30: 0.989737\tvalid_0's ndcg@40: 0.989737\tvalid_0's ndcg@50: 0.989737\n","[3]\tvalid_0's ndcg@10: 0.991065\tvalid_0's ndcg@20: 0.991155\tvalid_0's ndcg@30: 0.991155\tvalid_0's ndcg@40: 0.991155\tvalid_0's ndcg@50: 0.991155\tvalid_0's ndcg@10: 0.991065\tvalid_0's ndcg@20: 0.991155\tvalid_0's ndcg@30: 0.991155\tvalid_0's ndcg@40: 0.991155\tvalid_0's ndcg@50: 0.991155\n","[4]\tvalid_0's ndcg@10: 0.991545\tvalid_0's ndcg@20: 0.991679\tvalid_0's ndcg@30: 0.991679\tvalid_0's ndcg@40: 0.991679\tvalid_0's ndcg@50: 0.991679\tvalid_0's ndcg@10: 0.991545\tvalid_0's ndcg@20: 0.991679\tvalid_0's ndcg@30: 0.991679\tvalid_0's ndcg@40: 0.991679\tvalid_0's ndcg@50: 0.991679\n","[5]\tvalid_0's ndcg@10: 0.991719\tvalid_0's ndcg@20: 0.991808\tvalid_0's ndcg@30: 0.991808\tvalid_0's ndcg@40: 0.991808\tvalid_0's ndcg@50: 0.991808\tvalid_0's ndcg@10: 0.991719\tvalid_0's ndcg@20: 0.991808\tvalid_0's ndcg@30: 0.991808\tvalid_0's ndcg@40: 0.991808\tvalid_0's ndcg@50: 0.991808\n","[6]\tvalid_0's ndcg@10: 0.99158\tvalid_0's ndcg@20: 0.991671\tvalid_0's ndcg@30: 0.991671\tvalid_0's ndcg@40: 0.991671\tvalid_0's ndcg@50: 0.991671\tvalid_0's ndcg@10: 0.99158\tvalid_0's ndcg@20: 0.991671\tvalid_0's ndcg@30: 0.991671\tvalid_0's ndcg@40: 0.991671\tvalid_0's ndcg@50: 0.991671\n","[7]\tvalid_0's ndcg@10: 0.991871\tvalid_0's ndcg@20: 0.991961\tvalid_0's ndcg@30: 0.991961\tvalid_0's ndcg@40: 0.991961\tvalid_0's ndcg@50: 0.991961\tvalid_0's ndcg@10: 0.991871\tvalid_0's ndcg@20: 0.991961\tvalid_0's ndcg@30: 0.991961\tvalid_0's ndcg@40: 0.991961\tvalid_0's ndcg@50: 0.991961\n","[8]\tvalid_0's ndcg@10: 0.991912\tvalid_0's ndcg@20: 0.992002\tvalid_0's ndcg@30: 0.992002\tvalid_0's ndcg@40: 0.992002\tvalid_0's ndcg@50: 0.992002\tvalid_0's ndcg@10: 0.991912\tvalid_0's ndcg@20: 0.992002\tvalid_0's ndcg@30: 0.992002\tvalid_0's ndcg@40: 0.992002\tvalid_0's ndcg@50: 0.992002\n","[9]\tvalid_0's ndcg@10: 0.991986\tvalid_0's ndcg@20: 0.992076\tvalid_0's ndcg@30: 0.992076\tvalid_0's ndcg@40: 0.992076\tvalid_0's ndcg@50: 0.992076\tvalid_0's ndcg@10: 0.991986\tvalid_0's ndcg@20: 0.992076\tvalid_0's ndcg@30: 0.992076\tvalid_0's ndcg@40: 0.992076\tvalid_0's ndcg@50: 0.992076\n","[10]\tvalid_0's ndcg@10: 0.992282\tvalid_0's ndcg@20: 0.992372\tvalid_0's ndcg@30: 0.992372\tvalid_0's ndcg@40: 0.992372\tvalid_0's ndcg@50: 0.992372\tvalid_0's ndcg@10: 0.992282\tvalid_0's ndcg@20: 0.992372\tvalid_0's ndcg@30: 0.992372\tvalid_0's ndcg@40: 0.992372\tvalid_0's ndcg@50: 0.992372\n","[11]\tvalid_0's ndcg@10: 0.992392\tvalid_0's ndcg@20: 0.992481\tvalid_0's ndcg@30: 0.992481\tvalid_0's ndcg@40: 0.992481\tvalid_0's ndcg@50: 0.992481\tvalid_0's ndcg@10: 0.992392\tvalid_0's ndcg@20: 0.992481\tvalid_0's ndcg@30: 0.992481\tvalid_0's ndcg@40: 0.992481\tvalid_0's ndcg@50: 0.992481\n","[12]\tvalid_0's ndcg@10: 0.992388\tvalid_0's ndcg@20: 0.992478\tvalid_0's ndcg@30: 0.992478\tvalid_0's ndcg@40: 0.992478\tvalid_0's ndcg@50: 0.992478\tvalid_0's ndcg@10: 0.992388\tvalid_0's ndcg@20: 0.992478\tvalid_0's ndcg@30: 0.992478\tvalid_0's ndcg@40: 0.992478\tvalid_0's ndcg@50: 0.992478\n","[13]\tvalid_0's ndcg@10: 0.992402\tvalid_0's ndcg@20: 0.99249\tvalid_0's ndcg@30: 0.99249\tvalid_0's ndcg@40: 0.99249\tvalid_0's ndcg@50: 0.99249\tvalid_0's ndcg@10: 0.992402\tvalid_0's ndcg@20: 0.99249\tvalid_0's ndcg@30: 0.99249\tvalid_0's ndcg@40: 0.99249\tvalid_0's ndcg@50: 0.99249\n","[14]\tvalid_0's ndcg@10: 0.992458\tvalid_0's ndcg@20: 0.992547\tvalid_0's ndcg@30: 0.992547\tvalid_0's ndcg@40: 0.992547\tvalid_0's ndcg@50: 0.992547\tvalid_0's ndcg@10: 0.992458\tvalid_0's ndcg@20: 0.992547\tvalid_0's ndcg@30: 0.992547\tvalid_0's ndcg@40: 0.992547\tvalid_0's ndcg@50: 0.992547\n","[15]\tvalid_0's ndcg@10: 0.992343\tvalid_0's ndcg@20: 0.992431\tvalid_0's ndcg@30: 0.992431\tvalid_0's ndcg@40: 0.992431\tvalid_0's ndcg@50: 0.992431\tvalid_0's ndcg@10: 0.992343\tvalid_0's ndcg@20: 0.992431\tvalid_0's ndcg@30: 0.992431\tvalid_0's ndcg@40: 0.992431\tvalid_0's ndcg@50: 0.992431\n","[16]\tvalid_0's ndcg@10: 0.99214\tvalid_0's ndcg@20: 0.992228\tvalid_0's ndcg@30: 0.992228\tvalid_0's ndcg@40: 0.992228\tvalid_0's ndcg@50: 0.992228\tvalid_0's ndcg@10: 0.99214\tvalid_0's ndcg@20: 0.992228\tvalid_0's ndcg@30: 0.992228\tvalid_0's ndcg@40: 0.992228\tvalid_0's ndcg@50: 0.992228\n","[17]\tvalid_0's ndcg@10: 0.992204\tvalid_0's ndcg@20: 0.992292\tvalid_0's ndcg@30: 0.992292\tvalid_0's ndcg@40: 0.992292\tvalid_0's ndcg@50: 0.992292\tvalid_0's ndcg@10: 0.992204\tvalid_0's ndcg@20: 0.992292\tvalid_0's ndcg@30: 0.992292\tvalid_0's ndcg@40: 0.992292\tvalid_0's ndcg@50: 0.992292\n","[18]\tvalid_0's ndcg@10: 0.992369\tvalid_0's ndcg@20: 0.992457\tvalid_0's ndcg@30: 0.992457\tvalid_0's ndcg@40: 0.992457\tvalid_0's ndcg@50: 0.992457\tvalid_0's ndcg@10: 0.992369\tvalid_0's ndcg@20: 0.992457\tvalid_0's ndcg@30: 0.992457\tvalid_0's ndcg@40: 0.992457\tvalid_0's ndcg@50: 0.992457\n","[19]\tvalid_0's ndcg@10: 0.992395\tvalid_0's ndcg@20: 0.992483\tvalid_0's ndcg@30: 0.992483\tvalid_0's ndcg@40: 0.992483\tvalid_0's ndcg@50: 0.992483\tvalid_0's ndcg@10: 0.992395\tvalid_0's ndcg@20: 0.992483\tvalid_0's ndcg@30: 0.992483\tvalid_0's ndcg@40: 0.992483\tvalid_0's ndcg@50: 0.992483\n","[20]\tvalid_0's ndcg@10: 0.99234\tvalid_0's ndcg@20: 0.992428\tvalid_0's ndcg@30: 0.992428\tvalid_0's ndcg@40: 0.992428\tvalid_0's ndcg@50: 0.992428\tvalid_0's ndcg@10: 0.99234\tvalid_0's ndcg@20: 0.992428\tvalid_0's ndcg@30: 0.992428\tvalid_0's ndcg@40: 0.992428\tvalid_0's ndcg@50: 0.992428\n","[21]\tvalid_0's ndcg@10: 0.992416\tvalid_0's ndcg@20: 0.992504\tvalid_0's ndcg@30: 0.992504\tvalid_0's ndcg@40: 0.992504\tvalid_0's ndcg@50: 0.992504\tvalid_0's ndcg@10: 0.992416\tvalid_0's ndcg@20: 0.992504\tvalid_0's ndcg@30: 0.992504\tvalid_0's ndcg@40: 0.992504\tvalid_0's ndcg@50: 0.992504\n","[22]\tvalid_0's ndcg@10: 0.992468\tvalid_0's ndcg@20: 0.992556\tvalid_0's ndcg@30: 0.992556\tvalid_0's ndcg@40: 0.992556\tvalid_0's ndcg@50: 0.992556\tvalid_0's ndcg@10: 0.992468\tvalid_0's ndcg@20: 0.992556\tvalid_0's ndcg@30: 0.992556\tvalid_0's ndcg@40: 0.992556\tvalid_0's ndcg@50: 0.992556\n","[23]\tvalid_0's ndcg@10: 0.992337\tvalid_0's ndcg@20: 0.992425\tvalid_0's ndcg@30: 0.992425\tvalid_0's ndcg@40: 0.992425\tvalid_0's ndcg@50: 0.992425\tvalid_0's ndcg@10: 0.992337\tvalid_0's ndcg@20: 0.992425\tvalid_0's ndcg@30: 0.992425\tvalid_0's ndcg@40: 0.992425\tvalid_0's ndcg@50: 0.992425\n","[24]\tvalid_0's ndcg@10: 0.992237\tvalid_0's ndcg@20: 0.992325\tvalid_0's ndcg@30: 0.992325\tvalid_0's ndcg@40: 0.992325\tvalid_0's ndcg@50: 0.992325\tvalid_0's ndcg@10: 0.992237\tvalid_0's ndcg@20: 0.992325\tvalid_0's ndcg@30: 0.992325\tvalid_0's ndcg@40: 0.992325\tvalid_0's ndcg@50: 0.992325\n","[25]\tvalid_0's ndcg@10: 0.992239\tvalid_0's ndcg@20: 0.992327\tvalid_0's ndcg@30: 0.992327\tvalid_0's ndcg@40: 0.992327\tvalid_0's ndcg@50: 0.992327\tvalid_0's ndcg@10: 0.992239\tvalid_0's ndcg@20: 0.992327\tvalid_0's ndcg@30: 0.992327\tvalid_0's ndcg@40: 0.992327\tvalid_0's ndcg@50: 0.992327\n","[26]\tvalid_0's ndcg@10: 0.992301\tvalid_0's ndcg@20: 0.992389\tvalid_0's ndcg@30: 0.992389\tvalid_0's ndcg@40: 0.992389\tvalid_0's ndcg@50: 0.992389\tvalid_0's ndcg@10: 0.992301\tvalid_0's ndcg@20: 0.992389\tvalid_0's ndcg@30: 0.992389\tvalid_0's ndcg@40: 0.992389\tvalid_0's ndcg@50: 0.992389\n","[27]\tvalid_0's ndcg@10: 0.992288\tvalid_0's ndcg@20: 0.992376\tvalid_0's ndcg@30: 0.992376\tvalid_0's ndcg@40: 0.992376\tvalid_0's ndcg@50: 0.992376\tvalid_0's ndcg@10: 0.992288\tvalid_0's ndcg@20: 0.992376\tvalid_0's ndcg@30: 0.992376\tvalid_0's ndcg@40: 0.992376\tvalid_0's ndcg@50: 0.992376\n","[28]\tvalid_0's ndcg@10: 0.992095\tvalid_0's ndcg@20: 0.992183\tvalid_0's ndcg@30: 0.992183\tvalid_0's ndcg@40: 0.992183\tvalid_0's ndcg@50: 0.992183\tvalid_0's ndcg@10: 0.992095\tvalid_0's ndcg@20: 0.992183\tvalid_0's ndcg@30: 0.992183\tvalid_0's ndcg@40: 0.992183\tvalid_0's ndcg@50: 0.992183\n","[29]\tvalid_0's ndcg@10: 0.99209\tvalid_0's ndcg@20: 0.992178\tvalid_0's ndcg@30: 0.992178\tvalid_0's ndcg@40: 0.992178\tvalid_0's ndcg@50: 0.992178\tvalid_0's ndcg@10: 0.99209\tvalid_0's ndcg@20: 0.992178\tvalid_0's ndcg@30: 0.992178\tvalid_0's ndcg@40: 0.992178\tvalid_0's ndcg@50: 0.992178\n","[30]\tvalid_0's ndcg@10: 0.992048\tvalid_0's ndcg@20: 0.992136\tvalid_0's ndcg@30: 0.992136\tvalid_0's ndcg@40: 0.992136\tvalid_0's ndcg@50: 0.992136\tvalid_0's ndcg@10: 0.992048\tvalid_0's ndcg@20: 0.992136\tvalid_0's ndcg@30: 0.992136\tvalid_0's ndcg@40: 0.992136\tvalid_0's ndcg@50: 0.992136\n","[31]\tvalid_0's ndcg@10: 0.992118\tvalid_0's ndcg@20: 0.992206\tvalid_0's ndcg@30: 0.992206\tvalid_0's ndcg@40: 0.992206\tvalid_0's ndcg@50: 0.992206\tvalid_0's ndcg@10: 0.992118\tvalid_0's ndcg@20: 0.992206\tvalid_0's ndcg@30: 0.992206\tvalid_0's ndcg@40: 0.992206\tvalid_0's ndcg@50: 0.992206\n","[32]\tvalid_0's ndcg@10: 0.991996\tvalid_0's ndcg@20: 0.992084\tvalid_0's ndcg@30: 0.992084\tvalid_0's ndcg@40: 0.992084\tvalid_0's ndcg@50: 0.992084\tvalid_0's ndcg@10: 0.991996\tvalid_0's ndcg@20: 0.992084\tvalid_0's ndcg@30: 0.992084\tvalid_0's ndcg@40: 0.992084\tvalid_0's ndcg@50: 0.992084\n","[33]\tvalid_0's ndcg@10: 0.992026\tvalid_0's ndcg@20: 0.992114\tvalid_0's ndcg@30: 0.992114\tvalid_0's ndcg@40: 0.992114\tvalid_0's ndcg@50: 0.992114\tvalid_0's ndcg@10: 0.992026\tvalid_0's ndcg@20: 0.992114\tvalid_0's ndcg@30: 0.992114\tvalid_0's ndcg@40: 0.992114\tvalid_0's ndcg@50: 0.992114\n","[34]\tvalid_0's ndcg@10: 0.99197\tvalid_0's ndcg@20: 0.992058\tvalid_0's ndcg@30: 0.992058\tvalid_0's ndcg@40: 0.992058\tvalid_0's ndcg@50: 0.992058\tvalid_0's ndcg@10: 0.99197\tvalid_0's ndcg@20: 0.992058\tvalid_0's ndcg@30: 0.992058\tvalid_0's ndcg@40: 0.992058\tvalid_0's ndcg@50: 0.992058\n","[35]\tvalid_0's ndcg@10: 0.992114\tvalid_0's ndcg@20: 0.992202\tvalid_0's ndcg@30: 0.992202\tvalid_0's ndcg@40: 0.992202\tvalid_0's ndcg@50: 0.992202\tvalid_0's ndcg@10: 0.992114\tvalid_0's ndcg@20: 0.992202\tvalid_0's ndcg@30: 0.992202\tvalid_0's ndcg@40: 0.992202\tvalid_0's ndcg@50: 0.992202\n","[36]\tvalid_0's ndcg@10: 0.992042\tvalid_0's ndcg@20: 0.99213\tvalid_0's ndcg@30: 0.99213\tvalid_0's ndcg@40: 0.99213\tvalid_0's ndcg@50: 0.99213\tvalid_0's ndcg@10: 0.992042\tvalid_0's ndcg@20: 0.99213\tvalid_0's ndcg@30: 0.99213\tvalid_0's ndcg@40: 0.99213\tvalid_0's ndcg@50: 0.99213\n","[37]\tvalid_0's ndcg@10: 0.992082\tvalid_0's ndcg@20: 0.99217\tvalid_0's ndcg@30: 0.99217\tvalid_0's ndcg@40: 0.99217\tvalid_0's ndcg@50: 0.99217\tvalid_0's ndcg@10: 0.992082\tvalid_0's ndcg@20: 0.99217\tvalid_0's ndcg@30: 0.99217\tvalid_0's ndcg@40: 0.99217\tvalid_0's ndcg@50: 0.99217\n","[38]\tvalid_0's ndcg@10: 0.992012\tvalid_0's ndcg@20: 0.9921\tvalid_0's ndcg@30: 0.9921\tvalid_0's ndcg@40: 0.9921\tvalid_0's ndcg@50: 0.9921\tvalid_0's ndcg@10: 0.992012\tvalid_0's ndcg@20: 0.9921\tvalid_0's ndcg@30: 0.9921\tvalid_0's ndcg@40: 0.9921\tvalid_0's ndcg@50: 0.9921\n","[39]\tvalid_0's ndcg@10: 0.992111\tvalid_0's ndcg@20: 0.992199\tvalid_0's ndcg@30: 0.992199\tvalid_0's ndcg@40: 0.992199\tvalid_0's ndcg@50: 0.992199\tvalid_0's ndcg@10: 0.992111\tvalid_0's ndcg@20: 0.992199\tvalid_0's ndcg@30: 0.992199\tvalid_0's ndcg@40: 0.992199\tvalid_0's ndcg@50: 0.992199\n","[40]\tvalid_0's ndcg@10: 0.992244\tvalid_0's ndcg@20: 0.992332\tvalid_0's ndcg@30: 0.992332\tvalid_0's ndcg@40: 0.992332\tvalid_0's ndcg@50: 0.992332\tvalid_0's ndcg@10: 0.992244\tvalid_0's ndcg@20: 0.992332\tvalid_0's ndcg@30: 0.992332\tvalid_0's ndcg@40: 0.992332\tvalid_0's ndcg@50: 0.992332\n","[41]\tvalid_0's ndcg@10: 0.992313\tvalid_0's ndcg@20: 0.992401\tvalid_0's ndcg@30: 0.992401\tvalid_0's ndcg@40: 0.992401\tvalid_0's ndcg@50: 0.992401\tvalid_0's ndcg@10: 0.992313\tvalid_0's ndcg@20: 0.992401\tvalid_0's ndcg@30: 0.992401\tvalid_0's ndcg@40: 0.992401\tvalid_0's ndcg@50: 0.992401\n","[42]\tvalid_0's ndcg@10: 0.992185\tvalid_0's ndcg@20: 0.992273\tvalid_0's ndcg@30: 0.992273\tvalid_0's ndcg@40: 0.992273\tvalid_0's ndcg@50: 0.992273\tvalid_0's ndcg@10: 0.992185\tvalid_0's ndcg@20: 0.992273\tvalid_0's ndcg@30: 0.992273\tvalid_0's ndcg@40: 0.992273\tvalid_0's ndcg@50: 0.992273\n","[43]\tvalid_0's ndcg@10: 0.99221\tvalid_0's ndcg@20: 0.992298\tvalid_0's ndcg@30: 0.992298\tvalid_0's ndcg@40: 0.992298\tvalid_0's ndcg@50: 0.992298\tvalid_0's ndcg@10: 0.99221\tvalid_0's ndcg@20: 0.992298\tvalid_0's ndcg@30: 0.992298\tvalid_0's ndcg@40: 0.992298\tvalid_0's ndcg@50: 0.992298\n","[44]\tvalid_0's ndcg@10: 0.992221\tvalid_0's ndcg@20: 0.992309\tvalid_0's ndcg@30: 0.992309\tvalid_0's ndcg@40: 0.992309\tvalid_0's ndcg@50: 0.992309\tvalid_0's ndcg@10: 0.992221\tvalid_0's ndcg@20: 0.992309\tvalid_0's ndcg@30: 0.992309\tvalid_0's ndcg@40: 0.992309\tvalid_0's ndcg@50: 0.992309\n","[45]\tvalid_0's ndcg@10: 0.992226\tvalid_0's ndcg@20: 0.992314\tvalid_0's ndcg@30: 0.992314\tvalid_0's ndcg@40: 0.992314\tvalid_0's ndcg@50: 0.992314\tvalid_0's ndcg@10: 0.992226\tvalid_0's ndcg@20: 0.992314\tvalid_0's ndcg@30: 0.992314\tvalid_0's ndcg@40: 0.992314\tvalid_0's ndcg@50: 0.992314\n","[46]\tvalid_0's ndcg@10: 0.992263\tvalid_0's ndcg@20: 0.992351\tvalid_0's ndcg@30: 0.992351\tvalid_0's ndcg@40: 0.992351\tvalid_0's ndcg@50: 0.992351\tvalid_0's ndcg@10: 0.992263\tvalid_0's ndcg@20: 0.992351\tvalid_0's ndcg@30: 0.992351\tvalid_0's ndcg@40: 0.992351\tvalid_0's ndcg@50: 0.992351\n","[47]\tvalid_0's ndcg@10: 0.992141\tvalid_0's ndcg@20: 0.992229\tvalid_0's ndcg@30: 0.992229\tvalid_0's ndcg@40: 0.992229\tvalid_0's ndcg@50: 0.992229\tvalid_0's ndcg@10: 0.992141\tvalid_0's ndcg@20: 0.992229\tvalid_0's ndcg@30: 0.992229\tvalid_0's ndcg@40: 0.992229\tvalid_0's ndcg@50: 0.992229\n","[48]\tvalid_0's ndcg@10: 0.992053\tvalid_0's ndcg@20: 0.992141\tvalid_0's ndcg@30: 0.992141\tvalid_0's ndcg@40: 0.992141\tvalid_0's ndcg@50: 0.992141\tvalid_0's ndcg@10: 0.992053\tvalid_0's ndcg@20: 0.992141\tvalid_0's ndcg@30: 0.992141\tvalid_0's ndcg@40: 0.992141\tvalid_0's ndcg@50: 0.992141\n","[49]\tvalid_0's ndcg@10: 0.992064\tvalid_0's ndcg@20: 0.992152\tvalid_0's ndcg@30: 0.992152\tvalid_0's ndcg@40: 0.992152\tvalid_0's ndcg@50: 0.992152\tvalid_0's ndcg@10: 0.992064\tvalid_0's ndcg@20: 0.992152\tvalid_0's ndcg@30: 0.992152\tvalid_0's ndcg@40: 0.992152\tvalid_0's ndcg@50: 0.992152\n","[50]\tvalid_0's ndcg@10: 0.992082\tvalid_0's ndcg@20: 0.99217\tvalid_0's ndcg@30: 0.99217\tvalid_0's ndcg@40: 0.99217\tvalid_0's ndcg@50: 0.99217\tvalid_0's ndcg@10: 0.992082\tvalid_0's ndcg@20: 0.99217\tvalid_0's ndcg@30: 0.99217\tvalid_0's ndcg@40: 0.99217\tvalid_0's ndcg@50: 0.99217\n","[51]\tvalid_0's ndcg@10: 0.992167\tvalid_0's ndcg@20: 0.992255\tvalid_0's ndcg@30: 0.992255\tvalid_0's ndcg@40: 0.992255\tvalid_0's ndcg@50: 0.992255\tvalid_0's ndcg@10: 0.992167\tvalid_0's ndcg@20: 0.992255\tvalid_0's ndcg@30: 0.992255\tvalid_0's ndcg@40: 0.992255\tvalid_0's ndcg@50: 0.992255\n","[52]\tvalid_0's ndcg@10: 0.99224\tvalid_0's ndcg@20: 0.992328\tvalid_0's ndcg@30: 0.992328\tvalid_0's ndcg@40: 0.992328\tvalid_0's ndcg@50: 0.992328\tvalid_0's ndcg@10: 0.99224\tvalid_0's ndcg@20: 0.992328\tvalid_0's ndcg@30: 0.992328\tvalid_0's ndcg@40: 0.992328\tvalid_0's ndcg@50: 0.992328\n","[53]\tvalid_0's ndcg@10: 0.992138\tvalid_0's ndcg@20: 0.992226\tvalid_0's ndcg@30: 0.992226\tvalid_0's ndcg@40: 0.992226\tvalid_0's ndcg@50: 0.992226\tvalid_0's ndcg@10: 0.992138\tvalid_0's ndcg@20: 0.992226\tvalid_0's ndcg@30: 0.992226\tvalid_0's ndcg@40: 0.992226\tvalid_0's ndcg@50: 0.992226\n","[54]\tvalid_0's ndcg@10: 0.99209\tvalid_0's ndcg@20: 0.992178\tvalid_0's ndcg@30: 0.992178\tvalid_0's ndcg@40: 0.992178\tvalid_0's ndcg@50: 0.992178\tvalid_0's ndcg@10: 0.99209\tvalid_0's ndcg@20: 0.992178\tvalid_0's ndcg@30: 0.992178\tvalid_0's ndcg@40: 0.992178\tvalid_0's ndcg@50: 0.992178\n","[55]\tvalid_0's ndcg@10: 0.992235\tvalid_0's ndcg@20: 0.992323\tvalid_0's ndcg@30: 0.992323\tvalid_0's ndcg@40: 0.992323\tvalid_0's ndcg@50: 0.992323\tvalid_0's ndcg@10: 0.992235\tvalid_0's ndcg@20: 0.992323\tvalid_0's ndcg@30: 0.992323\tvalid_0's ndcg@40: 0.992323\tvalid_0's ndcg@50: 0.992323\n","[56]\tvalid_0's ndcg@10: 0.992309\tvalid_0's ndcg@20: 0.992397\tvalid_0's ndcg@30: 0.992397\tvalid_0's ndcg@40: 0.992397\tvalid_0's ndcg@50: 0.992397\tvalid_0's ndcg@10: 0.992309\tvalid_0's ndcg@20: 0.992397\tvalid_0's ndcg@30: 0.992397\tvalid_0's ndcg@40: 0.992397\tvalid_0's ndcg@50: 0.992397\n","[57]\tvalid_0's ndcg@10: 0.992297\tvalid_0's ndcg@20: 0.992385\tvalid_0's ndcg@30: 0.992385\tvalid_0's ndcg@40: 0.992385\tvalid_0's ndcg@50: 0.992385\tvalid_0's ndcg@10: 0.992297\tvalid_0's ndcg@20: 0.992385\tvalid_0's ndcg@30: 0.992385\tvalid_0's ndcg@40: 0.992385\tvalid_0's ndcg@50: 0.992385\n","[58]\tvalid_0's ndcg@10: 0.992224\tvalid_0's ndcg@20: 0.992312\tvalid_0's ndcg@30: 0.992312\tvalid_0's ndcg@40: 0.992312\tvalid_0's ndcg@50: 0.992312\tvalid_0's ndcg@10: 0.992224\tvalid_0's ndcg@20: 0.992312\tvalid_0's ndcg@30: 0.992312\tvalid_0's ndcg@40: 0.992312\tvalid_0's ndcg@50: 0.992312\n","[59]\tvalid_0's ndcg@10: 0.992301\tvalid_0's ndcg@20: 0.992389\tvalid_0's ndcg@30: 0.992389\tvalid_0's ndcg@40: 0.992389\tvalid_0's ndcg@50: 0.992389\tvalid_0's ndcg@10: 0.992301\tvalid_0's ndcg@20: 0.992389\tvalid_0's ndcg@30: 0.992389\tvalid_0's ndcg@40: 0.992389\tvalid_0's ndcg@50: 0.992389\n","[60]\tvalid_0's ndcg@10: 0.99233\tvalid_0's ndcg@20: 0.992418\tvalid_0's ndcg@30: 0.992418\tvalid_0's ndcg@40: 0.992418\tvalid_0's ndcg@50: 0.992418\tvalid_0's ndcg@10: 0.99233\tvalid_0's ndcg@20: 0.992418\tvalid_0's ndcg@30: 0.992418\tvalid_0's ndcg@40: 0.992418\tvalid_0's ndcg@50: 0.992418\n","[61]\tvalid_0's ndcg@10: 0.992146\tvalid_0's ndcg@20: 0.992235\tvalid_0's ndcg@30: 0.992235\tvalid_0's ndcg@40: 0.992235\tvalid_0's ndcg@50: 0.992235\tvalid_0's ndcg@10: 0.992146\tvalid_0's ndcg@20: 0.992235\tvalid_0's ndcg@30: 0.992235\tvalid_0's ndcg@40: 0.992235\tvalid_0's ndcg@50: 0.992235\n","[62]\tvalid_0's ndcg@10: 0.992216\tvalid_0's ndcg@20: 0.992306\tvalid_0's ndcg@30: 0.992306\tvalid_0's ndcg@40: 0.992306\tvalid_0's ndcg@50: 0.992306\tvalid_0's ndcg@10: 0.992216\tvalid_0's ndcg@20: 0.992306\tvalid_0's ndcg@30: 0.992306\tvalid_0's ndcg@40: 0.992306\tvalid_0's ndcg@50: 0.992306\n","[63]\tvalid_0's ndcg@10: 0.992272\tvalid_0's ndcg@20: 0.992361\tvalid_0's ndcg@30: 0.992361\tvalid_0's ndcg@40: 0.992361\tvalid_0's ndcg@50: 0.992361\tvalid_0's ndcg@10: 0.992272\tvalid_0's ndcg@20: 0.992361\tvalid_0's ndcg@30: 0.992361\tvalid_0's ndcg@40: 0.992361\tvalid_0's ndcg@50: 0.992361\n","[64]\tvalid_0's ndcg@10: 0.992272\tvalid_0's ndcg@20: 0.992361\tvalid_0's ndcg@30: 0.992361\tvalid_0's ndcg@40: 0.992361\tvalid_0's ndcg@50: 0.992361\tvalid_0's ndcg@10: 0.992272\tvalid_0's ndcg@20: 0.992361\tvalid_0's ndcg@30: 0.992361\tvalid_0's ndcg@40: 0.992361\tvalid_0's ndcg@50: 0.992361\n","[65]\tvalid_0's ndcg@10: 0.992248\tvalid_0's ndcg@20: 0.992337\tvalid_0's ndcg@30: 0.992337\tvalid_0's ndcg@40: 0.992337\tvalid_0's ndcg@50: 0.992337\tvalid_0's ndcg@10: 0.992248\tvalid_0's ndcg@20: 0.992337\tvalid_0's ndcg@30: 0.992337\tvalid_0's ndcg@40: 0.992337\tvalid_0's ndcg@50: 0.992337\n","[66]\tvalid_0's ndcg@10: 0.992248\tvalid_0's ndcg@20: 0.992337\tvalid_0's ndcg@30: 0.992337\tvalid_0's ndcg@40: 0.992337\tvalid_0's ndcg@50: 0.992337\tvalid_0's ndcg@10: 0.992248\tvalid_0's ndcg@20: 0.992337\tvalid_0's ndcg@30: 0.992337\tvalid_0's ndcg@40: 0.992337\tvalid_0's ndcg@50: 0.992337\n","[67]\tvalid_0's ndcg@10: 0.992232\tvalid_0's ndcg@20: 0.992321\tvalid_0's ndcg@30: 0.992321\tvalid_0's ndcg@40: 0.992321\tvalid_0's ndcg@50: 0.992321\tvalid_0's ndcg@10: 0.992232\tvalid_0's ndcg@20: 0.992321\tvalid_0's ndcg@30: 0.992321\tvalid_0's ndcg@40: 0.992321\tvalid_0's ndcg@50: 0.992321\n","[68]\tvalid_0's ndcg@10: 0.992251\tvalid_0's ndcg@20: 0.99234\tvalid_0's ndcg@30: 0.99234\tvalid_0's ndcg@40: 0.99234\tvalid_0's ndcg@50: 0.99234\tvalid_0's ndcg@10: 0.992251\tvalid_0's ndcg@20: 0.99234\tvalid_0's ndcg@30: 0.99234\tvalid_0's ndcg@40: 0.99234\tvalid_0's ndcg@50: 0.99234\n","[69]\tvalid_0's ndcg@10: 0.992292\tvalid_0's ndcg@20: 0.992381\tvalid_0's ndcg@30: 0.992381\tvalid_0's ndcg@40: 0.992381\tvalid_0's ndcg@50: 0.992381\tvalid_0's ndcg@10: 0.992292\tvalid_0's ndcg@20: 0.992381\tvalid_0's ndcg@30: 0.992381\tvalid_0's ndcg@40: 0.992381\tvalid_0's ndcg@50: 0.992381\n","[70]\tvalid_0's ndcg@10: 0.992235\tvalid_0's ndcg@20: 0.992324\tvalid_0's ndcg@30: 0.992324\tvalid_0's ndcg@40: 0.992324\tvalid_0's ndcg@50: 0.992324\tvalid_0's ndcg@10: 0.992235\tvalid_0's ndcg@20: 0.992324\tvalid_0's ndcg@30: 0.992324\tvalid_0's ndcg@40: 0.992324\tvalid_0's ndcg@50: 0.992324\n","[71]\tvalid_0's ndcg@10: 0.99223\tvalid_0's ndcg@20: 0.992319\tvalid_0's ndcg@30: 0.992319\tvalid_0's ndcg@40: 0.992319\tvalid_0's ndcg@50: 0.992319\tvalid_0's ndcg@10: 0.99223\tvalid_0's ndcg@20: 0.992319\tvalid_0's ndcg@30: 0.992319\tvalid_0's ndcg@40: 0.992319\tvalid_0's ndcg@50: 0.992319\n","[72]\tvalid_0's ndcg@10: 0.992175\tvalid_0's ndcg@20: 0.992264\tvalid_0's ndcg@30: 0.992264\tvalid_0's ndcg@40: 0.992264\tvalid_0's ndcg@50: 0.992264\tvalid_0's ndcg@10: 0.992175\tvalid_0's ndcg@20: 0.992264\tvalid_0's ndcg@30: 0.992264\tvalid_0's ndcg@40: 0.992264\tvalid_0's ndcg@50: 0.992264\n","Early stopping, best iteration is:\n","[22]\tvalid_0's ndcg@10: 0.992468\tvalid_0's ndcg@20: 0.992556\tvalid_0's ndcg@30: 0.992556\tvalid_0's ndcg@40: 0.992556\tvalid_0's ndcg@50: 0.992556\tvalid_0's ndcg@10: 0.992468\tvalid_0's ndcg@20: 0.992556\tvalid_0's ndcg@30: 0.992556\tvalid_0's ndcg@40: 0.992556\tvalid_0's ndcg@50: 0.992556\n","[1]\tvalid_0's ndcg@10: 0.991774\tvalid_0's ndcg@20: 0.991821\tvalid_0's ndcg@30: 0.991821\tvalid_0's ndcg@40: 0.991821\tvalid_0's ndcg@50: 0.991821\tvalid_0's ndcg@10: 0.991774\tvalid_0's ndcg@20: 0.991821\tvalid_0's ndcg@30: 0.991821\tvalid_0's ndcg@40: 0.991821\tvalid_0's ndcg@50: 0.991821\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.991375\tvalid_0's ndcg@20: 0.991375\tvalid_0's ndcg@30: 0.991375\tvalid_0's ndcg@40: 0.991375\tvalid_0's ndcg@50: 0.991375\tvalid_0's ndcg@10: 0.991375\tvalid_0's ndcg@20: 0.991375\tvalid_0's ndcg@30: 0.991375\tvalid_0's ndcg@40: 0.991375\tvalid_0's ndcg@50: 0.991375\n","[3]\tvalid_0's ndcg@10: 0.992355\tvalid_0's ndcg@20: 0.992355\tvalid_0's ndcg@30: 0.992355\tvalid_0's ndcg@40: 0.992355\tvalid_0's ndcg@50: 0.992355\tvalid_0's ndcg@10: 0.992355\tvalid_0's ndcg@20: 0.992355\tvalid_0's ndcg@30: 0.992355\tvalid_0's ndcg@40: 0.992355\tvalid_0's ndcg@50: 0.992355\n","[4]\tvalid_0's ndcg@10: 0.992886\tvalid_0's ndcg@20: 0.992886\tvalid_0's ndcg@30: 0.992886\tvalid_0's ndcg@40: 0.992886\tvalid_0's ndcg@50: 0.992886\tvalid_0's ndcg@10: 0.992886\tvalid_0's ndcg@20: 0.992886\tvalid_0's ndcg@30: 0.992886\tvalid_0's ndcg@40: 0.992886\tvalid_0's ndcg@50: 0.992886\n","[5]\tvalid_0's ndcg@10: 0.99283\tvalid_0's ndcg@20: 0.99283\tvalid_0's ndcg@30: 0.99283\tvalid_0's ndcg@40: 0.99283\tvalid_0's ndcg@50: 0.99283\tvalid_0's ndcg@10: 0.99283\tvalid_0's ndcg@20: 0.99283\tvalid_0's ndcg@30: 0.99283\tvalid_0's ndcg@40: 0.99283\tvalid_0's ndcg@50: 0.99283\n","[6]\tvalid_0's ndcg@10: 0.993008\tvalid_0's ndcg@20: 0.993008\tvalid_0's ndcg@30: 0.993008\tvalid_0's ndcg@40: 0.993008\tvalid_0's ndcg@50: 0.993008\tvalid_0's ndcg@10: 0.993008\tvalid_0's ndcg@20: 0.993008\tvalid_0's ndcg@30: 0.993008\tvalid_0's ndcg@40: 0.993008\tvalid_0's ndcg@50: 0.993008\n","[7]\tvalid_0's ndcg@10: 0.992938\tvalid_0's ndcg@20: 0.992938\tvalid_0's ndcg@30: 0.992938\tvalid_0's ndcg@40: 0.992938\tvalid_0's ndcg@50: 0.992938\tvalid_0's ndcg@10: 0.992938\tvalid_0's ndcg@20: 0.992938\tvalid_0's ndcg@30: 0.992938\tvalid_0's ndcg@40: 0.992938\tvalid_0's ndcg@50: 0.992938\n","[8]\tvalid_0's ndcg@10: 0.992925\tvalid_0's ndcg@20: 0.992925\tvalid_0's ndcg@30: 0.992925\tvalid_0's ndcg@40: 0.992925\tvalid_0's ndcg@50: 0.992925\tvalid_0's ndcg@10: 0.992925\tvalid_0's ndcg@20: 0.992925\tvalid_0's ndcg@30: 0.992925\tvalid_0's ndcg@40: 0.992925\tvalid_0's ndcg@50: 0.992925\n","[9]\tvalid_0's ndcg@10: 0.993176\tvalid_0's ndcg@20: 0.993176\tvalid_0's ndcg@30: 0.993176\tvalid_0's ndcg@40: 0.993176\tvalid_0's ndcg@50: 0.993176\tvalid_0's ndcg@10: 0.993176\tvalid_0's ndcg@20: 0.993176\tvalid_0's ndcg@30: 0.993176\tvalid_0's ndcg@40: 0.993176\tvalid_0's ndcg@50: 0.993176\n","[10]\tvalid_0's ndcg@10: 0.992865\tvalid_0's ndcg@20: 0.992865\tvalid_0's ndcg@30: 0.992865\tvalid_0's ndcg@40: 0.992865\tvalid_0's ndcg@50: 0.992865\tvalid_0's ndcg@10: 0.992865\tvalid_0's ndcg@20: 0.992865\tvalid_0's ndcg@30: 0.992865\tvalid_0's ndcg@40: 0.992865\tvalid_0's ndcg@50: 0.992865\n","[11]\tvalid_0's ndcg@10: 0.993044\tvalid_0's ndcg@20: 0.993044\tvalid_0's ndcg@30: 0.993044\tvalid_0's ndcg@40: 0.993044\tvalid_0's ndcg@50: 0.993044\tvalid_0's ndcg@10: 0.993044\tvalid_0's ndcg@20: 0.993044\tvalid_0's ndcg@30: 0.993044\tvalid_0's ndcg@40: 0.993044\tvalid_0's ndcg@50: 0.993044\n","[12]\tvalid_0's ndcg@10: 0.993156\tvalid_0's ndcg@20: 0.993156\tvalid_0's ndcg@30: 0.993156\tvalid_0's ndcg@40: 0.993156\tvalid_0's ndcg@50: 0.993156\tvalid_0's ndcg@10: 0.993156\tvalid_0's ndcg@20: 0.993156\tvalid_0's ndcg@30: 0.993156\tvalid_0's ndcg@40: 0.993156\tvalid_0's ndcg@50: 0.993156\n","[13]\tvalid_0's ndcg@10: 0.993256\tvalid_0's ndcg@20: 0.993256\tvalid_0's ndcg@30: 0.993256\tvalid_0's ndcg@40: 0.993256\tvalid_0's ndcg@50: 0.993256\tvalid_0's ndcg@10: 0.993256\tvalid_0's ndcg@20: 0.993256\tvalid_0's ndcg@30: 0.993256\tvalid_0's ndcg@40: 0.993256\tvalid_0's ndcg@50: 0.993256\n","[14]\tvalid_0's ndcg@10: 0.993102\tvalid_0's ndcg@20: 0.993102\tvalid_0's ndcg@30: 0.993102\tvalid_0's ndcg@40: 0.993102\tvalid_0's ndcg@50: 0.993102\tvalid_0's ndcg@10: 0.993102\tvalid_0's ndcg@20: 0.993102\tvalid_0's ndcg@30: 0.993102\tvalid_0's ndcg@40: 0.993102\tvalid_0's ndcg@50: 0.993102\n","[15]\tvalid_0's ndcg@10: 0.99329\tvalid_0's ndcg@20: 0.99329\tvalid_0's ndcg@30: 0.99329\tvalid_0's ndcg@40: 0.99329\tvalid_0's ndcg@50: 0.99329\tvalid_0's ndcg@10: 0.99329\tvalid_0's ndcg@20: 0.99329\tvalid_0's ndcg@30: 0.99329\tvalid_0's ndcg@40: 0.99329\tvalid_0's ndcg@50: 0.99329\n","[16]\tvalid_0's ndcg@10: 0.99343\tvalid_0's ndcg@20: 0.99343\tvalid_0's ndcg@30: 0.99343\tvalid_0's ndcg@40: 0.99343\tvalid_0's ndcg@50: 0.99343\tvalid_0's ndcg@10: 0.99343\tvalid_0's ndcg@20: 0.99343\tvalid_0's ndcg@30: 0.99343\tvalid_0's ndcg@40: 0.99343\tvalid_0's ndcg@50: 0.99343\n","[17]\tvalid_0's ndcg@10: 0.993277\tvalid_0's ndcg@20: 0.993277\tvalid_0's ndcg@30: 0.993277\tvalid_0's ndcg@40: 0.993277\tvalid_0's ndcg@50: 0.993277\tvalid_0's ndcg@10: 0.993277\tvalid_0's ndcg@20: 0.993277\tvalid_0's ndcg@30: 0.993277\tvalid_0's ndcg@40: 0.993277\tvalid_0's ndcg@50: 0.993277\n","[18]\tvalid_0's ndcg@10: 0.993385\tvalid_0's ndcg@20: 0.993385\tvalid_0's ndcg@30: 0.993385\tvalid_0's ndcg@40: 0.993385\tvalid_0's ndcg@50: 0.993385\tvalid_0's ndcg@10: 0.993385\tvalid_0's ndcg@20: 0.993385\tvalid_0's ndcg@30: 0.993385\tvalid_0's ndcg@40: 0.993385\tvalid_0's ndcg@50: 0.993385\n","[19]\tvalid_0's ndcg@10: 0.993464\tvalid_0's ndcg@20: 0.993464\tvalid_0's ndcg@30: 0.993464\tvalid_0's ndcg@40: 0.993464\tvalid_0's ndcg@50: 0.993464\tvalid_0's ndcg@10: 0.993464\tvalid_0's ndcg@20: 0.993464\tvalid_0's ndcg@30: 0.993464\tvalid_0's ndcg@40: 0.993464\tvalid_0's ndcg@50: 0.993464\n","[20]\tvalid_0's ndcg@10: 0.993342\tvalid_0's ndcg@20: 0.993342\tvalid_0's ndcg@30: 0.993342\tvalid_0's ndcg@40: 0.993342\tvalid_0's ndcg@50: 0.993342\tvalid_0's ndcg@10: 0.993342\tvalid_0's ndcg@20: 0.993342\tvalid_0's ndcg@30: 0.993342\tvalid_0's ndcg@40: 0.993342\tvalid_0's ndcg@50: 0.993342\n","[21]\tvalid_0's ndcg@10: 0.99354\tvalid_0's ndcg@20: 0.99354\tvalid_0's ndcg@30: 0.99354\tvalid_0's ndcg@40: 0.99354\tvalid_0's ndcg@50: 0.99354\tvalid_0's ndcg@10: 0.99354\tvalid_0's ndcg@20: 0.99354\tvalid_0's ndcg@30: 0.99354\tvalid_0's ndcg@40: 0.99354\tvalid_0's ndcg@50: 0.99354\n","[22]\tvalid_0's ndcg@10: 0.993425\tvalid_0's ndcg@20: 0.993425\tvalid_0's ndcg@30: 0.993425\tvalid_0's ndcg@40: 0.993425\tvalid_0's ndcg@50: 0.993425\tvalid_0's ndcg@10: 0.993425\tvalid_0's ndcg@20: 0.993425\tvalid_0's ndcg@30: 0.993425\tvalid_0's ndcg@40: 0.993425\tvalid_0's ndcg@50: 0.993425\n","[23]\tvalid_0's ndcg@10: 0.993459\tvalid_0's ndcg@20: 0.993459\tvalid_0's ndcg@30: 0.993459\tvalid_0's ndcg@40: 0.993459\tvalid_0's ndcg@50: 0.993459\tvalid_0's ndcg@10: 0.993459\tvalid_0's ndcg@20: 0.993459\tvalid_0's ndcg@30: 0.993459\tvalid_0's ndcg@40: 0.993459\tvalid_0's ndcg@50: 0.993459\n","[24]\tvalid_0's ndcg@10: 0.993519\tvalid_0's ndcg@20: 0.993519\tvalid_0's ndcg@30: 0.993519\tvalid_0's ndcg@40: 0.993519\tvalid_0's ndcg@50: 0.993519\tvalid_0's ndcg@10: 0.993519\tvalid_0's ndcg@20: 0.993519\tvalid_0's ndcg@30: 0.993519\tvalid_0's ndcg@40: 0.993519\tvalid_0's ndcg@50: 0.993519\n","[25]\tvalid_0's ndcg@10: 0.993583\tvalid_0's ndcg@20: 0.993583\tvalid_0's ndcg@30: 0.993583\tvalid_0's ndcg@40: 0.993583\tvalid_0's ndcg@50: 0.993583\tvalid_0's ndcg@10: 0.993583\tvalid_0's ndcg@20: 0.993583\tvalid_0's ndcg@30: 0.993583\tvalid_0's ndcg@40: 0.993583\tvalid_0's ndcg@50: 0.993583\n","[26]\tvalid_0's ndcg@10: 0.993542\tvalid_0's ndcg@20: 0.993542\tvalid_0's ndcg@30: 0.993542\tvalid_0's ndcg@40: 0.993542\tvalid_0's ndcg@50: 0.993542\tvalid_0's ndcg@10: 0.993542\tvalid_0's ndcg@20: 0.993542\tvalid_0's ndcg@30: 0.993542\tvalid_0's ndcg@40: 0.993542\tvalid_0's ndcg@50: 0.993542\n","[27]\tvalid_0's ndcg@10: 0.99342\tvalid_0's ndcg@20: 0.99342\tvalid_0's ndcg@30: 0.99342\tvalid_0's ndcg@40: 0.99342\tvalid_0's ndcg@50: 0.99342\tvalid_0's ndcg@10: 0.99342\tvalid_0's ndcg@20: 0.99342\tvalid_0's ndcg@30: 0.99342\tvalid_0's ndcg@40: 0.99342\tvalid_0's ndcg@50: 0.99342\n","[28]\tvalid_0's ndcg@10: 0.993375\tvalid_0's ndcg@20: 0.993375\tvalid_0's ndcg@30: 0.993375\tvalid_0's ndcg@40: 0.993375\tvalid_0's ndcg@50: 0.993375\tvalid_0's ndcg@10: 0.993375\tvalid_0's ndcg@20: 0.993375\tvalid_0's ndcg@30: 0.993375\tvalid_0's ndcg@40: 0.993375\tvalid_0's ndcg@50: 0.993375\n","[29]\tvalid_0's ndcg@10: 0.993353\tvalid_0's ndcg@20: 0.993353\tvalid_0's ndcg@30: 0.993353\tvalid_0's ndcg@40: 0.993353\tvalid_0's ndcg@50: 0.993353\tvalid_0's ndcg@10: 0.993353\tvalid_0's ndcg@20: 0.993353\tvalid_0's ndcg@30: 0.993353\tvalid_0's ndcg@40: 0.993353\tvalid_0's ndcg@50: 0.993353\n","[30]\tvalid_0's ndcg@10: 0.993545\tvalid_0's ndcg@20: 0.993545\tvalid_0's ndcg@30: 0.993545\tvalid_0's ndcg@40: 0.993545\tvalid_0's ndcg@50: 0.993545\tvalid_0's ndcg@10: 0.993545\tvalid_0's ndcg@20: 0.993545\tvalid_0's ndcg@30: 0.993545\tvalid_0's ndcg@40: 0.993545\tvalid_0's ndcg@50: 0.993545\n","[31]\tvalid_0's ndcg@10: 0.993396\tvalid_0's ndcg@20: 0.993396\tvalid_0's ndcg@30: 0.993396\tvalid_0's ndcg@40: 0.993396\tvalid_0's ndcg@50: 0.993396\tvalid_0's ndcg@10: 0.993396\tvalid_0's ndcg@20: 0.993396\tvalid_0's ndcg@30: 0.993396\tvalid_0's ndcg@40: 0.993396\tvalid_0's ndcg@50: 0.993396\n","[32]\tvalid_0's ndcg@10: 0.993278\tvalid_0's ndcg@20: 0.993278\tvalid_0's ndcg@30: 0.993278\tvalid_0's ndcg@40: 0.993278\tvalid_0's ndcg@50: 0.993278\tvalid_0's ndcg@10: 0.993278\tvalid_0's ndcg@20: 0.993278\tvalid_0's ndcg@30: 0.993278\tvalid_0's ndcg@40: 0.993278\tvalid_0's ndcg@50: 0.993278\n","[33]\tvalid_0's ndcg@10: 0.993341\tvalid_0's ndcg@20: 0.993341\tvalid_0's ndcg@30: 0.993341\tvalid_0's ndcg@40: 0.993341\tvalid_0's ndcg@50: 0.993341\tvalid_0's ndcg@10: 0.993341\tvalid_0's ndcg@20: 0.993341\tvalid_0's ndcg@30: 0.993341\tvalid_0's ndcg@40: 0.993341\tvalid_0's ndcg@50: 0.993341\n","[34]\tvalid_0's ndcg@10: 0.993319\tvalid_0's ndcg@20: 0.993319\tvalid_0's ndcg@30: 0.993319\tvalid_0's ndcg@40: 0.993319\tvalid_0's ndcg@50: 0.993319\tvalid_0's ndcg@10: 0.993319\tvalid_0's ndcg@20: 0.993319\tvalid_0's ndcg@30: 0.993319\tvalid_0's ndcg@40: 0.993319\tvalid_0's ndcg@50: 0.993319\n","[35]\tvalid_0's ndcg@10: 0.993419\tvalid_0's ndcg@20: 0.993419\tvalid_0's ndcg@30: 0.993419\tvalid_0's ndcg@40: 0.993419\tvalid_0's ndcg@50: 0.993419\tvalid_0's ndcg@10: 0.993419\tvalid_0's ndcg@20: 0.993419\tvalid_0's ndcg@30: 0.993419\tvalid_0's ndcg@40: 0.993419\tvalid_0's ndcg@50: 0.993419\n","[36]\tvalid_0's ndcg@10: 0.993395\tvalid_0's ndcg@20: 0.993395\tvalid_0's ndcg@30: 0.993395\tvalid_0's ndcg@40: 0.993395\tvalid_0's ndcg@50: 0.993395\tvalid_0's ndcg@10: 0.993395\tvalid_0's ndcg@20: 0.993395\tvalid_0's ndcg@30: 0.993395\tvalid_0's ndcg@40: 0.993395\tvalid_0's ndcg@50: 0.993395\n","[37]\tvalid_0's ndcg@10: 0.99352\tvalid_0's ndcg@20: 0.99352\tvalid_0's ndcg@30: 0.99352\tvalid_0's ndcg@40: 0.99352\tvalid_0's ndcg@50: 0.99352\tvalid_0's ndcg@10: 0.99352\tvalid_0's ndcg@20: 0.99352\tvalid_0's ndcg@30: 0.99352\tvalid_0's ndcg@40: 0.99352\tvalid_0's ndcg@50: 0.99352\n","[38]\tvalid_0's ndcg@10: 0.99348\tvalid_0's ndcg@20: 0.99348\tvalid_0's ndcg@30: 0.99348\tvalid_0's ndcg@40: 0.99348\tvalid_0's ndcg@50: 0.99348\tvalid_0's ndcg@10: 0.99348\tvalid_0's ndcg@20: 0.99348\tvalid_0's ndcg@30: 0.99348\tvalid_0's ndcg@40: 0.99348\tvalid_0's ndcg@50: 0.99348\n","[39]\tvalid_0's ndcg@10: 0.993485\tvalid_0's ndcg@20: 0.993485\tvalid_0's ndcg@30: 0.993485\tvalid_0's ndcg@40: 0.993485\tvalid_0's ndcg@50: 0.993485\tvalid_0's ndcg@10: 0.993485\tvalid_0's ndcg@20: 0.993485\tvalid_0's ndcg@30: 0.993485\tvalid_0's ndcg@40: 0.993485\tvalid_0's ndcg@50: 0.993485\n","[40]\tvalid_0's ndcg@10: 0.993406\tvalid_0's ndcg@20: 0.993406\tvalid_0's ndcg@30: 0.993406\tvalid_0's ndcg@40: 0.993406\tvalid_0's ndcg@50: 0.993406\tvalid_0's ndcg@10: 0.993406\tvalid_0's ndcg@20: 0.993406\tvalid_0's ndcg@30: 0.993406\tvalid_0's ndcg@40: 0.993406\tvalid_0's ndcg@50: 0.993406\n","[41]\tvalid_0's ndcg@10: 0.99328\tvalid_0's ndcg@20: 0.99328\tvalid_0's ndcg@30: 0.99328\tvalid_0's ndcg@40: 0.99328\tvalid_0's ndcg@50: 0.99328\tvalid_0's ndcg@10: 0.99328\tvalid_0's ndcg@20: 0.99328\tvalid_0's ndcg@30: 0.99328\tvalid_0's ndcg@40: 0.99328\tvalid_0's ndcg@50: 0.99328\n","[42]\tvalid_0's ndcg@10: 0.99321\tvalid_0's ndcg@20: 0.99321\tvalid_0's ndcg@30: 0.99321\tvalid_0's ndcg@40: 0.99321\tvalid_0's ndcg@50: 0.99321\tvalid_0's ndcg@10: 0.99321\tvalid_0's ndcg@20: 0.99321\tvalid_0's ndcg@30: 0.99321\tvalid_0's ndcg@40: 0.99321\tvalid_0's ndcg@50: 0.99321\n","[43]\tvalid_0's ndcg@10: 0.99321\tvalid_0's ndcg@20: 0.99321\tvalid_0's ndcg@30: 0.99321\tvalid_0's ndcg@40: 0.99321\tvalid_0's ndcg@50: 0.99321\tvalid_0's ndcg@10: 0.99321\tvalid_0's ndcg@20: 0.99321\tvalid_0's ndcg@30: 0.99321\tvalid_0's ndcg@40: 0.99321\tvalid_0's ndcg@50: 0.99321\n","[44]\tvalid_0's ndcg@10: 0.993216\tvalid_0's ndcg@20: 0.993216\tvalid_0's ndcg@30: 0.993216\tvalid_0's ndcg@40: 0.993216\tvalid_0's ndcg@50: 0.993216\tvalid_0's ndcg@10: 0.993216\tvalid_0's ndcg@20: 0.993216\tvalid_0's ndcg@30: 0.993216\tvalid_0's ndcg@40: 0.993216\tvalid_0's ndcg@50: 0.993216\n","[45]\tvalid_0's ndcg@10: 0.993227\tvalid_0's ndcg@20: 0.993227\tvalid_0's ndcg@30: 0.993227\tvalid_0's ndcg@40: 0.993227\tvalid_0's ndcg@50: 0.993227\tvalid_0's ndcg@10: 0.993227\tvalid_0's ndcg@20: 0.993227\tvalid_0's ndcg@30: 0.993227\tvalid_0's ndcg@40: 0.993227\tvalid_0's ndcg@50: 0.993227\n","[46]\tvalid_0's ndcg@10: 0.993255\tvalid_0's ndcg@20: 0.993255\tvalid_0's ndcg@30: 0.993255\tvalid_0's ndcg@40: 0.993255\tvalid_0's ndcg@50: 0.993255\tvalid_0's ndcg@10: 0.993255\tvalid_0's ndcg@20: 0.993255\tvalid_0's ndcg@30: 0.993255\tvalid_0's ndcg@40: 0.993255\tvalid_0's ndcg@50: 0.993255\n","[47]\tvalid_0's ndcg@10: 0.993329\tvalid_0's ndcg@20: 0.993329\tvalid_0's ndcg@30: 0.993329\tvalid_0's ndcg@40: 0.993329\tvalid_0's ndcg@50: 0.993329\tvalid_0's ndcg@10: 0.993329\tvalid_0's ndcg@20: 0.993329\tvalid_0's ndcg@30: 0.993329\tvalid_0's ndcg@40: 0.993329\tvalid_0's ndcg@50: 0.993329\n","[48]\tvalid_0's ndcg@10: 0.993369\tvalid_0's ndcg@20: 0.993369\tvalid_0's ndcg@30: 0.993369\tvalid_0's ndcg@40: 0.993369\tvalid_0's ndcg@50: 0.993369\tvalid_0's ndcg@10: 0.993369\tvalid_0's ndcg@20: 0.993369\tvalid_0's ndcg@30: 0.993369\tvalid_0's ndcg@40: 0.993369\tvalid_0's ndcg@50: 0.993369\n","[49]\tvalid_0's ndcg@10: 0.993396\tvalid_0's ndcg@20: 0.993396\tvalid_0's ndcg@30: 0.993396\tvalid_0's ndcg@40: 0.993396\tvalid_0's ndcg@50: 0.993396\tvalid_0's ndcg@10: 0.993396\tvalid_0's ndcg@20: 0.993396\tvalid_0's ndcg@30: 0.993396\tvalid_0's ndcg@40: 0.993396\tvalid_0's ndcg@50: 0.993396\n","[50]\tvalid_0's ndcg@10: 0.993288\tvalid_0's ndcg@20: 0.993288\tvalid_0's ndcg@30: 0.993288\tvalid_0's ndcg@40: 0.993288\tvalid_0's ndcg@50: 0.993288\tvalid_0's ndcg@10: 0.993288\tvalid_0's ndcg@20: 0.993288\tvalid_0's ndcg@30: 0.993288\tvalid_0's ndcg@40: 0.993288\tvalid_0's ndcg@50: 0.993288\n","[51]\tvalid_0's ndcg@10: 0.993262\tvalid_0's ndcg@20: 0.993262\tvalid_0's ndcg@30: 0.993262\tvalid_0's ndcg@40: 0.993262\tvalid_0's ndcg@50: 0.993262\tvalid_0's ndcg@10: 0.993262\tvalid_0's ndcg@20: 0.993262\tvalid_0's ndcg@30: 0.993262\tvalid_0's ndcg@40: 0.993262\tvalid_0's ndcg@50: 0.993262\n","[52]\tvalid_0's ndcg@10: 0.993313\tvalid_0's ndcg@20: 0.993313\tvalid_0's ndcg@30: 0.993313\tvalid_0's ndcg@40: 0.993313\tvalid_0's ndcg@50: 0.993313\tvalid_0's ndcg@10: 0.993313\tvalid_0's ndcg@20: 0.993313\tvalid_0's ndcg@30: 0.993313\tvalid_0's ndcg@40: 0.993313\tvalid_0's ndcg@50: 0.993313\n","[53]\tvalid_0's ndcg@10: 0.993376\tvalid_0's ndcg@20: 0.993376\tvalid_0's ndcg@30: 0.993376\tvalid_0's ndcg@40: 0.993376\tvalid_0's ndcg@50: 0.993376\tvalid_0's ndcg@10: 0.993376\tvalid_0's ndcg@20: 0.993376\tvalid_0's ndcg@30: 0.993376\tvalid_0's ndcg@40: 0.993376\tvalid_0's ndcg@50: 0.993376\n","[54]\tvalid_0's ndcg@10: 0.993462\tvalid_0's ndcg@20: 0.993462\tvalid_0's ndcg@30: 0.993462\tvalid_0's ndcg@40: 0.993462\tvalid_0's ndcg@50: 0.993462\tvalid_0's ndcg@10: 0.993462\tvalid_0's ndcg@20: 0.993462\tvalid_0's ndcg@30: 0.993462\tvalid_0's ndcg@40: 0.993462\tvalid_0's ndcg@50: 0.993462\n","[55]\tvalid_0's ndcg@10: 0.993399\tvalid_0's ndcg@20: 0.993399\tvalid_0's ndcg@30: 0.993399\tvalid_0's ndcg@40: 0.993399\tvalid_0's ndcg@50: 0.993399\tvalid_0's ndcg@10: 0.993399\tvalid_0's ndcg@20: 0.993399\tvalid_0's ndcg@30: 0.993399\tvalid_0's ndcg@40: 0.993399\tvalid_0's ndcg@50: 0.993399\n","[56]\tvalid_0's ndcg@10: 0.993389\tvalid_0's ndcg@20: 0.993389\tvalid_0's ndcg@30: 0.993389\tvalid_0's ndcg@40: 0.993389\tvalid_0's ndcg@50: 0.993389\tvalid_0's ndcg@10: 0.993389\tvalid_0's ndcg@20: 0.993389\tvalid_0's ndcg@30: 0.993389\tvalid_0's ndcg@40: 0.993389\tvalid_0's ndcg@50: 0.993389\n","[57]\tvalid_0's ndcg@10: 0.993365\tvalid_0's ndcg@20: 0.993365\tvalid_0's ndcg@30: 0.993365\tvalid_0's ndcg@40: 0.993365\tvalid_0's ndcg@50: 0.993365\tvalid_0's ndcg@10: 0.993365\tvalid_0's ndcg@20: 0.993365\tvalid_0's ndcg@30: 0.993365\tvalid_0's ndcg@40: 0.993365\tvalid_0's ndcg@50: 0.993365\n","[58]\tvalid_0's ndcg@10: 0.993387\tvalid_0's ndcg@20: 0.993387\tvalid_0's ndcg@30: 0.993387\tvalid_0's ndcg@40: 0.993387\tvalid_0's ndcg@50: 0.993387\tvalid_0's ndcg@10: 0.993387\tvalid_0's ndcg@20: 0.993387\tvalid_0's ndcg@30: 0.993387\tvalid_0's ndcg@40: 0.993387\tvalid_0's ndcg@50: 0.993387\n","[59]\tvalid_0's ndcg@10: 0.99328\tvalid_0's ndcg@20: 0.99328\tvalid_0's ndcg@30: 0.99328\tvalid_0's ndcg@40: 0.99328\tvalid_0's ndcg@50: 0.99328\tvalid_0's ndcg@10: 0.99328\tvalid_0's ndcg@20: 0.99328\tvalid_0's ndcg@30: 0.99328\tvalid_0's ndcg@40: 0.99328\tvalid_0's ndcg@50: 0.99328\n","[60]\tvalid_0's ndcg@10: 0.993297\tvalid_0's ndcg@20: 0.993297\tvalid_0's ndcg@30: 0.993297\tvalid_0's ndcg@40: 0.993297\tvalid_0's ndcg@50: 0.993297\tvalid_0's ndcg@10: 0.993297\tvalid_0's ndcg@20: 0.993297\tvalid_0's ndcg@30: 0.993297\tvalid_0's ndcg@40: 0.993297\tvalid_0's ndcg@50: 0.993297\n","[61]\tvalid_0's ndcg@10: 0.99328\tvalid_0's ndcg@20: 0.99328\tvalid_0's ndcg@30: 0.99328\tvalid_0's ndcg@40: 0.99328\tvalid_0's ndcg@50: 0.99328\tvalid_0's ndcg@10: 0.99328\tvalid_0's ndcg@20: 0.99328\tvalid_0's ndcg@30: 0.99328\tvalid_0's ndcg@40: 0.99328\tvalid_0's ndcg@50: 0.99328\n","[62]\tvalid_0's ndcg@10: 0.99328\tvalid_0's ndcg@20: 0.99328\tvalid_0's ndcg@30: 0.99328\tvalid_0's ndcg@40: 0.99328\tvalid_0's ndcg@50: 0.99328\tvalid_0's ndcg@10: 0.99328\tvalid_0's ndcg@20: 0.99328\tvalid_0's ndcg@30: 0.99328\tvalid_0's ndcg@40: 0.99328\tvalid_0's ndcg@50: 0.99328\n","[63]\tvalid_0's ndcg@10: 0.993343\tvalid_0's ndcg@20: 0.993343\tvalid_0's ndcg@30: 0.993343\tvalid_0's ndcg@40: 0.993343\tvalid_0's ndcg@50: 0.993343\tvalid_0's ndcg@10: 0.993343\tvalid_0's ndcg@20: 0.993343\tvalid_0's ndcg@30: 0.993343\tvalid_0's ndcg@40: 0.993343\tvalid_0's ndcg@50: 0.993343\n","[64]\tvalid_0's ndcg@10: 0.993343\tvalid_0's ndcg@20: 0.993343\tvalid_0's ndcg@30: 0.993343\tvalid_0's ndcg@40: 0.993343\tvalid_0's ndcg@50: 0.993343\tvalid_0's ndcg@10: 0.993343\tvalid_0's ndcg@20: 0.993343\tvalid_0's ndcg@30: 0.993343\tvalid_0's ndcg@40: 0.993343\tvalid_0's ndcg@50: 0.993343\n","[65]\tvalid_0's ndcg@10: 0.99328\tvalid_0's ndcg@20: 0.99328\tvalid_0's ndcg@30: 0.99328\tvalid_0's ndcg@40: 0.99328\tvalid_0's ndcg@50: 0.99328\tvalid_0's ndcg@10: 0.99328\tvalid_0's ndcg@20: 0.99328\tvalid_0's ndcg@30: 0.99328\tvalid_0's ndcg@40: 0.99328\tvalid_0's ndcg@50: 0.99328\n","[66]\tvalid_0's ndcg@10: 0.993299\tvalid_0's ndcg@20: 0.993299\tvalid_0's ndcg@30: 0.993299\tvalid_0's ndcg@40: 0.993299\tvalid_0's ndcg@50: 0.993299\tvalid_0's ndcg@10: 0.993299\tvalid_0's ndcg@20: 0.993299\tvalid_0's ndcg@30: 0.993299\tvalid_0's ndcg@40: 0.993299\tvalid_0's ndcg@50: 0.993299\n","[67]\tvalid_0's ndcg@10: 0.99328\tvalid_0's ndcg@20: 0.99328\tvalid_0's ndcg@30: 0.99328\tvalid_0's ndcg@40: 0.99328\tvalid_0's ndcg@50: 0.99328\tvalid_0's ndcg@10: 0.99328\tvalid_0's ndcg@20: 0.99328\tvalid_0's ndcg@30: 0.99328\tvalid_0's ndcg@40: 0.99328\tvalid_0's ndcg@50: 0.99328\n","[68]\tvalid_0's ndcg@10: 0.993343\tvalid_0's ndcg@20: 0.993343\tvalid_0's ndcg@30: 0.993343\tvalid_0's ndcg@40: 0.993343\tvalid_0's ndcg@50: 0.993343\tvalid_0's ndcg@10: 0.993343\tvalid_0's ndcg@20: 0.993343\tvalid_0's ndcg@30: 0.993343\tvalid_0's ndcg@40: 0.993343\tvalid_0's ndcg@50: 0.993343\n","[69]\tvalid_0's ndcg@10: 0.993373\tvalid_0's ndcg@20: 0.993373\tvalid_0's ndcg@30: 0.993373\tvalid_0's ndcg@40: 0.993373\tvalid_0's ndcg@50: 0.993373\tvalid_0's ndcg@10: 0.993373\tvalid_0's ndcg@20: 0.993373\tvalid_0's ndcg@30: 0.993373\tvalid_0's ndcg@40: 0.993373\tvalid_0's ndcg@50: 0.993373\n","[70]\tvalid_0's ndcg@10: 0.993355\tvalid_0's ndcg@20: 0.993355\tvalid_0's ndcg@30: 0.993355\tvalid_0's ndcg@40: 0.993355\tvalid_0's ndcg@50: 0.993355\tvalid_0's ndcg@10: 0.993355\tvalid_0's ndcg@20: 0.993355\tvalid_0's ndcg@30: 0.993355\tvalid_0's ndcg@40: 0.993355\tvalid_0's ndcg@50: 0.993355\n","[71]\tvalid_0's ndcg@10: 0.993377\tvalid_0's ndcg@20: 0.993377\tvalid_0's ndcg@30: 0.993377\tvalid_0's ndcg@40: 0.993377\tvalid_0's ndcg@50: 0.993377\tvalid_0's ndcg@10: 0.993377\tvalid_0's ndcg@20: 0.993377\tvalid_0's ndcg@30: 0.993377\tvalid_0's ndcg@40: 0.993377\tvalid_0's ndcg@50: 0.993377\n","[72]\tvalid_0's ndcg@10: 0.993399\tvalid_0's ndcg@20: 0.993399\tvalid_0's ndcg@30: 0.993399\tvalid_0's ndcg@40: 0.993399\tvalid_0's ndcg@50: 0.993399\tvalid_0's ndcg@10: 0.993399\tvalid_0's ndcg@20: 0.993399\tvalid_0's ndcg@30: 0.993399\tvalid_0's ndcg@40: 0.993399\tvalid_0's ndcg@50: 0.993399\n","[73]\tvalid_0's ndcg@10: 0.993263\tvalid_0's ndcg@20: 0.993263\tvalid_0's ndcg@30: 0.993263\tvalid_0's ndcg@40: 0.993263\tvalid_0's ndcg@50: 0.993263\tvalid_0's ndcg@10: 0.993263\tvalid_0's ndcg@20: 0.993263\tvalid_0's ndcg@30: 0.993263\tvalid_0's ndcg@40: 0.993263\tvalid_0's ndcg@50: 0.993263\n","[74]\tvalid_0's ndcg@10: 0.99324\tvalid_0's ndcg@20: 0.99324\tvalid_0's ndcg@30: 0.99324\tvalid_0's ndcg@40: 0.99324\tvalid_0's ndcg@50: 0.99324\tvalid_0's ndcg@10: 0.99324\tvalid_0's ndcg@20: 0.99324\tvalid_0's ndcg@30: 0.99324\tvalid_0's ndcg@40: 0.99324\tvalid_0's ndcg@50: 0.99324\n","[75]\tvalid_0's ndcg@10: 0.99331\tvalid_0's ndcg@20: 0.99331\tvalid_0's ndcg@30: 0.99331\tvalid_0's ndcg@40: 0.99331\tvalid_0's ndcg@50: 0.99331\tvalid_0's ndcg@10: 0.99331\tvalid_0's ndcg@20: 0.99331\tvalid_0's ndcg@30: 0.99331\tvalid_0's ndcg@40: 0.99331\tvalid_0's ndcg@50: 0.99331\n","Early stopping, best iteration is:\n","[25]\tvalid_0's ndcg@10: 0.993583\tvalid_0's ndcg@20: 0.993583\tvalid_0's ndcg@30: 0.993583\tvalid_0's ndcg@40: 0.993583\tvalid_0's ndcg@50: 0.993583\tvalid_0's ndcg@10: 0.993583\tvalid_0's ndcg@20: 0.993583\tvalid_0's ndcg@30: 0.993583\tvalid_0's ndcg@40: 0.993583\tvalid_0's ndcg@50: 0.993583\n","[1]\tvalid_0's ndcg@10: 0.989821\tvalid_0's ndcg@20: 0.989867\tvalid_0's ndcg@30: 0.989867\tvalid_0's ndcg@40: 0.989867\tvalid_0's ndcg@50: 0.989867\tvalid_0's ndcg@10: 0.989821\tvalid_0's ndcg@20: 0.989867\tvalid_0's ndcg@30: 0.989867\tvalid_0's ndcg@40: 0.989867\tvalid_0's ndcg@50: 0.989867\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.990329\tvalid_0's ndcg@20: 0.990375\tvalid_0's ndcg@30: 0.990375\tvalid_0's ndcg@40: 0.990375\tvalid_0's ndcg@50: 0.990375\tvalid_0's ndcg@10: 0.990329\tvalid_0's ndcg@20: 0.990375\tvalid_0's ndcg@30: 0.990375\tvalid_0's ndcg@40: 0.990375\tvalid_0's ndcg@50: 0.990375\n","[3]\tvalid_0's ndcg@10: 0.991069\tvalid_0's ndcg@20: 0.991115\tvalid_0's ndcg@30: 0.991115\tvalid_0's ndcg@40: 0.991115\tvalid_0's ndcg@50: 0.991115\tvalid_0's ndcg@10: 0.991069\tvalid_0's ndcg@20: 0.991115\tvalid_0's ndcg@30: 0.991115\tvalid_0's ndcg@40: 0.991115\tvalid_0's ndcg@50: 0.991115\n","[4]\tvalid_0's ndcg@10: 0.991172\tvalid_0's ndcg@20: 0.991215\tvalid_0's ndcg@30: 0.991215\tvalid_0's ndcg@40: 0.991215\tvalid_0's ndcg@50: 0.991215\tvalid_0's ndcg@10: 0.991172\tvalid_0's ndcg@20: 0.991215\tvalid_0's ndcg@30: 0.991215\tvalid_0's ndcg@40: 0.991215\tvalid_0's ndcg@50: 0.991215\n","[5]\tvalid_0's ndcg@10: 0.991312\tvalid_0's ndcg@20: 0.991356\tvalid_0's ndcg@30: 0.991356\tvalid_0's ndcg@40: 0.991356\tvalid_0's ndcg@50: 0.991356\tvalid_0's ndcg@10: 0.991312\tvalid_0's ndcg@20: 0.991356\tvalid_0's ndcg@30: 0.991356\tvalid_0's ndcg@40: 0.991356\tvalid_0's ndcg@50: 0.991356\n","[6]\tvalid_0's ndcg@10: 0.991282\tvalid_0's ndcg@20: 0.991327\tvalid_0's ndcg@30: 0.991327\tvalid_0's ndcg@40: 0.991327\tvalid_0's ndcg@50: 0.991327\tvalid_0's ndcg@10: 0.991282\tvalid_0's ndcg@20: 0.991327\tvalid_0's ndcg@30: 0.991327\tvalid_0's ndcg@40: 0.991327\tvalid_0's ndcg@50: 0.991327\n","[7]\tvalid_0's ndcg@10: 0.991486\tvalid_0's ndcg@20: 0.99153\tvalid_0's ndcg@30: 0.99153\tvalid_0's ndcg@40: 0.99153\tvalid_0's ndcg@50: 0.99153\tvalid_0's ndcg@10: 0.991486\tvalid_0's ndcg@20: 0.99153\tvalid_0's ndcg@30: 0.99153\tvalid_0's ndcg@40: 0.99153\tvalid_0's ndcg@50: 0.99153\n","[8]\tvalid_0's ndcg@10: 0.991724\tvalid_0's ndcg@20: 0.991768\tvalid_0's ndcg@30: 0.991768\tvalid_0's ndcg@40: 0.991768\tvalid_0's ndcg@50: 0.991768\tvalid_0's ndcg@10: 0.991724\tvalid_0's ndcg@20: 0.991768\tvalid_0's ndcg@30: 0.991768\tvalid_0's ndcg@40: 0.991768\tvalid_0's ndcg@50: 0.991768\n","[9]\tvalid_0's ndcg@10: 0.991642\tvalid_0's ndcg@20: 0.991686\tvalid_0's ndcg@30: 0.991686\tvalid_0's ndcg@40: 0.991686\tvalid_0's ndcg@50: 0.991686\tvalid_0's ndcg@10: 0.991642\tvalid_0's ndcg@20: 0.991686\tvalid_0's ndcg@30: 0.991686\tvalid_0's ndcg@40: 0.991686\tvalid_0's ndcg@50: 0.991686\n","[10]\tvalid_0's ndcg@10: 0.991647\tvalid_0's ndcg@20: 0.991692\tvalid_0's ndcg@30: 0.991692\tvalid_0's ndcg@40: 0.991692\tvalid_0's ndcg@50: 0.991692\tvalid_0's ndcg@10: 0.991647\tvalid_0's ndcg@20: 0.991692\tvalid_0's ndcg@30: 0.991692\tvalid_0's ndcg@40: 0.991692\tvalid_0's ndcg@50: 0.991692\n","[11]\tvalid_0's ndcg@10: 0.991658\tvalid_0's ndcg@20: 0.991703\tvalid_0's ndcg@30: 0.991703\tvalid_0's ndcg@40: 0.991703\tvalid_0's ndcg@50: 0.991703\tvalid_0's ndcg@10: 0.991658\tvalid_0's ndcg@20: 0.991703\tvalid_0's ndcg@30: 0.991703\tvalid_0's ndcg@40: 0.991703\tvalid_0's ndcg@50: 0.991703\n","[12]\tvalid_0's ndcg@10: 0.991605\tvalid_0's ndcg@20: 0.99165\tvalid_0's ndcg@30: 0.99165\tvalid_0's ndcg@40: 0.99165\tvalid_0's ndcg@50: 0.99165\tvalid_0's ndcg@10: 0.991605\tvalid_0's ndcg@20: 0.99165\tvalid_0's ndcg@30: 0.99165\tvalid_0's ndcg@40: 0.99165\tvalid_0's ndcg@50: 0.99165\n","[13]\tvalid_0's ndcg@10: 0.991657\tvalid_0's ndcg@20: 0.991703\tvalid_0's ndcg@30: 0.991703\tvalid_0's ndcg@40: 0.991703\tvalid_0's ndcg@50: 0.991703\tvalid_0's ndcg@10: 0.991657\tvalid_0's ndcg@20: 0.991703\tvalid_0's ndcg@30: 0.991703\tvalid_0's ndcg@40: 0.991703\tvalid_0's ndcg@50: 0.991703\n","[14]\tvalid_0's ndcg@10: 0.991546\tvalid_0's ndcg@20: 0.991592\tvalid_0's ndcg@30: 0.991592\tvalid_0's ndcg@40: 0.991592\tvalid_0's ndcg@50: 0.991592\tvalid_0's ndcg@10: 0.991546\tvalid_0's ndcg@20: 0.991592\tvalid_0's ndcg@30: 0.991592\tvalid_0's ndcg@40: 0.991592\tvalid_0's ndcg@50: 0.991592\n","[15]\tvalid_0's ndcg@10: 0.991506\tvalid_0's ndcg@20: 0.991554\tvalid_0's ndcg@30: 0.991554\tvalid_0's ndcg@40: 0.991554\tvalid_0's ndcg@50: 0.991554\tvalid_0's ndcg@10: 0.991506\tvalid_0's ndcg@20: 0.991554\tvalid_0's ndcg@30: 0.991554\tvalid_0's ndcg@40: 0.991554\tvalid_0's ndcg@50: 0.991554\n","[16]\tvalid_0's ndcg@10: 0.991534\tvalid_0's ndcg@20: 0.991534\tvalid_0's ndcg@30: 0.991534\tvalid_0's ndcg@40: 0.991534\tvalid_0's ndcg@50: 0.991534\tvalid_0's ndcg@10: 0.991534\tvalid_0's ndcg@20: 0.991534\tvalid_0's ndcg@30: 0.991534\tvalid_0's ndcg@40: 0.991534\tvalid_0's ndcg@50: 0.991534\n","[17]\tvalid_0's ndcg@10: 0.991704\tvalid_0's ndcg@20: 0.991704\tvalid_0's ndcg@30: 0.991704\tvalid_0's ndcg@40: 0.991704\tvalid_0's ndcg@50: 0.991704\tvalid_0's ndcg@10: 0.991704\tvalid_0's ndcg@20: 0.991704\tvalid_0's ndcg@30: 0.991704\tvalid_0's ndcg@40: 0.991704\tvalid_0's ndcg@50: 0.991704\n","[18]\tvalid_0's ndcg@10: 0.991727\tvalid_0's ndcg@20: 0.991727\tvalid_0's ndcg@30: 0.991727\tvalid_0's ndcg@40: 0.991727\tvalid_0's ndcg@50: 0.991727\tvalid_0's ndcg@10: 0.991727\tvalid_0's ndcg@20: 0.991727\tvalid_0's ndcg@30: 0.991727\tvalid_0's ndcg@40: 0.991727\tvalid_0's ndcg@50: 0.991727\n","[19]\tvalid_0's ndcg@10: 0.991723\tvalid_0's ndcg@20: 0.991723\tvalid_0's ndcg@30: 0.991723\tvalid_0's ndcg@40: 0.991723\tvalid_0's ndcg@50: 0.991723\tvalid_0's ndcg@10: 0.991723\tvalid_0's ndcg@20: 0.991723\tvalid_0's ndcg@30: 0.991723\tvalid_0's ndcg@40: 0.991723\tvalid_0's ndcg@50: 0.991723\n","[20]\tvalid_0's ndcg@10: 0.9918\tvalid_0's ndcg@20: 0.9918\tvalid_0's ndcg@30: 0.9918\tvalid_0's ndcg@40: 0.9918\tvalid_0's ndcg@50: 0.9918\tvalid_0's ndcg@10: 0.9918\tvalid_0's ndcg@20: 0.9918\tvalid_0's ndcg@30: 0.9918\tvalid_0's ndcg@40: 0.9918\tvalid_0's ndcg@50: 0.9918\n","[21]\tvalid_0's ndcg@10: 0.991663\tvalid_0's ndcg@20: 0.991663\tvalid_0's ndcg@30: 0.991663\tvalid_0's ndcg@40: 0.991663\tvalid_0's ndcg@50: 0.991663\tvalid_0's ndcg@10: 0.991663\tvalid_0's ndcg@20: 0.991663\tvalid_0's ndcg@30: 0.991663\tvalid_0's ndcg@40: 0.991663\tvalid_0's ndcg@50: 0.991663\n","[22]\tvalid_0's ndcg@10: 0.991882\tvalid_0's ndcg@20: 0.991882\tvalid_0's ndcg@30: 0.991882\tvalid_0's ndcg@40: 0.991882\tvalid_0's ndcg@50: 0.991882\tvalid_0's ndcg@10: 0.991882\tvalid_0's ndcg@20: 0.991882\tvalid_0's ndcg@30: 0.991882\tvalid_0's ndcg@40: 0.991882\tvalid_0's ndcg@50: 0.991882\n","[23]\tvalid_0's ndcg@10: 0.991819\tvalid_0's ndcg@20: 0.991819\tvalid_0's ndcg@30: 0.991819\tvalid_0's ndcg@40: 0.991819\tvalid_0's ndcg@50: 0.991819\tvalid_0's ndcg@10: 0.991819\tvalid_0's ndcg@20: 0.991819\tvalid_0's ndcg@30: 0.991819\tvalid_0's ndcg@40: 0.991819\tvalid_0's ndcg@50: 0.991819\n","[24]\tvalid_0's ndcg@10: 0.991836\tvalid_0's ndcg@20: 0.991836\tvalid_0's ndcg@30: 0.991836\tvalid_0's ndcg@40: 0.991836\tvalid_0's ndcg@50: 0.991836\tvalid_0's ndcg@10: 0.991836\tvalid_0's ndcg@20: 0.991836\tvalid_0's ndcg@30: 0.991836\tvalid_0's ndcg@40: 0.991836\tvalid_0's ndcg@50: 0.991836\n","[25]\tvalid_0's ndcg@10: 0.99187\tvalid_0's ndcg@20: 0.99187\tvalid_0's ndcg@30: 0.99187\tvalid_0's ndcg@40: 0.99187\tvalid_0's ndcg@50: 0.99187\tvalid_0's ndcg@10: 0.99187\tvalid_0's ndcg@20: 0.99187\tvalid_0's ndcg@30: 0.99187\tvalid_0's ndcg@40: 0.99187\tvalid_0's ndcg@50: 0.99187\n","[26]\tvalid_0's ndcg@10: 0.991939\tvalid_0's ndcg@20: 0.991939\tvalid_0's ndcg@30: 0.991939\tvalid_0's ndcg@40: 0.991939\tvalid_0's ndcg@50: 0.991939\tvalid_0's ndcg@10: 0.991939\tvalid_0's ndcg@20: 0.991939\tvalid_0's ndcg@30: 0.991939\tvalid_0's ndcg@40: 0.991939\tvalid_0's ndcg@50: 0.991939\n","[27]\tvalid_0's ndcg@10: 0.991967\tvalid_0's ndcg@20: 0.991967\tvalid_0's ndcg@30: 0.991967\tvalid_0's ndcg@40: 0.991967\tvalid_0's ndcg@50: 0.991967\tvalid_0's ndcg@10: 0.991967\tvalid_0's ndcg@20: 0.991967\tvalid_0's ndcg@30: 0.991967\tvalid_0's ndcg@40: 0.991967\tvalid_0's ndcg@50: 0.991967\n","[28]\tvalid_0's ndcg@10: 0.991937\tvalid_0's ndcg@20: 0.991937\tvalid_0's ndcg@30: 0.991937\tvalid_0's ndcg@40: 0.991937\tvalid_0's ndcg@50: 0.991937\tvalid_0's ndcg@10: 0.991937\tvalid_0's ndcg@20: 0.991937\tvalid_0's ndcg@30: 0.991937\tvalid_0's ndcg@40: 0.991937\tvalid_0's ndcg@50: 0.991937\n","[29]\tvalid_0's ndcg@10: 0.991952\tvalid_0's ndcg@20: 0.991952\tvalid_0's ndcg@30: 0.991952\tvalid_0's ndcg@40: 0.991952\tvalid_0's ndcg@50: 0.991952\tvalid_0's ndcg@10: 0.991952\tvalid_0's ndcg@20: 0.991952\tvalid_0's ndcg@30: 0.991952\tvalid_0's ndcg@40: 0.991952\tvalid_0's ndcg@50: 0.991952\n","[30]\tvalid_0's ndcg@10: 0.991939\tvalid_0's ndcg@20: 0.991939\tvalid_0's ndcg@30: 0.991939\tvalid_0's ndcg@40: 0.991939\tvalid_0's ndcg@50: 0.991939\tvalid_0's ndcg@10: 0.991939\tvalid_0's ndcg@20: 0.991939\tvalid_0's ndcg@30: 0.991939\tvalid_0's ndcg@40: 0.991939\tvalid_0's ndcg@50: 0.991939\n","[31]\tvalid_0's ndcg@10: 0.991861\tvalid_0's ndcg@20: 0.991861\tvalid_0's ndcg@30: 0.991861\tvalid_0's ndcg@40: 0.991861\tvalid_0's ndcg@50: 0.991861\tvalid_0's ndcg@10: 0.991861\tvalid_0's ndcg@20: 0.991861\tvalid_0's ndcg@30: 0.991861\tvalid_0's ndcg@40: 0.991861\tvalid_0's ndcg@50: 0.991861\n","[32]\tvalid_0's ndcg@10: 0.991813\tvalid_0's ndcg@20: 0.991813\tvalid_0's ndcg@30: 0.991813\tvalid_0's ndcg@40: 0.991813\tvalid_0's ndcg@50: 0.991813\tvalid_0's ndcg@10: 0.991813\tvalid_0's ndcg@20: 0.991813\tvalid_0's ndcg@30: 0.991813\tvalid_0's ndcg@40: 0.991813\tvalid_0's ndcg@50: 0.991813\n","[33]\tvalid_0's ndcg@10: 0.991972\tvalid_0's ndcg@20: 0.991972\tvalid_0's ndcg@30: 0.991972\tvalid_0's ndcg@40: 0.991972\tvalid_0's ndcg@50: 0.991972\tvalid_0's ndcg@10: 0.991972\tvalid_0's ndcg@20: 0.991972\tvalid_0's ndcg@30: 0.991972\tvalid_0's ndcg@40: 0.991972\tvalid_0's ndcg@50: 0.991972\n","[34]\tvalid_0's ndcg@10: 0.991999\tvalid_0's ndcg@20: 0.991999\tvalid_0's ndcg@30: 0.991999\tvalid_0's ndcg@40: 0.991999\tvalid_0's ndcg@50: 0.991999\tvalid_0's ndcg@10: 0.991999\tvalid_0's ndcg@20: 0.991999\tvalid_0's ndcg@30: 0.991999\tvalid_0's ndcg@40: 0.991999\tvalid_0's ndcg@50: 0.991999\n","[35]\tvalid_0's ndcg@10: 0.992062\tvalid_0's ndcg@20: 0.992062\tvalid_0's ndcg@30: 0.992062\tvalid_0's ndcg@40: 0.992062\tvalid_0's ndcg@50: 0.992062\tvalid_0's ndcg@10: 0.992062\tvalid_0's ndcg@20: 0.992062\tvalid_0's ndcg@30: 0.992062\tvalid_0's ndcg@40: 0.992062\tvalid_0's ndcg@50: 0.992062\n","[36]\tvalid_0's ndcg@10: 0.991971\tvalid_0's ndcg@20: 0.991971\tvalid_0's ndcg@30: 0.991971\tvalid_0's ndcg@40: 0.991971\tvalid_0's ndcg@50: 0.991971\tvalid_0's ndcg@10: 0.991971\tvalid_0's ndcg@20: 0.991971\tvalid_0's ndcg@30: 0.991971\tvalid_0's ndcg@40: 0.991971\tvalid_0's ndcg@50: 0.991971\n","[37]\tvalid_0's ndcg@10: 0.991952\tvalid_0's ndcg@20: 0.991952\tvalid_0's ndcg@30: 0.991952\tvalid_0's ndcg@40: 0.991952\tvalid_0's ndcg@50: 0.991952\tvalid_0's ndcg@10: 0.991952\tvalid_0's ndcg@20: 0.991952\tvalid_0's ndcg@30: 0.991952\tvalid_0's ndcg@40: 0.991952\tvalid_0's ndcg@50: 0.991952\n","[38]\tvalid_0's ndcg@10: 0.992009\tvalid_0's ndcg@20: 0.992009\tvalid_0's ndcg@30: 0.992009\tvalid_0's ndcg@40: 0.992009\tvalid_0's ndcg@50: 0.992009\tvalid_0's ndcg@10: 0.992009\tvalid_0's ndcg@20: 0.992009\tvalid_0's ndcg@30: 0.992009\tvalid_0's ndcg@40: 0.992009\tvalid_0's ndcg@50: 0.992009\n","[39]\tvalid_0's ndcg@10: 0.991886\tvalid_0's ndcg@20: 0.991886\tvalid_0's ndcg@30: 0.991886\tvalid_0's ndcg@40: 0.991886\tvalid_0's ndcg@50: 0.991886\tvalid_0's ndcg@10: 0.991886\tvalid_0's ndcg@20: 0.991886\tvalid_0's ndcg@30: 0.991886\tvalid_0's ndcg@40: 0.991886\tvalid_0's ndcg@50: 0.991886\n","[40]\tvalid_0's ndcg@10: 0.991876\tvalid_0's ndcg@20: 0.991876\tvalid_0's ndcg@30: 0.991876\tvalid_0's ndcg@40: 0.991876\tvalid_0's ndcg@50: 0.991876\tvalid_0's ndcg@10: 0.991876\tvalid_0's ndcg@20: 0.991876\tvalid_0's ndcg@30: 0.991876\tvalid_0's ndcg@40: 0.991876\tvalid_0's ndcg@50: 0.991876\n","[41]\tvalid_0's ndcg@10: 0.991853\tvalid_0's ndcg@20: 0.991853\tvalid_0's ndcg@30: 0.991853\tvalid_0's ndcg@40: 0.991853\tvalid_0's ndcg@50: 0.991853\tvalid_0's ndcg@10: 0.991853\tvalid_0's ndcg@20: 0.991853\tvalid_0's ndcg@30: 0.991853\tvalid_0's ndcg@40: 0.991853\tvalid_0's ndcg@50: 0.991853\n","[42]\tvalid_0's ndcg@10: 0.991914\tvalid_0's ndcg@20: 0.991914\tvalid_0's ndcg@30: 0.991914\tvalid_0's ndcg@40: 0.991914\tvalid_0's ndcg@50: 0.991914\tvalid_0's ndcg@10: 0.991914\tvalid_0's ndcg@20: 0.991914\tvalid_0's ndcg@30: 0.991914\tvalid_0's ndcg@40: 0.991914\tvalid_0's ndcg@50: 0.991914\n","[43]\tvalid_0's ndcg@10: 0.991999\tvalid_0's ndcg@20: 0.991999\tvalid_0's ndcg@30: 0.991999\tvalid_0's ndcg@40: 0.991999\tvalid_0's ndcg@50: 0.991999\tvalid_0's ndcg@10: 0.991999\tvalid_0's ndcg@20: 0.991999\tvalid_0's ndcg@30: 0.991999\tvalid_0's ndcg@40: 0.991999\tvalid_0's ndcg@50: 0.991999\n","[44]\tvalid_0's ndcg@10: 0.991946\tvalid_0's ndcg@20: 0.991946\tvalid_0's ndcg@30: 0.991946\tvalid_0's ndcg@40: 0.991946\tvalid_0's ndcg@50: 0.991946\tvalid_0's ndcg@10: 0.991946\tvalid_0's ndcg@20: 0.991946\tvalid_0's ndcg@30: 0.991946\tvalid_0's ndcg@40: 0.991946\tvalid_0's ndcg@50: 0.991946\n","[45]\tvalid_0's ndcg@10: 0.992014\tvalid_0's ndcg@20: 0.992014\tvalid_0's ndcg@30: 0.992014\tvalid_0's ndcg@40: 0.992014\tvalid_0's ndcg@50: 0.992014\tvalid_0's ndcg@10: 0.992014\tvalid_0's ndcg@20: 0.992014\tvalid_0's ndcg@30: 0.992014\tvalid_0's ndcg@40: 0.992014\tvalid_0's ndcg@50: 0.992014\n","[46]\tvalid_0's ndcg@10: 0.992035\tvalid_0's ndcg@20: 0.992035\tvalid_0's ndcg@30: 0.992035\tvalid_0's ndcg@40: 0.992035\tvalid_0's ndcg@50: 0.992035\tvalid_0's ndcg@10: 0.992035\tvalid_0's ndcg@20: 0.992035\tvalid_0's ndcg@30: 0.992035\tvalid_0's ndcg@40: 0.992035\tvalid_0's ndcg@50: 0.992035\n","[47]\tvalid_0's ndcg@10: 0.992025\tvalid_0's ndcg@20: 0.992025\tvalid_0's ndcg@30: 0.992025\tvalid_0's ndcg@40: 0.992025\tvalid_0's ndcg@50: 0.992025\tvalid_0's ndcg@10: 0.992025\tvalid_0's ndcg@20: 0.992025\tvalid_0's ndcg@30: 0.992025\tvalid_0's ndcg@40: 0.992025\tvalid_0's ndcg@50: 0.992025\n","[48]\tvalid_0's ndcg@10: 0.992028\tvalid_0's ndcg@20: 0.992028\tvalid_0's ndcg@30: 0.992028\tvalid_0's ndcg@40: 0.992028\tvalid_0's ndcg@50: 0.992028\tvalid_0's ndcg@10: 0.992028\tvalid_0's ndcg@20: 0.992028\tvalid_0's ndcg@30: 0.992028\tvalid_0's ndcg@40: 0.992028\tvalid_0's ndcg@50: 0.992028\n","[49]\tvalid_0's ndcg@10: 0.991987\tvalid_0's ndcg@20: 0.991987\tvalid_0's ndcg@30: 0.991987\tvalid_0's ndcg@40: 0.991987\tvalid_0's ndcg@50: 0.991987\tvalid_0's ndcg@10: 0.991987\tvalid_0's ndcg@20: 0.991987\tvalid_0's ndcg@30: 0.991987\tvalid_0's ndcg@40: 0.991987\tvalid_0's ndcg@50: 0.991987\n","[50]\tvalid_0's ndcg@10: 0.992001\tvalid_0's ndcg@20: 0.992001\tvalid_0's ndcg@30: 0.992001\tvalid_0's ndcg@40: 0.992001\tvalid_0's ndcg@50: 0.992001\tvalid_0's ndcg@10: 0.992001\tvalid_0's ndcg@20: 0.992001\tvalid_0's ndcg@30: 0.992001\tvalid_0's ndcg@40: 0.992001\tvalid_0's ndcg@50: 0.992001\n","[51]\tvalid_0's ndcg@10: 0.99199\tvalid_0's ndcg@20: 0.99199\tvalid_0's ndcg@30: 0.99199\tvalid_0's ndcg@40: 0.99199\tvalid_0's ndcg@50: 0.99199\tvalid_0's ndcg@10: 0.99199\tvalid_0's ndcg@20: 0.99199\tvalid_0's ndcg@30: 0.99199\tvalid_0's ndcg@40: 0.99199\tvalid_0's ndcg@50: 0.99199\n","[52]\tvalid_0's ndcg@10: 0.991925\tvalid_0's ndcg@20: 0.991925\tvalid_0's ndcg@30: 0.991925\tvalid_0's ndcg@40: 0.991925\tvalid_0's ndcg@50: 0.991925\tvalid_0's ndcg@10: 0.991925\tvalid_0's ndcg@20: 0.991925\tvalid_0's ndcg@30: 0.991925\tvalid_0's ndcg@40: 0.991925\tvalid_0's ndcg@50: 0.991925\n","[53]\tvalid_0's ndcg@10: 0.991858\tvalid_0's ndcg@20: 0.991858\tvalid_0's ndcg@30: 0.991858\tvalid_0's ndcg@40: 0.991858\tvalid_0's ndcg@50: 0.991858\tvalid_0's ndcg@10: 0.991858\tvalid_0's ndcg@20: 0.991858\tvalid_0's ndcg@30: 0.991858\tvalid_0's ndcg@40: 0.991858\tvalid_0's ndcg@50: 0.991858\n","[54]\tvalid_0's ndcg@10: 0.99186\tvalid_0's ndcg@20: 0.99186\tvalid_0's ndcg@30: 0.99186\tvalid_0's ndcg@40: 0.99186\tvalid_0's ndcg@50: 0.99186\tvalid_0's ndcg@10: 0.99186\tvalid_0's ndcg@20: 0.99186\tvalid_0's ndcg@30: 0.99186\tvalid_0's ndcg@40: 0.99186\tvalid_0's ndcg@50: 0.99186\n","[55]\tvalid_0's ndcg@10: 0.991875\tvalid_0's ndcg@20: 0.991875\tvalid_0's ndcg@30: 0.991875\tvalid_0's ndcg@40: 0.991875\tvalid_0's ndcg@50: 0.991875\tvalid_0's ndcg@10: 0.991875\tvalid_0's ndcg@20: 0.991875\tvalid_0's ndcg@30: 0.991875\tvalid_0's ndcg@40: 0.991875\tvalid_0's ndcg@50: 0.991875\n","[56]\tvalid_0's ndcg@10: 0.991968\tvalid_0's ndcg@20: 0.991968\tvalid_0's ndcg@30: 0.991968\tvalid_0's ndcg@40: 0.991968\tvalid_0's ndcg@50: 0.991968\tvalid_0's ndcg@10: 0.991968\tvalid_0's ndcg@20: 0.991968\tvalid_0's ndcg@30: 0.991968\tvalid_0's ndcg@40: 0.991968\tvalid_0's ndcg@50: 0.991968\n","[57]\tvalid_0's ndcg@10: 0.991968\tvalid_0's ndcg@20: 0.991968\tvalid_0's ndcg@30: 0.991968\tvalid_0's ndcg@40: 0.991968\tvalid_0's ndcg@50: 0.991968\tvalid_0's ndcg@10: 0.991968\tvalid_0's ndcg@20: 0.991968\tvalid_0's ndcg@30: 0.991968\tvalid_0's ndcg@40: 0.991968\tvalid_0's ndcg@50: 0.991968\n","[58]\tvalid_0's ndcg@10: 0.992008\tvalid_0's ndcg@20: 0.992008\tvalid_0's ndcg@30: 0.992008\tvalid_0's ndcg@40: 0.992008\tvalid_0's ndcg@50: 0.992008\tvalid_0's ndcg@10: 0.992008\tvalid_0's ndcg@20: 0.992008\tvalid_0's ndcg@30: 0.992008\tvalid_0's ndcg@40: 0.992008\tvalid_0's ndcg@50: 0.992008\n","[59]\tvalid_0's ndcg@10: 0.99196\tvalid_0's ndcg@20: 0.99196\tvalid_0's ndcg@30: 0.99196\tvalid_0's ndcg@40: 0.99196\tvalid_0's ndcg@50: 0.99196\tvalid_0's ndcg@10: 0.99196\tvalid_0's ndcg@20: 0.99196\tvalid_0's ndcg@30: 0.99196\tvalid_0's ndcg@40: 0.99196\tvalid_0's ndcg@50: 0.99196\n","[60]\tvalid_0's ndcg@10: 0.992055\tvalid_0's ndcg@20: 0.992055\tvalid_0's ndcg@30: 0.992055\tvalid_0's ndcg@40: 0.992055\tvalid_0's ndcg@50: 0.992055\tvalid_0's ndcg@10: 0.992055\tvalid_0's ndcg@20: 0.992055\tvalid_0's ndcg@30: 0.992055\tvalid_0's ndcg@40: 0.992055\tvalid_0's ndcg@50: 0.992055\n","[61]\tvalid_0's ndcg@10: 0.992077\tvalid_0's ndcg@20: 0.992077\tvalid_0's ndcg@30: 0.992077\tvalid_0's ndcg@40: 0.992077\tvalid_0's ndcg@50: 0.992077\tvalid_0's ndcg@10: 0.992077\tvalid_0's ndcg@20: 0.992077\tvalid_0's ndcg@30: 0.992077\tvalid_0's ndcg@40: 0.992077\tvalid_0's ndcg@50: 0.992077\n","[62]\tvalid_0's ndcg@10: 0.992075\tvalid_0's ndcg@20: 0.992075\tvalid_0's ndcg@30: 0.992075\tvalid_0's ndcg@40: 0.992075\tvalid_0's ndcg@50: 0.992075\tvalid_0's ndcg@10: 0.992075\tvalid_0's ndcg@20: 0.992075\tvalid_0's ndcg@30: 0.992075\tvalid_0's ndcg@40: 0.992075\tvalid_0's ndcg@50: 0.992075\n","[63]\tvalid_0's ndcg@10: 0.992202\tvalid_0's ndcg@20: 0.992202\tvalid_0's ndcg@30: 0.992202\tvalid_0's ndcg@40: 0.992202\tvalid_0's ndcg@50: 0.992202\tvalid_0's ndcg@10: 0.992202\tvalid_0's ndcg@20: 0.992202\tvalid_0's ndcg@30: 0.992202\tvalid_0's ndcg@40: 0.992202\tvalid_0's ndcg@50: 0.992202\n","[64]\tvalid_0's ndcg@10: 0.992173\tvalid_0's ndcg@20: 0.992173\tvalid_0's ndcg@30: 0.992173\tvalid_0's ndcg@40: 0.992173\tvalid_0's ndcg@50: 0.992173\tvalid_0's ndcg@10: 0.992173\tvalid_0's ndcg@20: 0.992173\tvalid_0's ndcg@30: 0.992173\tvalid_0's ndcg@40: 0.992173\tvalid_0's ndcg@50: 0.992173\n","[65]\tvalid_0's ndcg@10: 0.992093\tvalid_0's ndcg@20: 0.992093\tvalid_0's ndcg@30: 0.992093\tvalid_0's ndcg@40: 0.992093\tvalid_0's ndcg@50: 0.992093\tvalid_0's ndcg@10: 0.992093\tvalid_0's ndcg@20: 0.992093\tvalid_0's ndcg@30: 0.992093\tvalid_0's ndcg@40: 0.992093\tvalid_0's ndcg@50: 0.992093\n","[66]\tvalid_0's ndcg@10: 0.992092\tvalid_0's ndcg@20: 0.992092\tvalid_0's ndcg@30: 0.992092\tvalid_0's ndcg@40: 0.992092\tvalid_0's ndcg@50: 0.992092\tvalid_0's ndcg@10: 0.992092\tvalid_0's ndcg@20: 0.992092\tvalid_0's ndcg@30: 0.992092\tvalid_0's ndcg@40: 0.992092\tvalid_0's ndcg@50: 0.992092\n","[67]\tvalid_0's ndcg@10: 0.99207\tvalid_0's ndcg@20: 0.99207\tvalid_0's ndcg@30: 0.99207\tvalid_0's ndcg@40: 0.99207\tvalid_0's ndcg@50: 0.99207\tvalid_0's ndcg@10: 0.99207\tvalid_0's ndcg@20: 0.99207\tvalid_0's ndcg@30: 0.99207\tvalid_0's ndcg@40: 0.99207\tvalid_0's ndcg@50: 0.99207\n","[68]\tvalid_0's ndcg@10: 0.992085\tvalid_0's ndcg@20: 0.992085\tvalid_0's ndcg@30: 0.992085\tvalid_0's ndcg@40: 0.992085\tvalid_0's ndcg@50: 0.992085\tvalid_0's ndcg@10: 0.992085\tvalid_0's ndcg@20: 0.992085\tvalid_0's ndcg@30: 0.992085\tvalid_0's ndcg@40: 0.992085\tvalid_0's ndcg@50: 0.992085\n","[69]\tvalid_0's ndcg@10: 0.992077\tvalid_0's ndcg@20: 0.992077\tvalid_0's ndcg@30: 0.992077\tvalid_0's ndcg@40: 0.992077\tvalid_0's ndcg@50: 0.992077\tvalid_0's ndcg@10: 0.992077\tvalid_0's ndcg@20: 0.992077\tvalid_0's ndcg@30: 0.992077\tvalid_0's ndcg@40: 0.992077\tvalid_0's ndcg@50: 0.992077\n","[70]\tvalid_0's ndcg@10: 0.992022\tvalid_0's ndcg@20: 0.992022\tvalid_0's ndcg@30: 0.992022\tvalid_0's ndcg@40: 0.992022\tvalid_0's ndcg@50: 0.992022\tvalid_0's ndcg@10: 0.992022\tvalid_0's ndcg@20: 0.992022\tvalid_0's ndcg@30: 0.992022\tvalid_0's ndcg@40: 0.992022\tvalid_0's ndcg@50: 0.992022\n","[71]\tvalid_0's ndcg@10: 0.992044\tvalid_0's ndcg@20: 0.992044\tvalid_0's ndcg@30: 0.992044\tvalid_0's ndcg@40: 0.992044\tvalid_0's ndcg@50: 0.992044\tvalid_0's ndcg@10: 0.992044\tvalid_0's ndcg@20: 0.992044\tvalid_0's ndcg@30: 0.992044\tvalid_0's ndcg@40: 0.992044\tvalid_0's ndcg@50: 0.992044\n","[72]\tvalid_0's ndcg@10: 0.992052\tvalid_0's ndcg@20: 0.992052\tvalid_0's ndcg@30: 0.992052\tvalid_0's ndcg@40: 0.992052\tvalid_0's ndcg@50: 0.992052\tvalid_0's ndcg@10: 0.992052\tvalid_0's ndcg@20: 0.992052\tvalid_0's ndcg@30: 0.992052\tvalid_0's ndcg@40: 0.992052\tvalid_0's ndcg@50: 0.992052\n","[73]\tvalid_0's ndcg@10: 0.992011\tvalid_0's ndcg@20: 0.992011\tvalid_0's ndcg@30: 0.992011\tvalid_0's ndcg@40: 0.992011\tvalid_0's ndcg@50: 0.992011\tvalid_0's ndcg@10: 0.992011\tvalid_0's ndcg@20: 0.992011\tvalid_0's ndcg@30: 0.992011\tvalid_0's ndcg@40: 0.992011\tvalid_0's ndcg@50: 0.992011\n","[74]\tvalid_0's ndcg@10: 0.992089\tvalid_0's ndcg@20: 0.992089\tvalid_0's ndcg@30: 0.992089\tvalid_0's ndcg@40: 0.992089\tvalid_0's ndcg@50: 0.992089\tvalid_0's ndcg@10: 0.992089\tvalid_0's ndcg@20: 0.992089\tvalid_0's ndcg@30: 0.992089\tvalid_0's ndcg@40: 0.992089\tvalid_0's ndcg@50: 0.992089\n","[75]\tvalid_0's ndcg@10: 0.991953\tvalid_0's ndcg@20: 0.991953\tvalid_0's ndcg@30: 0.991953\tvalid_0's ndcg@40: 0.991953\tvalid_0's ndcg@50: 0.991953\tvalid_0's ndcg@10: 0.991953\tvalid_0's ndcg@20: 0.991953\tvalid_0's ndcg@30: 0.991953\tvalid_0's ndcg@40: 0.991953\tvalid_0's ndcg@50: 0.991953\n","[76]\tvalid_0's ndcg@10: 0.991953\tvalid_0's ndcg@20: 0.991953\tvalid_0's ndcg@30: 0.991953\tvalid_0's ndcg@40: 0.991953\tvalid_0's ndcg@50: 0.991953\tvalid_0's ndcg@10: 0.991953\tvalid_0's ndcg@20: 0.991953\tvalid_0's ndcg@30: 0.991953\tvalid_0's ndcg@40: 0.991953\tvalid_0's ndcg@50: 0.991953\n","[77]\tvalid_0's ndcg@10: 0.992016\tvalid_0's ndcg@20: 0.992016\tvalid_0's ndcg@30: 0.992016\tvalid_0's ndcg@40: 0.992016\tvalid_0's ndcg@50: 0.992016\tvalid_0's ndcg@10: 0.992016\tvalid_0's ndcg@20: 0.992016\tvalid_0's ndcg@30: 0.992016\tvalid_0's ndcg@40: 0.992016\tvalid_0's ndcg@50: 0.992016\n","[78]\tvalid_0's ndcg@10: 0.992066\tvalid_0's ndcg@20: 0.992066\tvalid_0's ndcg@30: 0.992066\tvalid_0's ndcg@40: 0.992066\tvalid_0's ndcg@50: 0.992066\tvalid_0's ndcg@10: 0.992066\tvalid_0's ndcg@20: 0.992066\tvalid_0's ndcg@30: 0.992066\tvalid_0's ndcg@40: 0.992066\tvalid_0's ndcg@50: 0.992066\n","[79]\tvalid_0's ndcg@10: 0.992004\tvalid_0's ndcg@20: 0.992004\tvalid_0's ndcg@30: 0.992004\tvalid_0's ndcg@40: 0.992004\tvalid_0's ndcg@50: 0.992004\tvalid_0's ndcg@10: 0.992004\tvalid_0's ndcg@20: 0.992004\tvalid_0's ndcg@30: 0.992004\tvalid_0's ndcg@40: 0.992004\tvalid_0's ndcg@50: 0.992004\n","[80]\tvalid_0's ndcg@10: 0.992004\tvalid_0's ndcg@20: 0.992004\tvalid_0's ndcg@30: 0.992004\tvalid_0's ndcg@40: 0.992004\tvalid_0's ndcg@50: 0.992004\tvalid_0's ndcg@10: 0.992004\tvalid_0's ndcg@20: 0.992004\tvalid_0's ndcg@30: 0.992004\tvalid_0's ndcg@40: 0.992004\tvalid_0's ndcg@50: 0.992004\n","[81]\tvalid_0's ndcg@10: 0.992066\tvalid_0's ndcg@20: 0.992066\tvalid_0's ndcg@30: 0.992066\tvalid_0's ndcg@40: 0.992066\tvalid_0's ndcg@50: 0.992066\tvalid_0's ndcg@10: 0.992066\tvalid_0's ndcg@20: 0.992066\tvalid_0's ndcg@30: 0.992066\tvalid_0's ndcg@40: 0.992066\tvalid_0's ndcg@50: 0.992066\n","[82]\tvalid_0's ndcg@10: 0.992089\tvalid_0's ndcg@20: 0.992089\tvalid_0's ndcg@30: 0.992089\tvalid_0's ndcg@40: 0.992089\tvalid_0's ndcg@50: 0.992089\tvalid_0's ndcg@10: 0.992089\tvalid_0's ndcg@20: 0.992089\tvalid_0's ndcg@30: 0.992089\tvalid_0's ndcg@40: 0.992089\tvalid_0's ndcg@50: 0.992089\n","[83]\tvalid_0's ndcg@10: 0.992081\tvalid_0's ndcg@20: 0.992081\tvalid_0's ndcg@30: 0.992081\tvalid_0's ndcg@40: 0.992081\tvalid_0's ndcg@50: 0.992081\tvalid_0's ndcg@10: 0.992081\tvalid_0's ndcg@20: 0.992081\tvalid_0's ndcg@30: 0.992081\tvalid_0's ndcg@40: 0.992081\tvalid_0's ndcg@50: 0.992081\n","[84]\tvalid_0's ndcg@10: 0.992066\tvalid_0's ndcg@20: 0.992066\tvalid_0's ndcg@30: 0.992066\tvalid_0's ndcg@40: 0.992066\tvalid_0's ndcg@50: 0.992066\tvalid_0's ndcg@10: 0.992066\tvalid_0's ndcg@20: 0.992066\tvalid_0's ndcg@30: 0.992066\tvalid_0's ndcg@40: 0.992066\tvalid_0's ndcg@50: 0.992066\n","[85]\tvalid_0's ndcg@10: 0.992074\tvalid_0's ndcg@20: 0.992074\tvalid_0's ndcg@30: 0.992074\tvalid_0's ndcg@40: 0.992074\tvalid_0's ndcg@50: 0.992074\tvalid_0's ndcg@10: 0.992074\tvalid_0's ndcg@20: 0.992074\tvalid_0's ndcg@30: 0.992074\tvalid_0's ndcg@40: 0.992074\tvalid_0's ndcg@50: 0.992074\n","[86]\tvalid_0's ndcg@10: 0.992026\tvalid_0's ndcg@20: 0.992026\tvalid_0's ndcg@30: 0.992026\tvalid_0's ndcg@40: 0.992026\tvalid_0's ndcg@50: 0.992026\tvalid_0's ndcg@10: 0.992026\tvalid_0's ndcg@20: 0.992026\tvalid_0's ndcg@30: 0.992026\tvalid_0's ndcg@40: 0.992026\tvalid_0's ndcg@50: 0.992026\n","[87]\tvalid_0's ndcg@10: 0.99207\tvalid_0's ndcg@20: 0.99207\tvalid_0's ndcg@30: 0.99207\tvalid_0's ndcg@40: 0.99207\tvalid_0's ndcg@50: 0.99207\tvalid_0's ndcg@10: 0.99207\tvalid_0's ndcg@20: 0.99207\tvalid_0's ndcg@30: 0.99207\tvalid_0's ndcg@40: 0.99207\tvalid_0's ndcg@50: 0.99207\n","[88]\tvalid_0's ndcg@10: 0.992007\tvalid_0's ndcg@20: 0.992007\tvalid_0's ndcg@30: 0.992007\tvalid_0's ndcg@40: 0.992007\tvalid_0's ndcg@50: 0.992007\tvalid_0's ndcg@10: 0.992007\tvalid_0's ndcg@20: 0.992007\tvalid_0's ndcg@30: 0.992007\tvalid_0's ndcg@40: 0.992007\tvalid_0's ndcg@50: 0.992007\n","[89]\tvalid_0's ndcg@10: 0.99197\tvalid_0's ndcg@20: 0.99197\tvalid_0's ndcg@30: 0.99197\tvalid_0's ndcg@40: 0.99197\tvalid_0's ndcg@50: 0.99197\tvalid_0's ndcg@10: 0.99197\tvalid_0's ndcg@20: 0.99197\tvalid_0's ndcg@30: 0.99197\tvalid_0's ndcg@40: 0.99197\tvalid_0's ndcg@50: 0.99197\n","[90]\tvalid_0's ndcg@10: 0.991955\tvalid_0's ndcg@20: 0.991955\tvalid_0's ndcg@30: 0.991955\tvalid_0's ndcg@40: 0.991955\tvalid_0's ndcg@50: 0.991955\tvalid_0's ndcg@10: 0.991955\tvalid_0's ndcg@20: 0.991955\tvalid_0's ndcg@30: 0.991955\tvalid_0's ndcg@40: 0.991955\tvalid_0's ndcg@50: 0.991955\n","[91]\tvalid_0's ndcg@10: 0.991898\tvalid_0's ndcg@20: 0.991898\tvalid_0's ndcg@30: 0.991898\tvalid_0's ndcg@40: 0.991898\tvalid_0's ndcg@50: 0.991898\tvalid_0's ndcg@10: 0.991898\tvalid_0's ndcg@20: 0.991898\tvalid_0's ndcg@30: 0.991898\tvalid_0's ndcg@40: 0.991898\tvalid_0's ndcg@50: 0.991898\n","[92]\tvalid_0's ndcg@10: 0.991973\tvalid_0's ndcg@20: 0.991973\tvalid_0's ndcg@30: 0.991973\tvalid_0's ndcg@40: 0.991973\tvalid_0's ndcg@50: 0.991973\tvalid_0's ndcg@10: 0.991973\tvalid_0's ndcg@20: 0.991973\tvalid_0's ndcg@30: 0.991973\tvalid_0's ndcg@40: 0.991973\tvalid_0's ndcg@50: 0.991973\n","[93]\tvalid_0's ndcg@10: 0.991917\tvalid_0's ndcg@20: 0.991917\tvalid_0's ndcg@30: 0.991917\tvalid_0's ndcg@40: 0.991917\tvalid_0's ndcg@50: 0.991917\tvalid_0's ndcg@10: 0.991917\tvalid_0's ndcg@20: 0.991917\tvalid_0's ndcg@30: 0.991917\tvalid_0's ndcg@40: 0.991917\tvalid_0's ndcg@50: 0.991917\n","[94]\tvalid_0's ndcg@10: 0.991932\tvalid_0's ndcg@20: 0.991932\tvalid_0's ndcg@30: 0.991932\tvalid_0's ndcg@40: 0.991932\tvalid_0's ndcg@50: 0.991932\tvalid_0's ndcg@10: 0.991932\tvalid_0's ndcg@20: 0.991932\tvalid_0's ndcg@30: 0.991932\tvalid_0's ndcg@40: 0.991932\tvalid_0's ndcg@50: 0.991932\n","[95]\tvalid_0's ndcg@10: 0.991936\tvalid_0's ndcg@20: 0.991936\tvalid_0's ndcg@30: 0.991936\tvalid_0's ndcg@40: 0.991936\tvalid_0's ndcg@50: 0.991936\tvalid_0's ndcg@10: 0.991936\tvalid_0's ndcg@20: 0.991936\tvalid_0's ndcg@30: 0.991936\tvalid_0's ndcg@40: 0.991936\tvalid_0's ndcg@50: 0.991936\n","[96]\tvalid_0's ndcg@10: 0.99205\tvalid_0's ndcg@20: 0.99205\tvalid_0's ndcg@30: 0.99205\tvalid_0's ndcg@40: 0.99205\tvalid_0's ndcg@50: 0.99205\tvalid_0's ndcg@10: 0.99205\tvalid_0's ndcg@20: 0.99205\tvalid_0's ndcg@30: 0.99205\tvalid_0's ndcg@40: 0.99205\tvalid_0's ndcg@50: 0.99205\n","[97]\tvalid_0's ndcg@10: 0.992062\tvalid_0's ndcg@20: 0.992062\tvalid_0's ndcg@30: 0.992062\tvalid_0's ndcg@40: 0.992062\tvalid_0's ndcg@50: 0.992062\tvalid_0's ndcg@10: 0.992062\tvalid_0's ndcg@20: 0.992062\tvalid_0's ndcg@30: 0.992062\tvalid_0's ndcg@40: 0.992062\tvalid_0's ndcg@50: 0.992062\n","[98]\tvalid_0's ndcg@10: 0.99205\tvalid_0's ndcg@20: 0.99205\tvalid_0's ndcg@30: 0.99205\tvalid_0's ndcg@40: 0.99205\tvalid_0's ndcg@50: 0.99205\tvalid_0's ndcg@10: 0.99205\tvalid_0's ndcg@20: 0.99205\tvalid_0's ndcg@30: 0.99205\tvalid_0's ndcg@40: 0.99205\tvalid_0's ndcg@50: 0.99205\n","[99]\tvalid_0's ndcg@10: 0.991924\tvalid_0's ndcg@20: 0.991924\tvalid_0's ndcg@30: 0.991924\tvalid_0's ndcg@40: 0.991924\tvalid_0's ndcg@50: 0.991924\tvalid_0's ndcg@10: 0.991924\tvalid_0's ndcg@20: 0.991924\tvalid_0's ndcg@30: 0.991924\tvalid_0's ndcg@40: 0.991924\tvalid_0's ndcg@50: 0.991924\n","[100]\tvalid_0's ndcg@10: 0.991999\tvalid_0's ndcg@20: 0.991999\tvalid_0's ndcg@30: 0.991999\tvalid_0's ndcg@40: 0.991999\tvalid_0's ndcg@50: 0.991999\tvalid_0's ndcg@10: 0.991999\tvalid_0's ndcg@20: 0.991999\tvalid_0's ndcg@30: 0.991999\tvalid_0's ndcg@40: 0.991999\tvalid_0's ndcg@50: 0.991999\n","Did not meet early stopping. Best iteration is:\n","[63]\tvalid_0's ndcg@10: 0.992202\tvalid_0's ndcg@20: 0.992202\tvalid_0's ndcg@30: 0.992202\tvalid_0's ndcg@40: 0.992202\tvalid_0's ndcg@50: 0.992202\tvalid_0's ndcg@10: 0.992202\tvalid_0's ndcg@20: 0.992202\tvalid_0's ndcg@30: 0.992202\tvalid_0's ndcg@40: 0.992202\tvalid_0's ndcg@50: 0.992202\n","[1]\tvalid_0's ndcg@10: 0.989833\tvalid_0's ndcg@20: 0.989833\tvalid_0's ndcg@30: 0.989833\tvalid_0's ndcg@40: 0.989833\tvalid_0's ndcg@50: 0.989833\tvalid_0's ndcg@10: 0.989833\tvalid_0's ndcg@20: 0.989833\tvalid_0's ndcg@30: 0.989833\tvalid_0's ndcg@40: 0.989833\tvalid_0's ndcg@50: 0.989833\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.990708\tvalid_0's ndcg@20: 0.990708\tvalid_0's ndcg@30: 0.990708\tvalid_0's ndcg@40: 0.990708\tvalid_0's ndcg@50: 0.990708\tvalid_0's ndcg@10: 0.990708\tvalid_0's ndcg@20: 0.990708\tvalid_0's ndcg@30: 0.990708\tvalid_0's ndcg@40: 0.990708\tvalid_0's ndcg@50: 0.990708\n","[3]\tvalid_0's ndcg@10: 0.99113\tvalid_0's ndcg@20: 0.99113\tvalid_0's ndcg@30: 0.99113\tvalid_0's ndcg@40: 0.99113\tvalid_0's ndcg@50: 0.99113\tvalid_0's ndcg@10: 0.99113\tvalid_0's ndcg@20: 0.99113\tvalid_0's ndcg@30: 0.99113\tvalid_0's ndcg@40: 0.99113\tvalid_0's ndcg@50: 0.99113\n","[4]\tvalid_0's ndcg@10: 0.991702\tvalid_0's ndcg@20: 0.991702\tvalid_0's ndcg@30: 0.991702\tvalid_0's ndcg@40: 0.991702\tvalid_0's ndcg@50: 0.991702\tvalid_0's ndcg@10: 0.991702\tvalid_0's ndcg@20: 0.991702\tvalid_0's ndcg@30: 0.991702\tvalid_0's ndcg@40: 0.991702\tvalid_0's ndcg@50: 0.991702\n","[5]\tvalid_0's ndcg@10: 0.991954\tvalid_0's ndcg@20: 0.991954\tvalid_0's ndcg@30: 0.991954\tvalid_0's ndcg@40: 0.991954\tvalid_0's ndcg@50: 0.991954\tvalid_0's ndcg@10: 0.991954\tvalid_0's ndcg@20: 0.991954\tvalid_0's ndcg@30: 0.991954\tvalid_0's ndcg@40: 0.991954\tvalid_0's ndcg@50: 0.991954\n","[6]\tvalid_0's ndcg@10: 0.991943\tvalid_0's ndcg@20: 0.991943\tvalid_0's ndcg@30: 0.991943\tvalid_0's ndcg@40: 0.991943\tvalid_0's ndcg@50: 0.991943\tvalid_0's ndcg@10: 0.991943\tvalid_0's ndcg@20: 0.991943\tvalid_0's ndcg@30: 0.991943\tvalid_0's ndcg@40: 0.991943\tvalid_0's ndcg@50: 0.991943\n","[7]\tvalid_0's ndcg@10: 0.991962\tvalid_0's ndcg@20: 0.991962\tvalid_0's ndcg@30: 0.991962\tvalid_0's ndcg@40: 0.991962\tvalid_0's ndcg@50: 0.991962\tvalid_0's ndcg@10: 0.991962\tvalid_0's ndcg@20: 0.991962\tvalid_0's ndcg@30: 0.991962\tvalid_0's ndcg@40: 0.991962\tvalid_0's ndcg@50: 0.991962\n","[8]\tvalid_0's ndcg@10: 0.991791\tvalid_0's ndcg@20: 0.991791\tvalid_0's ndcg@30: 0.991791\tvalid_0's ndcg@40: 0.991791\tvalid_0's ndcg@50: 0.991791\tvalid_0's ndcg@10: 0.991791\tvalid_0's ndcg@20: 0.991791\tvalid_0's ndcg@30: 0.991791\tvalid_0's ndcg@40: 0.991791\tvalid_0's ndcg@50: 0.991791\n","[9]\tvalid_0's ndcg@10: 0.992184\tvalid_0's ndcg@20: 0.992184\tvalid_0's ndcg@30: 0.992184\tvalid_0's ndcg@40: 0.992184\tvalid_0's ndcg@50: 0.992184\tvalid_0's ndcg@10: 0.992184\tvalid_0's ndcg@20: 0.992184\tvalid_0's ndcg@30: 0.992184\tvalid_0's ndcg@40: 0.992184\tvalid_0's ndcg@50: 0.992184\n","[10]\tvalid_0's ndcg@10: 0.99258\tvalid_0's ndcg@20: 0.99258\tvalid_0's ndcg@30: 0.99258\tvalid_0's ndcg@40: 0.99258\tvalid_0's ndcg@50: 0.99258\tvalid_0's ndcg@10: 0.99258\tvalid_0's ndcg@20: 0.99258\tvalid_0's ndcg@30: 0.99258\tvalid_0's ndcg@40: 0.99258\tvalid_0's ndcg@50: 0.99258\n","[11]\tvalid_0's ndcg@10: 0.992501\tvalid_0's ndcg@20: 0.992501\tvalid_0's ndcg@30: 0.992501\tvalid_0's ndcg@40: 0.992501\tvalid_0's ndcg@50: 0.992501\tvalid_0's ndcg@10: 0.992501\tvalid_0's ndcg@20: 0.992501\tvalid_0's ndcg@30: 0.992501\tvalid_0's ndcg@40: 0.992501\tvalid_0's ndcg@50: 0.992501\n","[12]\tvalid_0's ndcg@10: 0.99258\tvalid_0's ndcg@20: 0.99258\tvalid_0's ndcg@30: 0.99258\tvalid_0's ndcg@40: 0.99258\tvalid_0's ndcg@50: 0.99258\tvalid_0's ndcg@10: 0.99258\tvalid_0's ndcg@20: 0.99258\tvalid_0's ndcg@30: 0.99258\tvalid_0's ndcg@40: 0.99258\tvalid_0's ndcg@50: 0.99258\n","[13]\tvalid_0's ndcg@10: 0.992926\tvalid_0's ndcg@20: 0.992926\tvalid_0's ndcg@30: 0.992926\tvalid_0's ndcg@40: 0.992926\tvalid_0's ndcg@50: 0.992926\tvalid_0's ndcg@10: 0.992926\tvalid_0's ndcg@20: 0.992926\tvalid_0's ndcg@30: 0.992926\tvalid_0's ndcg@40: 0.992926\tvalid_0's ndcg@50: 0.992926\n","[14]\tvalid_0's ndcg@10: 0.992721\tvalid_0's ndcg@20: 0.992721\tvalid_0's ndcg@30: 0.992721\tvalid_0's ndcg@40: 0.992721\tvalid_0's ndcg@50: 0.992721\tvalid_0's ndcg@10: 0.992721\tvalid_0's ndcg@20: 0.992721\tvalid_0's ndcg@30: 0.992721\tvalid_0's ndcg@40: 0.992721\tvalid_0's ndcg@50: 0.992721\n","[15]\tvalid_0's ndcg@10: 0.992788\tvalid_0's ndcg@20: 0.992788\tvalid_0's ndcg@30: 0.992788\tvalid_0's ndcg@40: 0.992788\tvalid_0's ndcg@50: 0.992788\tvalid_0's ndcg@10: 0.992788\tvalid_0's ndcg@20: 0.992788\tvalid_0's ndcg@30: 0.992788\tvalid_0's ndcg@40: 0.992788\tvalid_0's ndcg@50: 0.992788\n","[16]\tvalid_0's ndcg@10: 0.99277\tvalid_0's ndcg@20: 0.99277\tvalid_0's ndcg@30: 0.99277\tvalid_0's ndcg@40: 0.99277\tvalid_0's ndcg@50: 0.99277\tvalid_0's ndcg@10: 0.99277\tvalid_0's ndcg@20: 0.99277\tvalid_0's ndcg@30: 0.99277\tvalid_0's ndcg@40: 0.99277\tvalid_0's ndcg@50: 0.99277\n","[17]\tvalid_0's ndcg@10: 0.992793\tvalid_0's ndcg@20: 0.992793\tvalid_0's ndcg@30: 0.992793\tvalid_0's ndcg@40: 0.992793\tvalid_0's ndcg@50: 0.992793\tvalid_0's ndcg@10: 0.992793\tvalid_0's ndcg@20: 0.992793\tvalid_0's ndcg@30: 0.992793\tvalid_0's ndcg@40: 0.992793\tvalid_0's ndcg@50: 0.992793\n","[18]\tvalid_0's ndcg@10: 0.992863\tvalid_0's ndcg@20: 0.992863\tvalid_0's ndcg@30: 0.992863\tvalid_0's ndcg@40: 0.992863\tvalid_0's ndcg@50: 0.992863\tvalid_0's ndcg@10: 0.992863\tvalid_0's ndcg@20: 0.992863\tvalid_0's ndcg@30: 0.992863\tvalid_0's ndcg@40: 0.992863\tvalid_0's ndcg@50: 0.992863\n","[19]\tvalid_0's ndcg@10: 0.993017\tvalid_0's ndcg@20: 0.993017\tvalid_0's ndcg@30: 0.993017\tvalid_0's ndcg@40: 0.993017\tvalid_0's ndcg@50: 0.993017\tvalid_0's ndcg@10: 0.993017\tvalid_0's ndcg@20: 0.993017\tvalid_0's ndcg@30: 0.993017\tvalid_0's ndcg@40: 0.993017\tvalid_0's ndcg@50: 0.993017\n","[20]\tvalid_0's ndcg@10: 0.993073\tvalid_0's ndcg@20: 0.993073\tvalid_0's ndcg@30: 0.993073\tvalid_0's ndcg@40: 0.993073\tvalid_0's ndcg@50: 0.993073\tvalid_0's ndcg@10: 0.993073\tvalid_0's ndcg@20: 0.993073\tvalid_0's ndcg@30: 0.993073\tvalid_0's ndcg@40: 0.993073\tvalid_0's ndcg@50: 0.993073\n","[21]\tvalid_0's ndcg@10: 0.993065\tvalid_0's ndcg@20: 0.993065\tvalid_0's ndcg@30: 0.993065\tvalid_0's ndcg@40: 0.993065\tvalid_0's ndcg@50: 0.993065\tvalid_0's ndcg@10: 0.993065\tvalid_0's ndcg@20: 0.993065\tvalid_0's ndcg@30: 0.993065\tvalid_0's ndcg@40: 0.993065\tvalid_0's ndcg@50: 0.993065\n","[22]\tvalid_0's ndcg@10: 0.993169\tvalid_0's ndcg@20: 0.993169\tvalid_0's ndcg@30: 0.993169\tvalid_0's ndcg@40: 0.993169\tvalid_0's ndcg@50: 0.993169\tvalid_0's ndcg@10: 0.993169\tvalid_0's ndcg@20: 0.993169\tvalid_0's ndcg@30: 0.993169\tvalid_0's ndcg@40: 0.993169\tvalid_0's ndcg@50: 0.993169\n","[23]\tvalid_0's ndcg@10: 0.993191\tvalid_0's ndcg@20: 0.993191\tvalid_0's ndcg@30: 0.993191\tvalid_0's ndcg@40: 0.993191\tvalid_0's ndcg@50: 0.993191\tvalid_0's ndcg@10: 0.993191\tvalid_0's ndcg@20: 0.993191\tvalid_0's ndcg@30: 0.993191\tvalid_0's ndcg@40: 0.993191\tvalid_0's ndcg@50: 0.993191\n","[24]\tvalid_0's ndcg@10: 0.993312\tvalid_0's ndcg@20: 0.993312\tvalid_0's ndcg@30: 0.993312\tvalid_0's ndcg@40: 0.993312\tvalid_0's ndcg@50: 0.993312\tvalid_0's ndcg@10: 0.993312\tvalid_0's ndcg@20: 0.993312\tvalid_0's ndcg@30: 0.993312\tvalid_0's ndcg@40: 0.993312\tvalid_0's ndcg@50: 0.993312\n","[25]\tvalid_0's ndcg@10: 0.993409\tvalid_0's ndcg@20: 0.993409\tvalid_0's ndcg@30: 0.993409\tvalid_0's ndcg@40: 0.993409\tvalid_0's ndcg@50: 0.993409\tvalid_0's ndcg@10: 0.993409\tvalid_0's ndcg@20: 0.993409\tvalid_0's ndcg@30: 0.993409\tvalid_0's ndcg@40: 0.993409\tvalid_0's ndcg@50: 0.993409\n","[26]\tvalid_0's ndcg@10: 0.993225\tvalid_0's ndcg@20: 0.993225\tvalid_0's ndcg@30: 0.993225\tvalid_0's ndcg@40: 0.993225\tvalid_0's ndcg@50: 0.993225\tvalid_0's ndcg@10: 0.993225\tvalid_0's ndcg@20: 0.993225\tvalid_0's ndcg@30: 0.993225\tvalid_0's ndcg@40: 0.993225\tvalid_0's ndcg@50: 0.993225\n","[27]\tvalid_0's ndcg@10: 0.993051\tvalid_0's ndcg@20: 0.993051\tvalid_0's ndcg@30: 0.993051\tvalid_0's ndcg@40: 0.993051\tvalid_0's ndcg@50: 0.993051\tvalid_0's ndcg@10: 0.993051\tvalid_0's ndcg@20: 0.993051\tvalid_0's ndcg@30: 0.993051\tvalid_0's ndcg@40: 0.993051\tvalid_0's ndcg@50: 0.993051\n","[28]\tvalid_0's ndcg@10: 0.993174\tvalid_0's ndcg@20: 0.993174\tvalid_0's ndcg@30: 0.993174\tvalid_0's ndcg@40: 0.993174\tvalid_0's ndcg@50: 0.993174\tvalid_0's ndcg@10: 0.993174\tvalid_0's ndcg@20: 0.993174\tvalid_0's ndcg@30: 0.993174\tvalid_0's ndcg@40: 0.993174\tvalid_0's ndcg@50: 0.993174\n","[29]\tvalid_0's ndcg@10: 0.993102\tvalid_0's ndcg@20: 0.993102\tvalid_0's ndcg@30: 0.993102\tvalid_0's ndcg@40: 0.993102\tvalid_0's ndcg@50: 0.993102\tvalid_0's ndcg@10: 0.993102\tvalid_0's ndcg@20: 0.993102\tvalid_0's ndcg@30: 0.993102\tvalid_0's ndcg@40: 0.993102\tvalid_0's ndcg@50: 0.993102\n","[30]\tvalid_0's ndcg@10: 0.993106\tvalid_0's ndcg@20: 0.993106\tvalid_0's ndcg@30: 0.993106\tvalid_0's ndcg@40: 0.993106\tvalid_0's ndcg@50: 0.993106\tvalid_0's ndcg@10: 0.993106\tvalid_0's ndcg@20: 0.993106\tvalid_0's ndcg@30: 0.993106\tvalid_0's ndcg@40: 0.993106\tvalid_0's ndcg@50: 0.993106\n","[31]\tvalid_0's ndcg@10: 0.993085\tvalid_0's ndcg@20: 0.993085\tvalid_0's ndcg@30: 0.993085\tvalid_0's ndcg@40: 0.993085\tvalid_0's ndcg@50: 0.993085\tvalid_0's ndcg@10: 0.993085\tvalid_0's ndcg@20: 0.993085\tvalid_0's ndcg@30: 0.993085\tvalid_0's ndcg@40: 0.993085\tvalid_0's ndcg@50: 0.993085\n","[32]\tvalid_0's ndcg@10: 0.993117\tvalid_0's ndcg@20: 0.993117\tvalid_0's ndcg@30: 0.993117\tvalid_0's ndcg@40: 0.993117\tvalid_0's ndcg@50: 0.993117\tvalid_0's ndcg@10: 0.993117\tvalid_0's ndcg@20: 0.993117\tvalid_0's ndcg@30: 0.993117\tvalid_0's ndcg@40: 0.993117\tvalid_0's ndcg@50: 0.993117\n","[33]\tvalid_0's ndcg@10: 0.993145\tvalid_0's ndcg@20: 0.993145\tvalid_0's ndcg@30: 0.993145\tvalid_0's ndcg@40: 0.993145\tvalid_0's ndcg@50: 0.993145\tvalid_0's ndcg@10: 0.993145\tvalid_0's ndcg@20: 0.993145\tvalid_0's ndcg@30: 0.993145\tvalid_0's ndcg@40: 0.993145\tvalid_0's ndcg@50: 0.993145\n","[34]\tvalid_0's ndcg@10: 0.993104\tvalid_0's ndcg@20: 0.993104\tvalid_0's ndcg@30: 0.993104\tvalid_0's ndcg@40: 0.993104\tvalid_0's ndcg@50: 0.993104\tvalid_0's ndcg@10: 0.993104\tvalid_0's ndcg@20: 0.993104\tvalid_0's ndcg@30: 0.993104\tvalid_0's ndcg@40: 0.993104\tvalid_0's ndcg@50: 0.993104\n","[35]\tvalid_0's ndcg@10: 0.993065\tvalid_0's ndcg@20: 0.993065\tvalid_0's ndcg@30: 0.993065\tvalid_0's ndcg@40: 0.993065\tvalid_0's ndcg@50: 0.993065\tvalid_0's ndcg@10: 0.993065\tvalid_0's ndcg@20: 0.993065\tvalid_0's ndcg@30: 0.993065\tvalid_0's ndcg@40: 0.993065\tvalid_0's ndcg@50: 0.993065\n","[36]\tvalid_0's ndcg@10: 0.993124\tvalid_0's ndcg@20: 0.993124\tvalid_0's ndcg@30: 0.993124\tvalid_0's ndcg@40: 0.993124\tvalid_0's ndcg@50: 0.993124\tvalid_0's ndcg@10: 0.993124\tvalid_0's ndcg@20: 0.993124\tvalid_0's ndcg@30: 0.993124\tvalid_0's ndcg@40: 0.993124\tvalid_0's ndcg@50: 0.993124\n","[37]\tvalid_0's ndcg@10: 0.993183\tvalid_0's ndcg@20: 0.993183\tvalid_0's ndcg@30: 0.993183\tvalid_0's ndcg@40: 0.993183\tvalid_0's ndcg@50: 0.993183\tvalid_0's ndcg@10: 0.993183\tvalid_0's ndcg@20: 0.993183\tvalid_0's ndcg@30: 0.993183\tvalid_0's ndcg@40: 0.993183\tvalid_0's ndcg@50: 0.993183\n","[38]\tvalid_0's ndcg@10: 0.993183\tvalid_0's ndcg@20: 0.993183\tvalid_0's ndcg@30: 0.993183\tvalid_0's ndcg@40: 0.993183\tvalid_0's ndcg@50: 0.993183\tvalid_0's ndcg@10: 0.993183\tvalid_0's ndcg@20: 0.993183\tvalid_0's ndcg@30: 0.993183\tvalid_0's ndcg@40: 0.993183\tvalid_0's ndcg@50: 0.993183\n","[39]\tvalid_0's ndcg@10: 0.993143\tvalid_0's ndcg@20: 0.993143\tvalid_0's ndcg@30: 0.993143\tvalid_0's ndcg@40: 0.993143\tvalid_0's ndcg@50: 0.993143\tvalid_0's ndcg@10: 0.993143\tvalid_0's ndcg@20: 0.993143\tvalid_0's ndcg@30: 0.993143\tvalid_0's ndcg@40: 0.993143\tvalid_0's ndcg@50: 0.993143\n","[40]\tvalid_0's ndcg@10: 0.993015\tvalid_0's ndcg@20: 0.993015\tvalid_0's ndcg@30: 0.993015\tvalid_0's ndcg@40: 0.993015\tvalid_0's ndcg@50: 0.993015\tvalid_0's ndcg@10: 0.993015\tvalid_0's ndcg@20: 0.993015\tvalid_0's ndcg@30: 0.993015\tvalid_0's ndcg@40: 0.993015\tvalid_0's ndcg@50: 0.993015\n","[41]\tvalid_0's ndcg@10: 0.993109\tvalid_0's ndcg@20: 0.993109\tvalid_0's ndcg@30: 0.993109\tvalid_0's ndcg@40: 0.993109\tvalid_0's ndcg@50: 0.993109\tvalid_0's ndcg@10: 0.993109\tvalid_0's ndcg@20: 0.993109\tvalid_0's ndcg@30: 0.993109\tvalid_0's ndcg@40: 0.993109\tvalid_0's ndcg@50: 0.993109\n","[42]\tvalid_0's ndcg@10: 0.993124\tvalid_0's ndcg@20: 0.993124\tvalid_0's ndcg@30: 0.993124\tvalid_0's ndcg@40: 0.993124\tvalid_0's ndcg@50: 0.993124\tvalid_0's ndcg@10: 0.993124\tvalid_0's ndcg@20: 0.993124\tvalid_0's ndcg@30: 0.993124\tvalid_0's ndcg@40: 0.993124\tvalid_0's ndcg@50: 0.993124\n","[43]\tvalid_0's ndcg@10: 0.993004\tvalid_0's ndcg@20: 0.993004\tvalid_0's ndcg@30: 0.993004\tvalid_0's ndcg@40: 0.993004\tvalid_0's ndcg@50: 0.993004\tvalid_0's ndcg@10: 0.993004\tvalid_0's ndcg@20: 0.993004\tvalid_0's ndcg@30: 0.993004\tvalid_0's ndcg@40: 0.993004\tvalid_0's ndcg@50: 0.993004\n","[44]\tvalid_0's ndcg@10: 0.992983\tvalid_0's ndcg@20: 0.992983\tvalid_0's ndcg@30: 0.992983\tvalid_0's ndcg@40: 0.992983\tvalid_0's ndcg@50: 0.992983\tvalid_0's ndcg@10: 0.992983\tvalid_0's ndcg@20: 0.992983\tvalid_0's ndcg@30: 0.992983\tvalid_0's ndcg@40: 0.992983\tvalid_0's ndcg@50: 0.992983\n","[45]\tvalid_0's ndcg@10: 0.993031\tvalid_0's ndcg@20: 0.993031\tvalid_0's ndcg@30: 0.993031\tvalid_0's ndcg@40: 0.993031\tvalid_0's ndcg@50: 0.993031\tvalid_0's ndcg@10: 0.993031\tvalid_0's ndcg@20: 0.993031\tvalid_0's ndcg@30: 0.993031\tvalid_0's ndcg@40: 0.993031\tvalid_0's ndcg@50: 0.993031\n","[46]\tvalid_0's ndcg@10: 0.993019\tvalid_0's ndcg@20: 0.993019\tvalid_0's ndcg@30: 0.993019\tvalid_0's ndcg@40: 0.993019\tvalid_0's ndcg@50: 0.993019\tvalid_0's ndcg@10: 0.993019\tvalid_0's ndcg@20: 0.993019\tvalid_0's ndcg@30: 0.993019\tvalid_0's ndcg@40: 0.993019\tvalid_0's ndcg@50: 0.993019\n","[47]\tvalid_0's ndcg@10: 0.993092\tvalid_0's ndcg@20: 0.993092\tvalid_0's ndcg@30: 0.993092\tvalid_0's ndcg@40: 0.993092\tvalid_0's ndcg@50: 0.993092\tvalid_0's ndcg@10: 0.993092\tvalid_0's ndcg@20: 0.993092\tvalid_0's ndcg@30: 0.993092\tvalid_0's ndcg@40: 0.993092\tvalid_0's ndcg@50: 0.993092\n","[48]\tvalid_0's ndcg@10: 0.993142\tvalid_0's ndcg@20: 0.993142\tvalid_0's ndcg@30: 0.993142\tvalid_0's ndcg@40: 0.993142\tvalid_0's ndcg@50: 0.993142\tvalid_0's ndcg@10: 0.993142\tvalid_0's ndcg@20: 0.993142\tvalid_0's ndcg@30: 0.993142\tvalid_0's ndcg@40: 0.993142\tvalid_0's ndcg@50: 0.993142\n","[49]\tvalid_0's ndcg@10: 0.993217\tvalid_0's ndcg@20: 0.993217\tvalid_0's ndcg@30: 0.993217\tvalid_0's ndcg@40: 0.993217\tvalid_0's ndcg@50: 0.993217\tvalid_0's ndcg@10: 0.993217\tvalid_0's ndcg@20: 0.993217\tvalid_0's ndcg@30: 0.993217\tvalid_0's ndcg@40: 0.993217\tvalid_0's ndcg@50: 0.993217\n","[50]\tvalid_0's ndcg@10: 0.993206\tvalid_0's ndcg@20: 0.993206\tvalid_0's ndcg@30: 0.993206\tvalid_0's ndcg@40: 0.993206\tvalid_0's ndcg@50: 0.993206\tvalid_0's ndcg@10: 0.993206\tvalid_0's ndcg@20: 0.993206\tvalid_0's ndcg@30: 0.993206\tvalid_0's ndcg@40: 0.993206\tvalid_0's ndcg@50: 0.993206\n","[51]\tvalid_0's ndcg@10: 0.993194\tvalid_0's ndcg@20: 0.993194\tvalid_0's ndcg@30: 0.993194\tvalid_0's ndcg@40: 0.993194\tvalid_0's ndcg@50: 0.993194\tvalid_0's ndcg@10: 0.993194\tvalid_0's ndcg@20: 0.993194\tvalid_0's ndcg@30: 0.993194\tvalid_0's ndcg@40: 0.993194\tvalid_0's ndcg@50: 0.993194\n","[52]\tvalid_0's ndcg@10: 0.99321\tvalid_0's ndcg@20: 0.99321\tvalid_0's ndcg@30: 0.99321\tvalid_0's ndcg@40: 0.99321\tvalid_0's ndcg@50: 0.99321\tvalid_0's ndcg@10: 0.99321\tvalid_0's ndcg@20: 0.99321\tvalid_0's ndcg@30: 0.99321\tvalid_0's ndcg@40: 0.99321\tvalid_0's ndcg@50: 0.99321\n","[53]\tvalid_0's ndcg@10: 0.993241\tvalid_0's ndcg@20: 0.993241\tvalid_0's ndcg@30: 0.993241\tvalid_0's ndcg@40: 0.993241\tvalid_0's ndcg@50: 0.993241\tvalid_0's ndcg@10: 0.993241\tvalid_0's ndcg@20: 0.993241\tvalid_0's ndcg@30: 0.993241\tvalid_0's ndcg@40: 0.993241\tvalid_0's ndcg@50: 0.993241\n","[54]\tvalid_0's ndcg@10: 0.993223\tvalid_0's ndcg@20: 0.993223\tvalid_0's ndcg@30: 0.993223\tvalid_0's ndcg@40: 0.993223\tvalid_0's ndcg@50: 0.993223\tvalid_0's ndcg@10: 0.993223\tvalid_0's ndcg@20: 0.993223\tvalid_0's ndcg@30: 0.993223\tvalid_0's ndcg@40: 0.993223\tvalid_0's ndcg@50: 0.993223\n","[55]\tvalid_0's ndcg@10: 0.99336\tvalid_0's ndcg@20: 0.99336\tvalid_0's ndcg@30: 0.99336\tvalid_0's ndcg@40: 0.99336\tvalid_0's ndcg@50: 0.99336\tvalid_0's ndcg@10: 0.99336\tvalid_0's ndcg@20: 0.99336\tvalid_0's ndcg@30: 0.99336\tvalid_0's ndcg@40: 0.99336\tvalid_0's ndcg@50: 0.99336\n","[56]\tvalid_0's ndcg@10: 0.993252\tvalid_0's ndcg@20: 0.993252\tvalid_0's ndcg@30: 0.993252\tvalid_0's ndcg@40: 0.993252\tvalid_0's ndcg@50: 0.993252\tvalid_0's ndcg@10: 0.993252\tvalid_0's ndcg@20: 0.993252\tvalid_0's ndcg@30: 0.993252\tvalid_0's ndcg@40: 0.993252\tvalid_0's ndcg@50: 0.993252\n","[57]\tvalid_0's ndcg@10: 0.99333\tvalid_0's ndcg@20: 0.99333\tvalid_0's ndcg@30: 0.99333\tvalid_0's ndcg@40: 0.99333\tvalid_0's ndcg@50: 0.99333\tvalid_0's ndcg@10: 0.99333\tvalid_0's ndcg@20: 0.99333\tvalid_0's ndcg@30: 0.99333\tvalid_0's ndcg@40: 0.99333\tvalid_0's ndcg@50: 0.99333\n","[58]\tvalid_0's ndcg@10: 0.993257\tvalid_0's ndcg@20: 0.993257\tvalid_0's ndcg@30: 0.993257\tvalid_0's ndcg@40: 0.993257\tvalid_0's ndcg@50: 0.993257\tvalid_0's ndcg@10: 0.993257\tvalid_0's ndcg@20: 0.993257\tvalid_0's ndcg@30: 0.993257\tvalid_0's ndcg@40: 0.993257\tvalid_0's ndcg@50: 0.993257\n","[59]\tvalid_0's ndcg@10: 0.993342\tvalid_0's ndcg@20: 0.993342\tvalid_0's ndcg@30: 0.993342\tvalid_0's ndcg@40: 0.993342\tvalid_0's ndcg@50: 0.993342\tvalid_0's ndcg@10: 0.993342\tvalid_0's ndcg@20: 0.993342\tvalid_0's ndcg@30: 0.993342\tvalid_0's ndcg@40: 0.993342\tvalid_0's ndcg@50: 0.993342\n","[60]\tvalid_0's ndcg@10: 0.993217\tvalid_0's ndcg@20: 0.993217\tvalid_0's ndcg@30: 0.993217\tvalid_0's ndcg@40: 0.993217\tvalid_0's ndcg@50: 0.993217\tvalid_0's ndcg@10: 0.993217\tvalid_0's ndcg@20: 0.993217\tvalid_0's ndcg@30: 0.993217\tvalid_0's ndcg@40: 0.993217\tvalid_0's ndcg@50: 0.993217\n","[61]\tvalid_0's ndcg@10: 0.993213\tvalid_0's ndcg@20: 0.993213\tvalid_0's ndcg@30: 0.993213\tvalid_0's ndcg@40: 0.993213\tvalid_0's ndcg@50: 0.993213\tvalid_0's ndcg@10: 0.993213\tvalid_0's ndcg@20: 0.993213\tvalid_0's ndcg@30: 0.993213\tvalid_0's ndcg@40: 0.993213\tvalid_0's ndcg@50: 0.993213\n","[62]\tvalid_0's ndcg@10: 0.993206\tvalid_0's ndcg@20: 0.993206\tvalid_0's ndcg@30: 0.993206\tvalid_0's ndcg@40: 0.993206\tvalid_0's ndcg@50: 0.993206\tvalid_0's ndcg@10: 0.993206\tvalid_0's ndcg@20: 0.993206\tvalid_0's ndcg@30: 0.993206\tvalid_0's ndcg@40: 0.993206\tvalid_0's ndcg@50: 0.993206\n","[63]\tvalid_0's ndcg@10: 0.993268\tvalid_0's ndcg@20: 0.993268\tvalid_0's ndcg@30: 0.993268\tvalid_0's ndcg@40: 0.993268\tvalid_0's ndcg@50: 0.993268\tvalid_0's ndcg@10: 0.993268\tvalid_0's ndcg@20: 0.993268\tvalid_0's ndcg@30: 0.993268\tvalid_0's ndcg@40: 0.993268\tvalid_0's ndcg@50: 0.993268\n","[64]\tvalid_0's ndcg@10: 0.993215\tvalid_0's ndcg@20: 0.993215\tvalid_0's ndcg@30: 0.993215\tvalid_0's ndcg@40: 0.993215\tvalid_0's ndcg@50: 0.993215\tvalid_0's ndcg@10: 0.993215\tvalid_0's ndcg@20: 0.993215\tvalid_0's ndcg@30: 0.993215\tvalid_0's ndcg@40: 0.993215\tvalid_0's ndcg@50: 0.993215\n","[65]\tvalid_0's ndcg@10: 0.993195\tvalid_0's ndcg@20: 0.993195\tvalid_0's ndcg@30: 0.993195\tvalid_0's ndcg@40: 0.993195\tvalid_0's ndcg@50: 0.993195\tvalid_0's ndcg@10: 0.993195\tvalid_0's ndcg@20: 0.993195\tvalid_0's ndcg@30: 0.993195\tvalid_0's ndcg@40: 0.993195\tvalid_0's ndcg@50: 0.993195\n","[66]\tvalid_0's ndcg@10: 0.993153\tvalid_0's ndcg@20: 0.993153\tvalid_0's ndcg@30: 0.993153\tvalid_0's ndcg@40: 0.993153\tvalid_0's ndcg@50: 0.993153\tvalid_0's ndcg@10: 0.993153\tvalid_0's ndcg@20: 0.993153\tvalid_0's ndcg@30: 0.993153\tvalid_0's ndcg@40: 0.993153\tvalid_0's ndcg@50: 0.993153\n","[67]\tvalid_0's ndcg@10: 0.993027\tvalid_0's ndcg@20: 0.993027\tvalid_0's ndcg@30: 0.993027\tvalid_0's ndcg@40: 0.993027\tvalid_0's ndcg@50: 0.993027\tvalid_0's ndcg@10: 0.993027\tvalid_0's ndcg@20: 0.993027\tvalid_0's ndcg@30: 0.993027\tvalid_0's ndcg@40: 0.993027\tvalid_0's ndcg@50: 0.993027\n","[68]\tvalid_0's ndcg@10: 0.993108\tvalid_0's ndcg@20: 0.993108\tvalid_0's ndcg@30: 0.993108\tvalid_0's ndcg@40: 0.993108\tvalid_0's ndcg@50: 0.993108\tvalid_0's ndcg@10: 0.993108\tvalid_0's ndcg@20: 0.993108\tvalid_0's ndcg@30: 0.993108\tvalid_0's ndcg@40: 0.993108\tvalid_0's ndcg@50: 0.993108\n","[69]\tvalid_0's ndcg@10: 0.993164\tvalid_0's ndcg@20: 0.993164\tvalid_0's ndcg@30: 0.993164\tvalid_0's ndcg@40: 0.993164\tvalid_0's ndcg@50: 0.993164\tvalid_0's ndcg@10: 0.993164\tvalid_0's ndcg@20: 0.993164\tvalid_0's ndcg@30: 0.993164\tvalid_0's ndcg@40: 0.993164\tvalid_0's ndcg@50: 0.993164\n","[70]\tvalid_0's ndcg@10: 0.993142\tvalid_0's ndcg@20: 0.993142\tvalid_0's ndcg@30: 0.993142\tvalid_0's ndcg@40: 0.993142\tvalid_0's ndcg@50: 0.993142\tvalid_0's ndcg@10: 0.993142\tvalid_0's ndcg@20: 0.993142\tvalid_0's ndcg@30: 0.993142\tvalid_0's ndcg@40: 0.993142\tvalid_0's ndcg@50: 0.993142\n","[71]\tvalid_0's ndcg@10: 0.993172\tvalid_0's ndcg@20: 0.993172\tvalid_0's ndcg@30: 0.993172\tvalid_0's ndcg@40: 0.993172\tvalid_0's ndcg@50: 0.993172\tvalid_0's ndcg@10: 0.993172\tvalid_0's ndcg@20: 0.993172\tvalid_0's ndcg@30: 0.993172\tvalid_0's ndcg@40: 0.993172\tvalid_0's ndcg@50: 0.993172\n","[72]\tvalid_0's ndcg@10: 0.993205\tvalid_0's ndcg@20: 0.993205\tvalid_0's ndcg@30: 0.993205\tvalid_0's ndcg@40: 0.993205\tvalid_0's ndcg@50: 0.993205\tvalid_0's ndcg@10: 0.993205\tvalid_0's ndcg@20: 0.993205\tvalid_0's ndcg@30: 0.993205\tvalid_0's ndcg@40: 0.993205\tvalid_0's ndcg@50: 0.993205\n","[73]\tvalid_0's ndcg@10: 0.993216\tvalid_0's ndcg@20: 0.993216\tvalid_0's ndcg@30: 0.993216\tvalid_0's ndcg@40: 0.993216\tvalid_0's ndcg@50: 0.993216\tvalid_0's ndcg@10: 0.993216\tvalid_0's ndcg@20: 0.993216\tvalid_0's ndcg@30: 0.993216\tvalid_0's ndcg@40: 0.993216\tvalid_0's ndcg@50: 0.993216\n","[74]\tvalid_0's ndcg@10: 0.993159\tvalid_0's ndcg@20: 0.993159\tvalid_0's ndcg@30: 0.993159\tvalid_0's ndcg@40: 0.993159\tvalid_0's ndcg@50: 0.993159\tvalid_0's ndcg@10: 0.993159\tvalid_0's ndcg@20: 0.993159\tvalid_0's ndcg@30: 0.993159\tvalid_0's ndcg@40: 0.993159\tvalid_0's ndcg@50: 0.993159\n","[75]\tvalid_0's ndcg@10: 0.993197\tvalid_0's ndcg@20: 0.993197\tvalid_0's ndcg@30: 0.993197\tvalid_0's ndcg@40: 0.993197\tvalid_0's ndcg@50: 0.993197\tvalid_0's ndcg@10: 0.993197\tvalid_0's ndcg@20: 0.993197\tvalid_0's ndcg@30: 0.993197\tvalid_0's ndcg@40: 0.993197\tvalid_0's ndcg@50: 0.993197\n","Early stopping, best iteration is:\n","[25]\tvalid_0's ndcg@10: 0.993409\tvalid_0's ndcg@20: 0.993409\tvalid_0's ndcg@30: 0.993409\tvalid_0's ndcg@40: 0.993409\tvalid_0's ndcg@50: 0.993409\tvalid_0's ndcg@10: 0.993409\tvalid_0's ndcg@20: 0.993409\tvalid_0's ndcg@30: 0.993409\tvalid_0's ndcg@40: 0.993409\tvalid_0's ndcg@50: 0.993409\n"]}]},{"cell_type":"code","source":["!python src/rank/GBDT_rank.py --task srgnn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8mSX5Fm43_xu","executionInfo":{"status":"ok","timestamp":1639076309485,"user_tz":-480,"elapsed":36551,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"c45d4ad3-a88d-467f-ec0f-13b06a9f12f9"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["[1]\tvalid_0's ndcg@10: 0.960458\tvalid_0's ndcg@20: 0.960589\tvalid_0's ndcg@30: 0.960589\tvalid_0's ndcg@40: 0.960589\tvalid_0's ndcg@50: 0.960589\tvalid_0's ndcg@10: 0.960458\tvalid_0's ndcg@20: 0.960589\tvalid_0's ndcg@30: 0.960589\tvalid_0's ndcg@40: 0.960589\tvalid_0's ndcg@50: 0.960589\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.962617\tvalid_0's ndcg@20: 0.962705\tvalid_0's ndcg@30: 0.962705\tvalid_0's ndcg@40: 0.962705\tvalid_0's ndcg@50: 0.962705\tvalid_0's ndcg@10: 0.962617\tvalid_0's ndcg@20: 0.962705\tvalid_0's ndcg@30: 0.962705\tvalid_0's ndcg@40: 0.962705\tvalid_0's ndcg@50: 0.962705\n","[3]\tvalid_0's ndcg@10: 0.962797\tvalid_0's ndcg@20: 0.962842\tvalid_0's ndcg@30: 0.962842\tvalid_0's ndcg@40: 0.962842\tvalid_0's ndcg@50: 0.962842\tvalid_0's ndcg@10: 0.962797\tvalid_0's ndcg@20: 0.962842\tvalid_0's ndcg@30: 0.962842\tvalid_0's ndcg@40: 0.962842\tvalid_0's ndcg@50: 0.962842\n","[4]\tvalid_0's ndcg@10: 0.963802\tvalid_0's ndcg@20: 0.963894\tvalid_0's ndcg@30: 0.963894\tvalid_0's ndcg@40: 0.963894\tvalid_0's ndcg@50: 0.963894\tvalid_0's ndcg@10: 0.963802\tvalid_0's ndcg@20: 0.963894\tvalid_0's ndcg@30: 0.963894\tvalid_0's ndcg@40: 0.963894\tvalid_0's ndcg@50: 0.963894\n","[5]\tvalid_0's ndcg@10: 0.964283\tvalid_0's ndcg@20: 0.964376\tvalid_0's ndcg@30: 0.964376\tvalid_0's ndcg@40: 0.964376\tvalid_0's ndcg@50: 0.964376\tvalid_0's ndcg@10: 0.964283\tvalid_0's ndcg@20: 0.964376\tvalid_0's ndcg@30: 0.964376\tvalid_0's ndcg@40: 0.964376\tvalid_0's ndcg@50: 0.964376\n","[6]\tvalid_0's ndcg@10: 0.96463\tvalid_0's ndcg@20: 0.96463\tvalid_0's ndcg@30: 0.96463\tvalid_0's ndcg@40: 0.96463\tvalid_0's ndcg@50: 0.96463\tvalid_0's ndcg@10: 0.96463\tvalid_0's ndcg@20: 0.96463\tvalid_0's ndcg@30: 0.96463\tvalid_0's ndcg@40: 0.96463\tvalid_0's ndcg@50: 0.96463\n","[7]\tvalid_0's ndcg@10: 0.963423\tvalid_0's ndcg@20: 0.963423\tvalid_0's ndcg@30: 0.963423\tvalid_0's ndcg@40: 0.963423\tvalid_0's ndcg@50: 0.963423\tvalid_0's ndcg@10: 0.963423\tvalid_0's ndcg@20: 0.963423\tvalid_0's ndcg@30: 0.963423\tvalid_0's ndcg@40: 0.963423\tvalid_0's ndcg@50: 0.963423\n","[8]\tvalid_0's ndcg@10: 0.963783\tvalid_0's ndcg@20: 0.963831\tvalid_0's ndcg@30: 0.963831\tvalid_0's ndcg@40: 0.963831\tvalid_0's ndcg@50: 0.963831\tvalid_0's ndcg@10: 0.963783\tvalid_0's ndcg@20: 0.963831\tvalid_0's ndcg@30: 0.963831\tvalid_0's ndcg@40: 0.963831\tvalid_0's ndcg@50: 0.963831\n","[9]\tvalid_0's ndcg@10: 0.963994\tvalid_0's ndcg@20: 0.964042\tvalid_0's ndcg@30: 0.964042\tvalid_0's ndcg@40: 0.964042\tvalid_0's ndcg@50: 0.964042\tvalid_0's ndcg@10: 0.963994\tvalid_0's ndcg@20: 0.964042\tvalid_0's ndcg@30: 0.964042\tvalid_0's ndcg@40: 0.964042\tvalid_0's ndcg@50: 0.964042\n","[10]\tvalid_0's ndcg@10: 0.964619\tvalid_0's ndcg@20: 0.964619\tvalid_0's ndcg@30: 0.964619\tvalid_0's ndcg@40: 0.964619\tvalid_0's ndcg@50: 0.964619\tvalid_0's ndcg@10: 0.964619\tvalid_0's ndcg@20: 0.964619\tvalid_0's ndcg@30: 0.964619\tvalid_0's ndcg@40: 0.964619\tvalid_0's ndcg@50: 0.964619\n","[11]\tvalid_0's ndcg@10: 0.964351\tvalid_0's ndcg@20: 0.964351\tvalid_0's ndcg@30: 0.964351\tvalid_0's ndcg@40: 0.964351\tvalid_0's ndcg@50: 0.964351\tvalid_0's ndcg@10: 0.964351\tvalid_0's ndcg@20: 0.964351\tvalid_0's ndcg@30: 0.964351\tvalid_0's ndcg@40: 0.964351\tvalid_0's ndcg@50: 0.964351\n","[12]\tvalid_0's ndcg@10: 0.964566\tvalid_0's ndcg@20: 0.964566\tvalid_0's ndcg@30: 0.964566\tvalid_0's ndcg@40: 0.964566\tvalid_0's ndcg@50: 0.964566\tvalid_0's ndcg@10: 0.964566\tvalid_0's ndcg@20: 0.964566\tvalid_0's ndcg@30: 0.964566\tvalid_0's ndcg@40: 0.964566\tvalid_0's ndcg@50: 0.964566\n","[13]\tvalid_0's ndcg@10: 0.965113\tvalid_0's ndcg@20: 0.965113\tvalid_0's ndcg@30: 0.965113\tvalid_0's ndcg@40: 0.965113\tvalid_0's ndcg@50: 0.965113\tvalid_0's ndcg@10: 0.965113\tvalid_0's ndcg@20: 0.965113\tvalid_0's ndcg@30: 0.965113\tvalid_0's ndcg@40: 0.965113\tvalid_0's ndcg@50: 0.965113\n","[14]\tvalid_0's ndcg@10: 0.964452\tvalid_0's ndcg@20: 0.964452\tvalid_0's ndcg@30: 0.964452\tvalid_0's ndcg@40: 0.964452\tvalid_0's ndcg@50: 0.964452\tvalid_0's ndcg@10: 0.964452\tvalid_0's ndcg@20: 0.964452\tvalid_0's ndcg@30: 0.964452\tvalid_0's ndcg@40: 0.964452\tvalid_0's ndcg@50: 0.964452\n","[15]\tvalid_0's ndcg@10: 0.964886\tvalid_0's ndcg@20: 0.964886\tvalid_0's ndcg@30: 0.964886\tvalid_0's ndcg@40: 0.964886\tvalid_0's ndcg@50: 0.964886\tvalid_0's ndcg@10: 0.964886\tvalid_0's ndcg@20: 0.964886\tvalid_0's ndcg@30: 0.964886\tvalid_0's ndcg@40: 0.964886\tvalid_0's ndcg@50: 0.964886\n","[16]\tvalid_0's ndcg@10: 0.965287\tvalid_0's ndcg@20: 0.965287\tvalid_0's ndcg@30: 0.965287\tvalid_0's ndcg@40: 0.965287\tvalid_0's ndcg@50: 0.965287\tvalid_0's ndcg@10: 0.965287\tvalid_0's ndcg@20: 0.965287\tvalid_0's ndcg@30: 0.965287\tvalid_0's ndcg@40: 0.965287\tvalid_0's ndcg@50: 0.965287\n","[17]\tvalid_0's ndcg@10: 0.964984\tvalid_0's ndcg@20: 0.964984\tvalid_0's ndcg@30: 0.964984\tvalid_0's ndcg@40: 0.964984\tvalid_0's ndcg@50: 0.964984\tvalid_0's ndcg@10: 0.964984\tvalid_0's ndcg@20: 0.964984\tvalid_0's ndcg@30: 0.964984\tvalid_0's ndcg@40: 0.964984\tvalid_0's ndcg@50: 0.964984\n","[18]\tvalid_0's ndcg@10: 0.965101\tvalid_0's ndcg@20: 0.965101\tvalid_0's ndcg@30: 0.965101\tvalid_0's ndcg@40: 0.965101\tvalid_0's ndcg@50: 0.965101\tvalid_0's ndcg@10: 0.965101\tvalid_0's ndcg@20: 0.965101\tvalid_0's ndcg@30: 0.965101\tvalid_0's ndcg@40: 0.965101\tvalid_0's ndcg@50: 0.965101\n","[19]\tvalid_0's ndcg@10: 0.964927\tvalid_0's ndcg@20: 0.964927\tvalid_0's ndcg@30: 0.964927\tvalid_0's ndcg@40: 0.964927\tvalid_0's ndcg@50: 0.964927\tvalid_0's ndcg@10: 0.964927\tvalid_0's ndcg@20: 0.964927\tvalid_0's ndcg@30: 0.964927\tvalid_0's ndcg@40: 0.964927\tvalid_0's ndcg@50: 0.964927\n","[20]\tvalid_0's ndcg@10: 0.965073\tvalid_0's ndcg@20: 0.965073\tvalid_0's ndcg@30: 0.965073\tvalid_0's ndcg@40: 0.965073\tvalid_0's ndcg@50: 0.965073\tvalid_0's ndcg@10: 0.965073\tvalid_0's ndcg@20: 0.965073\tvalid_0's ndcg@30: 0.965073\tvalid_0's ndcg@40: 0.965073\tvalid_0's ndcg@50: 0.965073\n","[21]\tvalid_0's ndcg@10: 0.965236\tvalid_0's ndcg@20: 0.965236\tvalid_0's ndcg@30: 0.965236\tvalid_0's ndcg@40: 0.965236\tvalid_0's ndcg@50: 0.965236\tvalid_0's ndcg@10: 0.965236\tvalid_0's ndcg@20: 0.965236\tvalid_0's ndcg@30: 0.965236\tvalid_0's ndcg@40: 0.965236\tvalid_0's ndcg@50: 0.965236\n","[22]\tvalid_0's ndcg@10: 0.965322\tvalid_0's ndcg@20: 0.965322\tvalid_0's ndcg@30: 0.965322\tvalid_0's ndcg@40: 0.965322\tvalid_0's ndcg@50: 0.965322\tvalid_0's ndcg@10: 0.965322\tvalid_0's ndcg@20: 0.965322\tvalid_0's ndcg@30: 0.965322\tvalid_0's ndcg@40: 0.965322\tvalid_0's ndcg@50: 0.965322\n","[23]\tvalid_0's ndcg@10: 0.96493\tvalid_0's ndcg@20: 0.964977\tvalid_0's ndcg@30: 0.964977\tvalid_0's ndcg@40: 0.964977\tvalid_0's ndcg@50: 0.964977\tvalid_0's ndcg@10: 0.96493\tvalid_0's ndcg@20: 0.964977\tvalid_0's ndcg@30: 0.964977\tvalid_0's ndcg@40: 0.964977\tvalid_0's ndcg@50: 0.964977\n","[24]\tvalid_0's ndcg@10: 0.964705\tvalid_0's ndcg@20: 0.964752\tvalid_0's ndcg@30: 0.964752\tvalid_0's ndcg@40: 0.964752\tvalid_0's ndcg@50: 0.964752\tvalid_0's ndcg@10: 0.964705\tvalid_0's ndcg@20: 0.964752\tvalid_0's ndcg@30: 0.964752\tvalid_0's ndcg@40: 0.964752\tvalid_0's ndcg@50: 0.964752\n","[25]\tvalid_0's ndcg@10: 0.964548\tvalid_0's ndcg@20: 0.964596\tvalid_0's ndcg@30: 0.964596\tvalid_0's ndcg@40: 0.964596\tvalid_0's ndcg@50: 0.964596\tvalid_0's ndcg@10: 0.964548\tvalid_0's ndcg@20: 0.964596\tvalid_0's ndcg@30: 0.964596\tvalid_0's ndcg@40: 0.964596\tvalid_0's ndcg@50: 0.964596\n","[26]\tvalid_0's ndcg@10: 0.964639\tvalid_0's ndcg@20: 0.964639\tvalid_0's ndcg@30: 0.964639\tvalid_0's ndcg@40: 0.964639\tvalid_0's ndcg@50: 0.964639\tvalid_0's ndcg@10: 0.964639\tvalid_0's ndcg@20: 0.964639\tvalid_0's ndcg@30: 0.964639\tvalid_0's ndcg@40: 0.964639\tvalid_0's ndcg@50: 0.964639\n","[27]\tvalid_0's ndcg@10: 0.964523\tvalid_0's ndcg@20: 0.964523\tvalid_0's ndcg@30: 0.964523\tvalid_0's ndcg@40: 0.964523\tvalid_0's ndcg@50: 0.964523\tvalid_0's ndcg@10: 0.964523\tvalid_0's ndcg@20: 0.964523\tvalid_0's ndcg@30: 0.964523\tvalid_0's ndcg@40: 0.964523\tvalid_0's ndcg@50: 0.964523\n","[28]\tvalid_0's ndcg@10: 0.964423\tvalid_0's ndcg@20: 0.964423\tvalid_0's ndcg@30: 0.964423\tvalid_0's ndcg@40: 0.964423\tvalid_0's ndcg@50: 0.964423\tvalid_0's ndcg@10: 0.964423\tvalid_0's ndcg@20: 0.964423\tvalid_0's ndcg@30: 0.964423\tvalid_0's ndcg@40: 0.964423\tvalid_0's ndcg@50: 0.964423\n","[29]\tvalid_0's ndcg@10: 0.964674\tvalid_0's ndcg@20: 0.964674\tvalid_0's ndcg@30: 0.964674\tvalid_0's ndcg@40: 0.964674\tvalid_0's ndcg@50: 0.964674\tvalid_0's ndcg@10: 0.964674\tvalid_0's ndcg@20: 0.964674\tvalid_0's ndcg@30: 0.964674\tvalid_0's ndcg@40: 0.964674\tvalid_0's ndcg@50: 0.964674\n","[30]\tvalid_0's ndcg@10: 0.964669\tvalid_0's ndcg@20: 0.964669\tvalid_0's ndcg@30: 0.964669\tvalid_0's ndcg@40: 0.964669\tvalid_0's ndcg@50: 0.964669\tvalid_0's ndcg@10: 0.964669\tvalid_0's ndcg@20: 0.964669\tvalid_0's ndcg@30: 0.964669\tvalid_0's ndcg@40: 0.964669\tvalid_0's ndcg@50: 0.964669\n","[31]\tvalid_0's ndcg@10: 0.964915\tvalid_0's ndcg@20: 0.964915\tvalid_0's ndcg@30: 0.964915\tvalid_0's ndcg@40: 0.964915\tvalid_0's ndcg@50: 0.964915\tvalid_0's ndcg@10: 0.964915\tvalid_0's ndcg@20: 0.964915\tvalid_0's ndcg@30: 0.964915\tvalid_0's ndcg@40: 0.964915\tvalid_0's ndcg@50: 0.964915\n","[32]\tvalid_0's ndcg@10: 0.964992\tvalid_0's ndcg@20: 0.964992\tvalid_0's ndcg@30: 0.964992\tvalid_0's ndcg@40: 0.964992\tvalid_0's ndcg@50: 0.964992\tvalid_0's ndcg@10: 0.964992\tvalid_0's ndcg@20: 0.964992\tvalid_0's ndcg@30: 0.964992\tvalid_0's ndcg@40: 0.964992\tvalid_0's ndcg@50: 0.964992\n","[33]\tvalid_0's ndcg@10: 0.964641\tvalid_0's ndcg@20: 0.964641\tvalid_0's ndcg@30: 0.964641\tvalid_0's ndcg@40: 0.964641\tvalid_0's ndcg@50: 0.964641\tvalid_0's ndcg@10: 0.964641\tvalid_0's ndcg@20: 0.964641\tvalid_0's ndcg@30: 0.964641\tvalid_0's ndcg@40: 0.964641\tvalid_0's ndcg@50: 0.964641\n","[34]\tvalid_0's ndcg@10: 0.964722\tvalid_0's ndcg@20: 0.964722\tvalid_0's ndcg@30: 0.964722\tvalid_0's ndcg@40: 0.964722\tvalid_0's ndcg@50: 0.964722\tvalid_0's ndcg@10: 0.964722\tvalid_0's ndcg@20: 0.964722\tvalid_0's ndcg@30: 0.964722\tvalid_0's ndcg@40: 0.964722\tvalid_0's ndcg@50: 0.964722\n","[35]\tvalid_0's ndcg@10: 0.964867\tvalid_0's ndcg@20: 0.964867\tvalid_0's ndcg@30: 0.964867\tvalid_0's ndcg@40: 0.964867\tvalid_0's ndcg@50: 0.964867\tvalid_0's ndcg@10: 0.964867\tvalid_0's ndcg@20: 0.964867\tvalid_0's ndcg@30: 0.964867\tvalid_0's ndcg@40: 0.964867\tvalid_0's ndcg@50: 0.964867\n","[36]\tvalid_0's ndcg@10: 0.964733\tvalid_0's ndcg@20: 0.964733\tvalid_0's ndcg@30: 0.964733\tvalid_0's ndcg@40: 0.964733\tvalid_0's ndcg@50: 0.964733\tvalid_0's ndcg@10: 0.964733\tvalid_0's ndcg@20: 0.964733\tvalid_0's ndcg@30: 0.964733\tvalid_0's ndcg@40: 0.964733\tvalid_0's ndcg@50: 0.964733\n","[37]\tvalid_0's ndcg@10: 0.96484\tvalid_0's ndcg@20: 0.96484\tvalid_0's ndcg@30: 0.96484\tvalid_0's ndcg@40: 0.96484\tvalid_0's ndcg@50: 0.96484\tvalid_0's ndcg@10: 0.96484\tvalid_0's ndcg@20: 0.96484\tvalid_0's ndcg@30: 0.96484\tvalid_0's ndcg@40: 0.96484\tvalid_0's ndcg@50: 0.96484\n","[38]\tvalid_0's ndcg@10: 0.964777\tvalid_0's ndcg@20: 0.964777\tvalid_0's ndcg@30: 0.964777\tvalid_0's ndcg@40: 0.964777\tvalid_0's ndcg@50: 0.964777\tvalid_0's ndcg@10: 0.964777\tvalid_0's ndcg@20: 0.964777\tvalid_0's ndcg@30: 0.964777\tvalid_0's ndcg@40: 0.964777\tvalid_0's ndcg@50: 0.964777\n","[39]\tvalid_0's ndcg@10: 0.964983\tvalid_0's ndcg@20: 0.964983\tvalid_0's ndcg@30: 0.964983\tvalid_0's ndcg@40: 0.964983\tvalid_0's ndcg@50: 0.964983\tvalid_0's ndcg@10: 0.964983\tvalid_0's ndcg@20: 0.964983\tvalid_0's ndcg@30: 0.964983\tvalid_0's ndcg@40: 0.964983\tvalid_0's ndcg@50: 0.964983\n","[40]\tvalid_0's ndcg@10: 0.965102\tvalid_0's ndcg@20: 0.965102\tvalid_0's ndcg@30: 0.965102\tvalid_0's ndcg@40: 0.965102\tvalid_0's ndcg@50: 0.965102\tvalid_0's ndcg@10: 0.965102\tvalid_0's ndcg@20: 0.965102\tvalid_0's ndcg@30: 0.965102\tvalid_0's ndcg@40: 0.965102\tvalid_0's ndcg@50: 0.965102\n","[41]\tvalid_0's ndcg@10: 0.965016\tvalid_0's ndcg@20: 0.965016\tvalid_0's ndcg@30: 0.965016\tvalid_0's ndcg@40: 0.965016\tvalid_0's ndcg@50: 0.965016\tvalid_0's ndcg@10: 0.965016\tvalid_0's ndcg@20: 0.965016\tvalid_0's ndcg@30: 0.965016\tvalid_0's ndcg@40: 0.965016\tvalid_0's ndcg@50: 0.965016\n","[42]\tvalid_0's ndcg@10: 0.964855\tvalid_0's ndcg@20: 0.964855\tvalid_0's ndcg@30: 0.964855\tvalid_0's ndcg@40: 0.964855\tvalid_0's ndcg@50: 0.964855\tvalid_0's ndcg@10: 0.964855\tvalid_0's ndcg@20: 0.964855\tvalid_0's ndcg@30: 0.964855\tvalid_0's ndcg@40: 0.964855\tvalid_0's ndcg@50: 0.964855\n","[43]\tvalid_0's ndcg@10: 0.964969\tvalid_0's ndcg@20: 0.964969\tvalid_0's ndcg@30: 0.964969\tvalid_0's ndcg@40: 0.964969\tvalid_0's ndcg@50: 0.964969\tvalid_0's ndcg@10: 0.964969\tvalid_0's ndcg@20: 0.964969\tvalid_0's ndcg@30: 0.964969\tvalid_0's ndcg@40: 0.964969\tvalid_0's ndcg@50: 0.964969\n","[44]\tvalid_0's ndcg@10: 0.965103\tvalid_0's ndcg@20: 0.965103\tvalid_0's ndcg@30: 0.965103\tvalid_0's ndcg@40: 0.965103\tvalid_0's ndcg@50: 0.965103\tvalid_0's ndcg@10: 0.965103\tvalid_0's ndcg@20: 0.965103\tvalid_0's ndcg@30: 0.965103\tvalid_0's ndcg@40: 0.965103\tvalid_0's ndcg@50: 0.965103\n","[45]\tvalid_0's ndcg@10: 0.964758\tvalid_0's ndcg@20: 0.964758\tvalid_0's ndcg@30: 0.964758\tvalid_0's ndcg@40: 0.964758\tvalid_0's ndcg@50: 0.964758\tvalid_0's ndcg@10: 0.964758\tvalid_0's ndcg@20: 0.964758\tvalid_0's ndcg@30: 0.964758\tvalid_0's ndcg@40: 0.964758\tvalid_0's ndcg@50: 0.964758\n","[46]\tvalid_0's ndcg@10: 0.964832\tvalid_0's ndcg@20: 0.964832\tvalid_0's ndcg@30: 0.964832\tvalid_0's ndcg@40: 0.964832\tvalid_0's ndcg@50: 0.964832\tvalid_0's ndcg@10: 0.964832\tvalid_0's ndcg@20: 0.964832\tvalid_0's ndcg@30: 0.964832\tvalid_0's ndcg@40: 0.964832\tvalid_0's ndcg@50: 0.964832\n","[47]\tvalid_0's ndcg@10: 0.964994\tvalid_0's ndcg@20: 0.964994\tvalid_0's ndcg@30: 0.964994\tvalid_0's ndcg@40: 0.964994\tvalid_0's ndcg@50: 0.964994\tvalid_0's ndcg@10: 0.964994\tvalid_0's ndcg@20: 0.964994\tvalid_0's ndcg@30: 0.964994\tvalid_0's ndcg@40: 0.964994\tvalid_0's ndcg@50: 0.964994\n","[48]\tvalid_0's ndcg@10: 0.964833\tvalid_0's ndcg@20: 0.964833\tvalid_0's ndcg@30: 0.964833\tvalid_0's ndcg@40: 0.964833\tvalid_0's ndcg@50: 0.964833\tvalid_0's ndcg@10: 0.964833\tvalid_0's ndcg@20: 0.964833\tvalid_0's ndcg@30: 0.964833\tvalid_0's ndcg@40: 0.964833\tvalid_0's ndcg@50: 0.964833\n","[49]\tvalid_0's ndcg@10: 0.964762\tvalid_0's ndcg@20: 0.964762\tvalid_0's ndcg@30: 0.964762\tvalid_0's ndcg@40: 0.964762\tvalid_0's ndcg@50: 0.964762\tvalid_0's ndcg@10: 0.964762\tvalid_0's ndcg@20: 0.964762\tvalid_0's ndcg@30: 0.964762\tvalid_0's ndcg@40: 0.964762\tvalid_0's ndcg@50: 0.964762\n","[50]\tvalid_0's ndcg@10: 0.964722\tvalid_0's ndcg@20: 0.964722\tvalid_0's ndcg@30: 0.964722\tvalid_0's ndcg@40: 0.964722\tvalid_0's ndcg@50: 0.964722\tvalid_0's ndcg@10: 0.964722\tvalid_0's ndcg@20: 0.964722\tvalid_0's ndcg@30: 0.964722\tvalid_0's ndcg@40: 0.964722\tvalid_0's ndcg@50: 0.964722\n","[51]\tvalid_0's ndcg@10: 0.964762\tvalid_0's ndcg@20: 0.964762\tvalid_0's ndcg@30: 0.964762\tvalid_0's ndcg@40: 0.964762\tvalid_0's ndcg@50: 0.964762\tvalid_0's ndcg@10: 0.964762\tvalid_0's ndcg@20: 0.964762\tvalid_0's ndcg@30: 0.964762\tvalid_0's ndcg@40: 0.964762\tvalid_0's ndcg@50: 0.964762\n","[52]\tvalid_0's ndcg@10: 0.964627\tvalid_0's ndcg@20: 0.964627\tvalid_0's ndcg@30: 0.964627\tvalid_0's ndcg@40: 0.964627\tvalid_0's ndcg@50: 0.964627\tvalid_0's ndcg@10: 0.964627\tvalid_0's ndcg@20: 0.964627\tvalid_0's ndcg@30: 0.964627\tvalid_0's ndcg@40: 0.964627\tvalid_0's ndcg@50: 0.964627\n","[53]\tvalid_0's ndcg@10: 0.96463\tvalid_0's ndcg@20: 0.96463\tvalid_0's ndcg@30: 0.96463\tvalid_0's ndcg@40: 0.96463\tvalid_0's ndcg@50: 0.96463\tvalid_0's ndcg@10: 0.96463\tvalid_0's ndcg@20: 0.96463\tvalid_0's ndcg@30: 0.96463\tvalid_0's ndcg@40: 0.96463\tvalid_0's ndcg@50: 0.96463\n","[54]\tvalid_0's ndcg@10: 0.964759\tvalid_0's ndcg@20: 0.964759\tvalid_0's ndcg@30: 0.964759\tvalid_0's ndcg@40: 0.964759\tvalid_0's ndcg@50: 0.964759\tvalid_0's ndcg@10: 0.964759\tvalid_0's ndcg@20: 0.964759\tvalid_0's ndcg@30: 0.964759\tvalid_0's ndcg@40: 0.964759\tvalid_0's ndcg@50: 0.964759\n","[55]\tvalid_0's ndcg@10: 0.964621\tvalid_0's ndcg@20: 0.964621\tvalid_0's ndcg@30: 0.964621\tvalid_0's ndcg@40: 0.964621\tvalid_0's ndcg@50: 0.964621\tvalid_0's ndcg@10: 0.964621\tvalid_0's ndcg@20: 0.964621\tvalid_0's ndcg@30: 0.964621\tvalid_0's ndcg@40: 0.964621\tvalid_0's ndcg@50: 0.964621\n","[56]\tvalid_0's ndcg@10: 0.964859\tvalid_0's ndcg@20: 0.964859\tvalid_0's ndcg@30: 0.964859\tvalid_0's ndcg@40: 0.964859\tvalid_0's ndcg@50: 0.964859\tvalid_0's ndcg@10: 0.964859\tvalid_0's ndcg@20: 0.964859\tvalid_0's ndcg@30: 0.964859\tvalid_0's ndcg@40: 0.964859\tvalid_0's ndcg@50: 0.964859\n","[57]\tvalid_0's ndcg@10: 0.964961\tvalid_0's ndcg@20: 0.964961\tvalid_0's ndcg@30: 0.964961\tvalid_0's ndcg@40: 0.964961\tvalid_0's ndcg@50: 0.964961\tvalid_0's ndcg@10: 0.964961\tvalid_0's ndcg@20: 0.964961\tvalid_0's ndcg@30: 0.964961\tvalid_0's ndcg@40: 0.964961\tvalid_0's ndcg@50: 0.964961\n","[58]\tvalid_0's ndcg@10: 0.965062\tvalid_0's ndcg@20: 0.965062\tvalid_0's ndcg@30: 0.965062\tvalid_0's ndcg@40: 0.965062\tvalid_0's ndcg@50: 0.965062\tvalid_0's ndcg@10: 0.965062\tvalid_0's ndcg@20: 0.965062\tvalid_0's ndcg@30: 0.965062\tvalid_0's ndcg@40: 0.965062\tvalid_0's ndcg@50: 0.965062\n","[59]\tvalid_0's ndcg@10: 0.965222\tvalid_0's ndcg@20: 0.965222\tvalid_0's ndcg@30: 0.965222\tvalid_0's ndcg@40: 0.965222\tvalid_0's ndcg@50: 0.965222\tvalid_0's ndcg@10: 0.965222\tvalid_0's ndcg@20: 0.965222\tvalid_0's ndcg@30: 0.965222\tvalid_0's ndcg@40: 0.965222\tvalid_0's ndcg@50: 0.965222\n","[60]\tvalid_0's ndcg@10: 0.965244\tvalid_0's ndcg@20: 0.965244\tvalid_0's ndcg@30: 0.965244\tvalid_0's ndcg@40: 0.965244\tvalid_0's ndcg@50: 0.965244\tvalid_0's ndcg@10: 0.965244\tvalid_0's ndcg@20: 0.965244\tvalid_0's ndcg@30: 0.965244\tvalid_0's ndcg@40: 0.965244\tvalid_0's ndcg@50: 0.965244\n","[61]\tvalid_0's ndcg@10: 0.96503\tvalid_0's ndcg@20: 0.96503\tvalid_0's ndcg@30: 0.96503\tvalid_0's ndcg@40: 0.96503\tvalid_0's ndcg@50: 0.96503\tvalid_0's ndcg@10: 0.96503\tvalid_0's ndcg@20: 0.96503\tvalid_0's ndcg@30: 0.96503\tvalid_0's ndcg@40: 0.96503\tvalid_0's ndcg@50: 0.96503\n","[62]\tvalid_0's ndcg@10: 0.964944\tvalid_0's ndcg@20: 0.964944\tvalid_0's ndcg@30: 0.964944\tvalid_0's ndcg@40: 0.964944\tvalid_0's ndcg@50: 0.964944\tvalid_0's ndcg@10: 0.964944\tvalid_0's ndcg@20: 0.964944\tvalid_0's ndcg@30: 0.964944\tvalid_0's ndcg@40: 0.964944\tvalid_0's ndcg@50: 0.964944\n","[63]\tvalid_0's ndcg@10: 0.964882\tvalid_0's ndcg@20: 0.964882\tvalid_0's ndcg@30: 0.964882\tvalid_0's ndcg@40: 0.964882\tvalid_0's ndcg@50: 0.964882\tvalid_0's ndcg@10: 0.964882\tvalid_0's ndcg@20: 0.964882\tvalid_0's ndcg@30: 0.964882\tvalid_0's ndcg@40: 0.964882\tvalid_0's ndcg@50: 0.964882\n","[64]\tvalid_0's ndcg@10: 0.965099\tvalid_0's ndcg@20: 0.965099\tvalid_0's ndcg@30: 0.965099\tvalid_0's ndcg@40: 0.965099\tvalid_0's ndcg@50: 0.965099\tvalid_0's ndcg@10: 0.965099\tvalid_0's ndcg@20: 0.965099\tvalid_0's ndcg@30: 0.965099\tvalid_0's ndcg@40: 0.965099\tvalid_0's ndcg@50: 0.965099\n","[65]\tvalid_0's ndcg@10: 0.964985\tvalid_0's ndcg@20: 0.964985\tvalid_0's ndcg@30: 0.964985\tvalid_0's ndcg@40: 0.964985\tvalid_0's ndcg@50: 0.964985\tvalid_0's ndcg@10: 0.964985\tvalid_0's ndcg@20: 0.964985\tvalid_0's ndcg@30: 0.964985\tvalid_0's ndcg@40: 0.964985\tvalid_0's ndcg@50: 0.964985\n","[66]\tvalid_0's ndcg@10: 0.964993\tvalid_0's ndcg@20: 0.964993\tvalid_0's ndcg@30: 0.964993\tvalid_0's ndcg@40: 0.964993\tvalid_0's ndcg@50: 0.964993\tvalid_0's ndcg@10: 0.964993\tvalid_0's ndcg@20: 0.964993\tvalid_0's ndcg@30: 0.964993\tvalid_0's ndcg@40: 0.964993\tvalid_0's ndcg@50: 0.964993\n","[67]\tvalid_0's ndcg@10: 0.965028\tvalid_0's ndcg@20: 0.965028\tvalid_0's ndcg@30: 0.965028\tvalid_0's ndcg@40: 0.965028\tvalid_0's ndcg@50: 0.965028\tvalid_0's ndcg@10: 0.965028\tvalid_0's ndcg@20: 0.965028\tvalid_0's ndcg@30: 0.965028\tvalid_0's ndcg@40: 0.965028\tvalid_0's ndcg@50: 0.965028\n","[68]\tvalid_0's ndcg@10: 0.965115\tvalid_0's ndcg@20: 0.965115\tvalid_0's ndcg@30: 0.965115\tvalid_0's ndcg@40: 0.965115\tvalid_0's ndcg@50: 0.965115\tvalid_0's ndcg@10: 0.965115\tvalid_0's ndcg@20: 0.965115\tvalid_0's ndcg@30: 0.965115\tvalid_0's ndcg@40: 0.965115\tvalid_0's ndcg@50: 0.965115\n","[69]\tvalid_0's ndcg@10: 0.965206\tvalid_0's ndcg@20: 0.965206\tvalid_0's ndcg@30: 0.965206\tvalid_0's ndcg@40: 0.965206\tvalid_0's ndcg@50: 0.965206\tvalid_0's ndcg@10: 0.965206\tvalid_0's ndcg@20: 0.965206\tvalid_0's ndcg@30: 0.965206\tvalid_0's ndcg@40: 0.965206\tvalid_0's ndcg@50: 0.965206\n","[70]\tvalid_0's ndcg@10: 0.965214\tvalid_0's ndcg@20: 0.965214\tvalid_0's ndcg@30: 0.965214\tvalid_0's ndcg@40: 0.965214\tvalid_0's ndcg@50: 0.965214\tvalid_0's ndcg@10: 0.965214\tvalid_0's ndcg@20: 0.965214\tvalid_0's ndcg@30: 0.965214\tvalid_0's ndcg@40: 0.965214\tvalid_0's ndcg@50: 0.965214\n","[71]\tvalid_0's ndcg@10: 0.965281\tvalid_0's ndcg@20: 0.965281\tvalid_0's ndcg@30: 0.965281\tvalid_0's ndcg@40: 0.965281\tvalid_0's ndcg@50: 0.965281\tvalid_0's ndcg@10: 0.965281\tvalid_0's ndcg@20: 0.965281\tvalid_0's ndcg@30: 0.965281\tvalid_0's ndcg@40: 0.965281\tvalid_0's ndcg@50: 0.965281\n","[72]\tvalid_0's ndcg@10: 0.965514\tvalid_0's ndcg@20: 0.965514\tvalid_0's ndcg@30: 0.965514\tvalid_0's ndcg@40: 0.965514\tvalid_0's ndcg@50: 0.965514\tvalid_0's ndcg@10: 0.965514\tvalid_0's ndcg@20: 0.965514\tvalid_0's ndcg@30: 0.965514\tvalid_0's ndcg@40: 0.965514\tvalid_0's ndcg@50: 0.965514\n","[73]\tvalid_0's ndcg@10: 0.96541\tvalid_0's ndcg@20: 0.96541\tvalid_0's ndcg@30: 0.96541\tvalid_0's ndcg@40: 0.96541\tvalid_0's ndcg@50: 0.96541\tvalid_0's ndcg@10: 0.96541\tvalid_0's ndcg@20: 0.96541\tvalid_0's ndcg@30: 0.96541\tvalid_0's ndcg@40: 0.96541\tvalid_0's ndcg@50: 0.96541\n","[74]\tvalid_0's ndcg@10: 0.965243\tvalid_0's ndcg@20: 0.965243\tvalid_0's ndcg@30: 0.965243\tvalid_0's ndcg@40: 0.965243\tvalid_0's ndcg@50: 0.965243\tvalid_0's ndcg@10: 0.965243\tvalid_0's ndcg@20: 0.965243\tvalid_0's ndcg@30: 0.965243\tvalid_0's ndcg@40: 0.965243\tvalid_0's ndcg@50: 0.965243\n","[75]\tvalid_0's ndcg@10: 0.96511\tvalid_0's ndcg@20: 0.96511\tvalid_0's ndcg@30: 0.96511\tvalid_0's ndcg@40: 0.96511\tvalid_0's ndcg@50: 0.96511\tvalid_0's ndcg@10: 0.96511\tvalid_0's ndcg@20: 0.96511\tvalid_0's ndcg@30: 0.96511\tvalid_0's ndcg@40: 0.96511\tvalid_0's ndcg@50: 0.96511\n","[76]\tvalid_0's ndcg@10: 0.965181\tvalid_0's ndcg@20: 0.965181\tvalid_0's ndcg@30: 0.965181\tvalid_0's ndcg@40: 0.965181\tvalid_0's ndcg@50: 0.965181\tvalid_0's ndcg@10: 0.965181\tvalid_0's ndcg@20: 0.965181\tvalid_0's ndcg@30: 0.965181\tvalid_0's ndcg@40: 0.965181\tvalid_0's ndcg@50: 0.965181\n","[77]\tvalid_0's ndcg@10: 0.96507\tvalid_0's ndcg@20: 0.96507\tvalid_0's ndcg@30: 0.96507\tvalid_0's ndcg@40: 0.96507\tvalid_0's ndcg@50: 0.96507\tvalid_0's ndcg@10: 0.96507\tvalid_0's ndcg@20: 0.96507\tvalid_0's ndcg@30: 0.96507\tvalid_0's ndcg@40: 0.96507\tvalid_0's ndcg@50: 0.96507\n","[78]\tvalid_0's ndcg@10: 0.965075\tvalid_0's ndcg@20: 0.965075\tvalid_0's ndcg@30: 0.965075\tvalid_0's ndcg@40: 0.965075\tvalid_0's ndcg@50: 0.965075\tvalid_0's ndcg@10: 0.965075\tvalid_0's ndcg@20: 0.965075\tvalid_0's ndcg@30: 0.965075\tvalid_0's ndcg@40: 0.965075\tvalid_0's ndcg@50: 0.965075\n","[79]\tvalid_0's ndcg@10: 0.965075\tvalid_0's ndcg@20: 0.965075\tvalid_0's ndcg@30: 0.965075\tvalid_0's ndcg@40: 0.965075\tvalid_0's ndcg@50: 0.965075\tvalid_0's ndcg@10: 0.965075\tvalid_0's ndcg@20: 0.965075\tvalid_0's ndcg@30: 0.965075\tvalid_0's ndcg@40: 0.965075\tvalid_0's ndcg@50: 0.965075\n","[80]\tvalid_0's ndcg@10: 0.965347\tvalid_0's ndcg@20: 0.965347\tvalid_0's ndcg@30: 0.965347\tvalid_0's ndcg@40: 0.965347\tvalid_0's ndcg@50: 0.965347\tvalid_0's ndcg@10: 0.965347\tvalid_0's ndcg@20: 0.965347\tvalid_0's ndcg@30: 0.965347\tvalid_0's ndcg@40: 0.965347\tvalid_0's ndcg@50: 0.965347\n","[81]\tvalid_0's ndcg@10: 0.965222\tvalid_0's ndcg@20: 0.965222\tvalid_0's ndcg@30: 0.965222\tvalid_0's ndcg@40: 0.965222\tvalid_0's ndcg@50: 0.965222\tvalid_0's ndcg@10: 0.965222\tvalid_0's ndcg@20: 0.965222\tvalid_0's ndcg@30: 0.965222\tvalid_0's ndcg@40: 0.965222\tvalid_0's ndcg@50: 0.965222\n","[82]\tvalid_0's ndcg@10: 0.965182\tvalid_0's ndcg@20: 0.965182\tvalid_0's ndcg@30: 0.965182\tvalid_0's ndcg@40: 0.965182\tvalid_0's ndcg@50: 0.965182\tvalid_0's ndcg@10: 0.965182\tvalid_0's ndcg@20: 0.965182\tvalid_0's ndcg@30: 0.965182\tvalid_0's ndcg@40: 0.965182\tvalid_0's ndcg@50: 0.965182\n","[83]\tvalid_0's ndcg@10: 0.965132\tvalid_0's ndcg@20: 0.965132\tvalid_0's ndcg@30: 0.965132\tvalid_0's ndcg@40: 0.965132\tvalid_0's ndcg@50: 0.965132\tvalid_0's ndcg@10: 0.965132\tvalid_0's ndcg@20: 0.965132\tvalid_0's ndcg@30: 0.965132\tvalid_0's ndcg@40: 0.965132\tvalid_0's ndcg@50: 0.965132\n","[84]\tvalid_0's ndcg@10: 0.965224\tvalid_0's ndcg@20: 0.965224\tvalid_0's ndcg@30: 0.965224\tvalid_0's ndcg@40: 0.965224\tvalid_0's ndcg@50: 0.965224\tvalid_0's ndcg@10: 0.965224\tvalid_0's ndcg@20: 0.965224\tvalid_0's ndcg@30: 0.965224\tvalid_0's ndcg@40: 0.965224\tvalid_0's ndcg@50: 0.965224\n","[85]\tvalid_0's ndcg@10: 0.965312\tvalid_0's ndcg@20: 0.965359\tvalid_0's ndcg@30: 0.965359\tvalid_0's ndcg@40: 0.965359\tvalid_0's ndcg@50: 0.965359\tvalid_0's ndcg@10: 0.965312\tvalid_0's ndcg@20: 0.965359\tvalid_0's ndcg@30: 0.965359\tvalid_0's ndcg@40: 0.965359\tvalid_0's ndcg@50: 0.965359\n","[86]\tvalid_0's ndcg@10: 0.965242\tvalid_0's ndcg@20: 0.965289\tvalid_0's ndcg@30: 0.965289\tvalid_0's ndcg@40: 0.965289\tvalid_0's ndcg@50: 0.965289\tvalid_0's ndcg@10: 0.965242\tvalid_0's ndcg@20: 0.965289\tvalid_0's ndcg@30: 0.965289\tvalid_0's ndcg@40: 0.965289\tvalid_0's ndcg@50: 0.965289\n","[87]\tvalid_0's ndcg@10: 0.965191\tvalid_0's ndcg@20: 0.965238\tvalid_0's ndcg@30: 0.965238\tvalid_0's ndcg@40: 0.965238\tvalid_0's ndcg@50: 0.965238\tvalid_0's ndcg@10: 0.965191\tvalid_0's ndcg@20: 0.965238\tvalid_0's ndcg@30: 0.965238\tvalid_0's ndcg@40: 0.965238\tvalid_0's ndcg@50: 0.965238\n","[88]\tvalid_0's ndcg@10: 0.965389\tvalid_0's ndcg@20: 0.965437\tvalid_0's ndcg@30: 0.965437\tvalid_0's ndcg@40: 0.965437\tvalid_0's ndcg@50: 0.965437\tvalid_0's ndcg@10: 0.965389\tvalid_0's ndcg@20: 0.965437\tvalid_0's ndcg@30: 0.965437\tvalid_0's ndcg@40: 0.965437\tvalid_0's ndcg@50: 0.965437\n","[89]\tvalid_0's ndcg@10: 0.96548\tvalid_0's ndcg@20: 0.965527\tvalid_0's ndcg@30: 0.965527\tvalid_0's ndcg@40: 0.965527\tvalid_0's ndcg@50: 0.965527\tvalid_0's ndcg@10: 0.96548\tvalid_0's ndcg@20: 0.965527\tvalid_0's ndcg@30: 0.965527\tvalid_0's ndcg@40: 0.965527\tvalid_0's ndcg@50: 0.965527\n","[90]\tvalid_0's ndcg@10: 0.965491\tvalid_0's ndcg@20: 0.965539\tvalid_0's ndcg@30: 0.965539\tvalid_0's ndcg@40: 0.965539\tvalid_0's ndcg@50: 0.965539\tvalid_0's ndcg@10: 0.965491\tvalid_0's ndcg@20: 0.965539\tvalid_0's ndcg@30: 0.965539\tvalid_0's ndcg@40: 0.965539\tvalid_0's ndcg@50: 0.965539\n","[91]\tvalid_0's ndcg@10: 0.965563\tvalid_0's ndcg@20: 0.965563\tvalid_0's ndcg@30: 0.965563\tvalid_0's ndcg@40: 0.965563\tvalid_0's ndcg@50: 0.965563\tvalid_0's ndcg@10: 0.965563\tvalid_0's ndcg@20: 0.965563\tvalid_0's ndcg@30: 0.965563\tvalid_0's ndcg@40: 0.965563\tvalid_0's ndcg@50: 0.965563\n","[92]\tvalid_0's ndcg@10: 0.965466\tvalid_0's ndcg@20: 0.965466\tvalid_0's ndcg@30: 0.965466\tvalid_0's ndcg@40: 0.965466\tvalid_0's ndcg@50: 0.965466\tvalid_0's ndcg@10: 0.965466\tvalid_0's ndcg@20: 0.965466\tvalid_0's ndcg@30: 0.965466\tvalid_0's ndcg@40: 0.965466\tvalid_0's ndcg@50: 0.965466\n","[93]\tvalid_0's ndcg@10: 0.965381\tvalid_0's ndcg@20: 0.965381\tvalid_0's ndcg@30: 0.965381\tvalid_0's ndcg@40: 0.965381\tvalid_0's ndcg@50: 0.965381\tvalid_0's ndcg@10: 0.965381\tvalid_0's ndcg@20: 0.965381\tvalid_0's ndcg@30: 0.965381\tvalid_0's ndcg@40: 0.965381\tvalid_0's ndcg@50: 0.965381\n","[94]\tvalid_0's ndcg@10: 0.965374\tvalid_0's ndcg@20: 0.965374\tvalid_0's ndcg@30: 0.965374\tvalid_0's ndcg@40: 0.965374\tvalid_0's ndcg@50: 0.965374\tvalid_0's ndcg@10: 0.965374\tvalid_0's ndcg@20: 0.965374\tvalid_0's ndcg@30: 0.965374\tvalid_0's ndcg@40: 0.965374\tvalid_0's ndcg@50: 0.965374\n","[95]\tvalid_0's ndcg@10: 0.96543\tvalid_0's ndcg@20: 0.96543\tvalid_0's ndcg@30: 0.96543\tvalid_0's ndcg@40: 0.96543\tvalid_0's ndcg@50: 0.96543\tvalid_0's ndcg@10: 0.96543\tvalid_0's ndcg@20: 0.96543\tvalid_0's ndcg@30: 0.96543\tvalid_0's ndcg@40: 0.96543\tvalid_0's ndcg@50: 0.96543\n","[96]\tvalid_0's ndcg@10: 0.9654\tvalid_0's ndcg@20: 0.9654\tvalid_0's ndcg@30: 0.9654\tvalid_0's ndcg@40: 0.9654\tvalid_0's ndcg@50: 0.9654\tvalid_0's ndcg@10: 0.9654\tvalid_0's ndcg@20: 0.9654\tvalid_0's ndcg@30: 0.9654\tvalid_0's ndcg@40: 0.9654\tvalid_0's ndcg@50: 0.9654\n","[97]\tvalid_0's ndcg@10: 0.965652\tvalid_0's ndcg@20: 0.965652\tvalid_0's ndcg@30: 0.965652\tvalid_0's ndcg@40: 0.965652\tvalid_0's ndcg@50: 0.965652\tvalid_0's ndcg@10: 0.965652\tvalid_0's ndcg@20: 0.965652\tvalid_0's ndcg@30: 0.965652\tvalid_0's ndcg@40: 0.965652\tvalid_0's ndcg@50: 0.965652\n","[98]\tvalid_0's ndcg@10: 0.96559\tvalid_0's ndcg@20: 0.96559\tvalid_0's ndcg@30: 0.96559\tvalid_0's ndcg@40: 0.96559\tvalid_0's ndcg@50: 0.96559\tvalid_0's ndcg@10: 0.96559\tvalid_0's ndcg@20: 0.96559\tvalid_0's ndcg@30: 0.96559\tvalid_0's ndcg@40: 0.96559\tvalid_0's ndcg@50: 0.96559\n","[99]\tvalid_0's ndcg@10: 0.96552\tvalid_0's ndcg@20: 0.96552\tvalid_0's ndcg@30: 0.96552\tvalid_0's ndcg@40: 0.96552\tvalid_0's ndcg@50: 0.96552\tvalid_0's ndcg@10: 0.96552\tvalid_0's ndcg@20: 0.96552\tvalid_0's ndcg@30: 0.96552\tvalid_0's ndcg@40: 0.96552\tvalid_0's ndcg@50: 0.96552\n","[100]\tvalid_0's ndcg@10: 0.965335\tvalid_0's ndcg@20: 0.965335\tvalid_0's ndcg@30: 0.965335\tvalid_0's ndcg@40: 0.965335\tvalid_0's ndcg@50: 0.965335\tvalid_0's ndcg@10: 0.965335\tvalid_0's ndcg@20: 0.965335\tvalid_0's ndcg@30: 0.965335\tvalid_0's ndcg@40: 0.965335\tvalid_0's ndcg@50: 0.965335\n","Did not meet early stopping. Best iteration is:\n","[97]\tvalid_0's ndcg@10: 0.965652\tvalid_0's ndcg@20: 0.965652\tvalid_0's ndcg@30: 0.965652\tvalid_0's ndcg@40: 0.965652\tvalid_0's ndcg@50: 0.965652\tvalid_0's ndcg@10: 0.965652\tvalid_0's ndcg@20: 0.965652\tvalid_0's ndcg@30: 0.965652\tvalid_0's ndcg@40: 0.965652\tvalid_0's ndcg@50: 0.965652\n","[1]\tvalid_0's ndcg@10: 0.960771\tvalid_0's ndcg@20: 0.960904\tvalid_0's ndcg@30: 0.960904\tvalid_0's ndcg@40: 0.960904\tvalid_0's ndcg@50: 0.960904\tvalid_0's ndcg@10: 0.960771\tvalid_0's ndcg@20: 0.960904\tvalid_0's ndcg@30: 0.960904\tvalid_0's ndcg@40: 0.960904\tvalid_0's ndcg@50: 0.960904\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.961207\tvalid_0's ndcg@20: 0.961207\tvalid_0's ndcg@30: 0.961207\tvalid_0's ndcg@40: 0.961207\tvalid_0's ndcg@50: 0.961207\tvalid_0's ndcg@10: 0.961207\tvalid_0's ndcg@20: 0.961207\tvalid_0's ndcg@30: 0.961207\tvalid_0's ndcg@40: 0.961207\tvalid_0's ndcg@50: 0.961207\n","[3]\tvalid_0's ndcg@10: 0.963487\tvalid_0's ndcg@20: 0.96353\tvalid_0's ndcg@30: 0.96353\tvalid_0's ndcg@40: 0.96353\tvalid_0's ndcg@50: 0.96353\tvalid_0's ndcg@10: 0.963487\tvalid_0's ndcg@20: 0.96353\tvalid_0's ndcg@30: 0.96353\tvalid_0's ndcg@40: 0.96353\tvalid_0's ndcg@50: 0.96353\n","[4]\tvalid_0's ndcg@10: 0.963155\tvalid_0's ndcg@20: 0.963155\tvalid_0's ndcg@30: 0.963155\tvalid_0's ndcg@40: 0.963155\tvalid_0's ndcg@50: 0.963155\tvalid_0's ndcg@10: 0.963155\tvalid_0's ndcg@20: 0.963155\tvalid_0's ndcg@30: 0.963155\tvalid_0's ndcg@40: 0.963155\tvalid_0's ndcg@50: 0.963155\n","[5]\tvalid_0's ndcg@10: 0.963805\tvalid_0's ndcg@20: 0.963805\tvalid_0's ndcg@30: 0.963805\tvalid_0's ndcg@40: 0.963805\tvalid_0's ndcg@50: 0.963805\tvalid_0's ndcg@10: 0.963805\tvalid_0's ndcg@20: 0.963805\tvalid_0's ndcg@30: 0.963805\tvalid_0's ndcg@40: 0.963805\tvalid_0's ndcg@50: 0.963805\n","[6]\tvalid_0's ndcg@10: 0.963934\tvalid_0's ndcg@20: 0.963934\tvalid_0's ndcg@30: 0.963934\tvalid_0's ndcg@40: 0.963934\tvalid_0's ndcg@50: 0.963934\tvalid_0's ndcg@10: 0.963934\tvalid_0's ndcg@20: 0.963934\tvalid_0's ndcg@30: 0.963934\tvalid_0's ndcg@40: 0.963934\tvalid_0's ndcg@50: 0.963934\n","[7]\tvalid_0's ndcg@10: 0.964127\tvalid_0's ndcg@20: 0.964127\tvalid_0's ndcg@30: 0.964127\tvalid_0's ndcg@40: 0.964127\tvalid_0's ndcg@50: 0.964127\tvalid_0's ndcg@10: 0.964127\tvalid_0's ndcg@20: 0.964127\tvalid_0's ndcg@30: 0.964127\tvalid_0's ndcg@40: 0.964127\tvalid_0's ndcg@50: 0.964127\n","[8]\tvalid_0's ndcg@10: 0.964134\tvalid_0's ndcg@20: 0.964181\tvalid_0's ndcg@30: 0.964181\tvalid_0's ndcg@40: 0.964181\tvalid_0's ndcg@50: 0.964181\tvalid_0's ndcg@10: 0.964134\tvalid_0's ndcg@20: 0.964181\tvalid_0's ndcg@30: 0.964181\tvalid_0's ndcg@40: 0.964181\tvalid_0's ndcg@50: 0.964181\n","[9]\tvalid_0's ndcg@10: 0.964761\tvalid_0's ndcg@20: 0.964761\tvalid_0's ndcg@30: 0.964761\tvalid_0's ndcg@40: 0.964761\tvalid_0's ndcg@50: 0.964761\tvalid_0's ndcg@10: 0.964761\tvalid_0's ndcg@20: 0.964761\tvalid_0's ndcg@30: 0.964761\tvalid_0's ndcg@40: 0.964761\tvalid_0's ndcg@50: 0.964761\n","[10]\tvalid_0's ndcg@10: 0.964122\tvalid_0's ndcg@20: 0.964122\tvalid_0's ndcg@30: 0.964122\tvalid_0's ndcg@40: 0.964122\tvalid_0's ndcg@50: 0.964122\tvalid_0's ndcg@10: 0.964122\tvalid_0's ndcg@20: 0.964122\tvalid_0's ndcg@30: 0.964122\tvalid_0's ndcg@40: 0.964122\tvalid_0's ndcg@50: 0.964122\n","[11]\tvalid_0's ndcg@10: 0.964341\tvalid_0's ndcg@20: 0.964341\tvalid_0's ndcg@30: 0.964341\tvalid_0's ndcg@40: 0.964341\tvalid_0's ndcg@50: 0.964341\tvalid_0's ndcg@10: 0.964341\tvalid_0's ndcg@20: 0.964341\tvalid_0's ndcg@30: 0.964341\tvalid_0's ndcg@40: 0.964341\tvalid_0's ndcg@50: 0.964341\n","[12]\tvalid_0's ndcg@10: 0.964572\tvalid_0's ndcg@20: 0.96462\tvalid_0's ndcg@30: 0.96462\tvalid_0's ndcg@40: 0.96462\tvalid_0's ndcg@50: 0.96462\tvalid_0's ndcg@10: 0.964572\tvalid_0's ndcg@20: 0.96462\tvalid_0's ndcg@30: 0.96462\tvalid_0's ndcg@40: 0.96462\tvalid_0's ndcg@50: 0.96462\n","[13]\tvalid_0's ndcg@10: 0.964319\tvalid_0's ndcg@20: 0.964366\tvalid_0's ndcg@30: 0.964366\tvalid_0's ndcg@40: 0.964366\tvalid_0's ndcg@50: 0.964366\tvalid_0's ndcg@10: 0.964319\tvalid_0's ndcg@20: 0.964366\tvalid_0's ndcg@30: 0.964366\tvalid_0's ndcg@40: 0.964366\tvalid_0's ndcg@50: 0.964366\n","[14]\tvalid_0's ndcg@10: 0.964378\tvalid_0's ndcg@20: 0.964424\tvalid_0's ndcg@30: 0.964424\tvalid_0's ndcg@40: 0.964424\tvalid_0's ndcg@50: 0.964424\tvalid_0's ndcg@10: 0.964378\tvalid_0's ndcg@20: 0.964424\tvalid_0's ndcg@30: 0.964424\tvalid_0's ndcg@40: 0.964424\tvalid_0's ndcg@50: 0.964424\n","[15]\tvalid_0's ndcg@10: 0.964552\tvalid_0's ndcg@20: 0.964598\tvalid_0's ndcg@30: 0.964598\tvalid_0's ndcg@40: 0.964598\tvalid_0's ndcg@50: 0.964598\tvalid_0's ndcg@10: 0.964552\tvalid_0's ndcg@20: 0.964598\tvalid_0's ndcg@30: 0.964598\tvalid_0's ndcg@40: 0.964598\tvalid_0's ndcg@50: 0.964598\n","[16]\tvalid_0's ndcg@10: 0.964531\tvalid_0's ndcg@20: 0.964577\tvalid_0's ndcg@30: 0.964577\tvalid_0's ndcg@40: 0.964577\tvalid_0's ndcg@50: 0.964577\tvalid_0's ndcg@10: 0.964531\tvalid_0's ndcg@20: 0.964577\tvalid_0's ndcg@30: 0.964577\tvalid_0's ndcg@40: 0.964577\tvalid_0's ndcg@50: 0.964577\n","[17]\tvalid_0's ndcg@10: 0.964609\tvalid_0's ndcg@20: 0.964655\tvalid_0's ndcg@30: 0.964655\tvalid_0's ndcg@40: 0.964655\tvalid_0's ndcg@50: 0.964655\tvalid_0's ndcg@10: 0.964609\tvalid_0's ndcg@20: 0.964655\tvalid_0's ndcg@30: 0.964655\tvalid_0's ndcg@40: 0.964655\tvalid_0's ndcg@50: 0.964655\n","[18]\tvalid_0's ndcg@10: 0.964729\tvalid_0's ndcg@20: 0.964776\tvalid_0's ndcg@30: 0.964776\tvalid_0's ndcg@40: 0.964776\tvalid_0's ndcg@50: 0.964776\tvalid_0's ndcg@10: 0.964729\tvalid_0's ndcg@20: 0.964776\tvalid_0's ndcg@30: 0.964776\tvalid_0's ndcg@40: 0.964776\tvalid_0's ndcg@50: 0.964776\n","[19]\tvalid_0's ndcg@10: 0.9645\tvalid_0's ndcg@20: 0.964547\tvalid_0's ndcg@30: 0.964547\tvalid_0's ndcg@40: 0.964547\tvalid_0's ndcg@50: 0.964547\tvalid_0's ndcg@10: 0.9645\tvalid_0's ndcg@20: 0.964547\tvalid_0's ndcg@30: 0.964547\tvalid_0's ndcg@40: 0.964547\tvalid_0's ndcg@50: 0.964547\n","[20]\tvalid_0's ndcg@10: 0.964556\tvalid_0's ndcg@20: 0.964603\tvalid_0's ndcg@30: 0.964603\tvalid_0's ndcg@40: 0.964603\tvalid_0's ndcg@50: 0.964603\tvalid_0's ndcg@10: 0.964556\tvalid_0's ndcg@20: 0.964603\tvalid_0's ndcg@30: 0.964603\tvalid_0's ndcg@40: 0.964603\tvalid_0's ndcg@50: 0.964603\n","[21]\tvalid_0's ndcg@10: 0.964488\tvalid_0's ndcg@20: 0.964535\tvalid_0's ndcg@30: 0.964535\tvalid_0's ndcg@40: 0.964535\tvalid_0's ndcg@50: 0.964535\tvalid_0's ndcg@10: 0.964488\tvalid_0's ndcg@20: 0.964535\tvalid_0's ndcg@30: 0.964535\tvalid_0's ndcg@40: 0.964535\tvalid_0's ndcg@50: 0.964535\n","[22]\tvalid_0's ndcg@10: 0.964182\tvalid_0's ndcg@20: 0.964229\tvalid_0's ndcg@30: 0.964229\tvalid_0's ndcg@40: 0.964229\tvalid_0's ndcg@50: 0.964229\tvalid_0's ndcg@10: 0.964182\tvalid_0's ndcg@20: 0.964229\tvalid_0's ndcg@30: 0.964229\tvalid_0's ndcg@40: 0.964229\tvalid_0's ndcg@50: 0.964229\n","[23]\tvalid_0's ndcg@10: 0.964302\tvalid_0's ndcg@20: 0.96435\tvalid_0's ndcg@30: 0.96435\tvalid_0's ndcg@40: 0.96435\tvalid_0's ndcg@50: 0.96435\tvalid_0's ndcg@10: 0.964302\tvalid_0's ndcg@20: 0.96435\tvalid_0's ndcg@30: 0.96435\tvalid_0's ndcg@40: 0.96435\tvalid_0's ndcg@50: 0.96435\n","[24]\tvalid_0's ndcg@10: 0.964494\tvalid_0's ndcg@20: 0.964542\tvalid_0's ndcg@30: 0.964542\tvalid_0's ndcg@40: 0.964542\tvalid_0's ndcg@50: 0.964542\tvalid_0's ndcg@10: 0.964494\tvalid_0's ndcg@20: 0.964542\tvalid_0's ndcg@30: 0.964542\tvalid_0's ndcg@40: 0.964542\tvalid_0's ndcg@50: 0.964542\n","[25]\tvalid_0's ndcg@10: 0.964587\tvalid_0's ndcg@20: 0.964635\tvalid_0's ndcg@30: 0.964635\tvalid_0's ndcg@40: 0.964635\tvalid_0's ndcg@50: 0.964635\tvalid_0's ndcg@10: 0.964587\tvalid_0's ndcg@20: 0.964635\tvalid_0's ndcg@30: 0.964635\tvalid_0's ndcg@40: 0.964635\tvalid_0's ndcg@50: 0.964635\n","[26]\tvalid_0's ndcg@10: 0.964093\tvalid_0's ndcg@20: 0.96414\tvalid_0's ndcg@30: 0.96414\tvalid_0's ndcg@40: 0.96414\tvalid_0's ndcg@50: 0.96414\tvalid_0's ndcg@10: 0.964093\tvalid_0's ndcg@20: 0.96414\tvalid_0's ndcg@30: 0.96414\tvalid_0's ndcg@40: 0.96414\tvalid_0's ndcg@50: 0.96414\n","[27]\tvalid_0's ndcg@10: 0.964297\tvalid_0's ndcg@20: 0.964345\tvalid_0's ndcg@30: 0.964345\tvalid_0's ndcg@40: 0.964345\tvalid_0's ndcg@50: 0.964345\tvalid_0's ndcg@10: 0.964297\tvalid_0's ndcg@20: 0.964345\tvalid_0's ndcg@30: 0.964345\tvalid_0's ndcg@40: 0.964345\tvalid_0's ndcg@50: 0.964345\n","[28]\tvalid_0's ndcg@10: 0.964169\tvalid_0's ndcg@20: 0.964216\tvalid_0's ndcg@30: 0.964216\tvalid_0's ndcg@40: 0.964216\tvalid_0's ndcg@50: 0.964216\tvalid_0's ndcg@10: 0.964169\tvalid_0's ndcg@20: 0.964216\tvalid_0's ndcg@30: 0.964216\tvalid_0's ndcg@40: 0.964216\tvalid_0's ndcg@50: 0.964216\n","[29]\tvalid_0's ndcg@10: 0.964184\tvalid_0's ndcg@20: 0.964232\tvalid_0's ndcg@30: 0.964232\tvalid_0's ndcg@40: 0.964232\tvalid_0's ndcg@50: 0.964232\tvalid_0's ndcg@10: 0.964184\tvalid_0's ndcg@20: 0.964232\tvalid_0's ndcg@30: 0.964232\tvalid_0's ndcg@40: 0.964232\tvalid_0's ndcg@50: 0.964232\n","[30]\tvalid_0's ndcg@10: 0.964025\tvalid_0's ndcg@20: 0.964073\tvalid_0's ndcg@30: 0.964073\tvalid_0's ndcg@40: 0.964073\tvalid_0's ndcg@50: 0.964073\tvalid_0's ndcg@10: 0.964025\tvalid_0's ndcg@20: 0.964073\tvalid_0's ndcg@30: 0.964073\tvalid_0's ndcg@40: 0.964073\tvalid_0's ndcg@50: 0.964073\n","[31]\tvalid_0's ndcg@10: 0.964202\tvalid_0's ndcg@20: 0.964248\tvalid_0's ndcg@30: 0.964248\tvalid_0's ndcg@40: 0.964248\tvalid_0's ndcg@50: 0.964248\tvalid_0's ndcg@10: 0.964202\tvalid_0's ndcg@20: 0.964248\tvalid_0's ndcg@30: 0.964248\tvalid_0's ndcg@40: 0.964248\tvalid_0's ndcg@50: 0.964248\n","[32]\tvalid_0's ndcg@10: 0.964042\tvalid_0's ndcg@20: 0.964089\tvalid_0's ndcg@30: 0.964089\tvalid_0's ndcg@40: 0.964089\tvalid_0's ndcg@50: 0.964089\tvalid_0's ndcg@10: 0.964042\tvalid_0's ndcg@20: 0.964089\tvalid_0's ndcg@30: 0.964089\tvalid_0's ndcg@40: 0.964089\tvalid_0's ndcg@50: 0.964089\n","[33]\tvalid_0's ndcg@10: 0.963995\tvalid_0's ndcg@20: 0.964042\tvalid_0's ndcg@30: 0.964042\tvalid_0's ndcg@40: 0.964042\tvalid_0's ndcg@50: 0.964042\tvalid_0's ndcg@10: 0.963995\tvalid_0's ndcg@20: 0.964042\tvalid_0's ndcg@30: 0.964042\tvalid_0's ndcg@40: 0.964042\tvalid_0's ndcg@50: 0.964042\n","[34]\tvalid_0's ndcg@10: 0.963975\tvalid_0's ndcg@20: 0.964022\tvalid_0's ndcg@30: 0.964022\tvalid_0's ndcg@40: 0.964022\tvalid_0's ndcg@50: 0.964022\tvalid_0's ndcg@10: 0.963975\tvalid_0's ndcg@20: 0.964022\tvalid_0's ndcg@30: 0.964022\tvalid_0's ndcg@40: 0.964022\tvalid_0's ndcg@50: 0.964022\n","[35]\tvalid_0's ndcg@10: 0.964226\tvalid_0's ndcg@20: 0.964273\tvalid_0's ndcg@30: 0.964273\tvalid_0's ndcg@40: 0.964273\tvalid_0's ndcg@50: 0.964273\tvalid_0's ndcg@10: 0.964226\tvalid_0's ndcg@20: 0.964273\tvalid_0's ndcg@30: 0.964273\tvalid_0's ndcg@40: 0.964273\tvalid_0's ndcg@50: 0.964273\n","[36]\tvalid_0's ndcg@10: 0.964041\tvalid_0's ndcg@20: 0.964088\tvalid_0's ndcg@30: 0.964088\tvalid_0's ndcg@40: 0.964088\tvalid_0's ndcg@50: 0.964088\tvalid_0's ndcg@10: 0.964041\tvalid_0's ndcg@20: 0.964088\tvalid_0's ndcg@30: 0.964088\tvalid_0's ndcg@40: 0.964088\tvalid_0's ndcg@50: 0.964088\n","[37]\tvalid_0's ndcg@10: 0.964088\tvalid_0's ndcg@20: 0.964136\tvalid_0's ndcg@30: 0.964136\tvalid_0's ndcg@40: 0.964136\tvalid_0's ndcg@50: 0.964136\tvalid_0's ndcg@10: 0.964088\tvalid_0's ndcg@20: 0.964136\tvalid_0's ndcg@30: 0.964136\tvalid_0's ndcg@40: 0.964136\tvalid_0's ndcg@50: 0.964136\n","[38]\tvalid_0's ndcg@10: 0.964356\tvalid_0's ndcg@20: 0.964404\tvalid_0's ndcg@30: 0.964404\tvalid_0's ndcg@40: 0.964404\tvalid_0's ndcg@50: 0.964404\tvalid_0's ndcg@10: 0.964356\tvalid_0's ndcg@20: 0.964404\tvalid_0's ndcg@30: 0.964404\tvalid_0's ndcg@40: 0.964404\tvalid_0's ndcg@50: 0.964404\n","[39]\tvalid_0's ndcg@10: 0.964283\tvalid_0's ndcg@20: 0.96433\tvalid_0's ndcg@30: 0.96433\tvalid_0's ndcg@40: 0.96433\tvalid_0's ndcg@50: 0.96433\tvalid_0's ndcg@10: 0.964283\tvalid_0's ndcg@20: 0.96433\tvalid_0's ndcg@30: 0.96433\tvalid_0's ndcg@40: 0.96433\tvalid_0's ndcg@50: 0.96433\n","[40]\tvalid_0's ndcg@10: 0.964378\tvalid_0's ndcg@20: 0.964425\tvalid_0's ndcg@30: 0.964425\tvalid_0's ndcg@40: 0.964425\tvalid_0's ndcg@50: 0.964425\tvalid_0's ndcg@10: 0.964378\tvalid_0's ndcg@20: 0.964425\tvalid_0's ndcg@30: 0.964425\tvalid_0's ndcg@40: 0.964425\tvalid_0's ndcg@50: 0.964425\n","[41]\tvalid_0's ndcg@10: 0.964398\tvalid_0's ndcg@20: 0.964446\tvalid_0's ndcg@30: 0.964446\tvalid_0's ndcg@40: 0.964446\tvalid_0's ndcg@50: 0.964446\tvalid_0's ndcg@10: 0.964398\tvalid_0's ndcg@20: 0.964446\tvalid_0's ndcg@30: 0.964446\tvalid_0's ndcg@40: 0.964446\tvalid_0's ndcg@50: 0.964446\n","[42]\tvalid_0's ndcg@10: 0.964183\tvalid_0's ndcg@20: 0.96423\tvalid_0's ndcg@30: 0.96423\tvalid_0's ndcg@40: 0.96423\tvalid_0's ndcg@50: 0.96423\tvalid_0's ndcg@10: 0.964183\tvalid_0's ndcg@20: 0.96423\tvalid_0's ndcg@30: 0.96423\tvalid_0's ndcg@40: 0.96423\tvalid_0's ndcg@50: 0.96423\n","[43]\tvalid_0's ndcg@10: 0.964118\tvalid_0's ndcg@20: 0.964165\tvalid_0's ndcg@30: 0.964165\tvalid_0's ndcg@40: 0.964165\tvalid_0's ndcg@50: 0.964165\tvalid_0's ndcg@10: 0.964118\tvalid_0's ndcg@20: 0.964165\tvalid_0's ndcg@30: 0.964165\tvalid_0's ndcg@40: 0.964165\tvalid_0's ndcg@50: 0.964165\n","[44]\tvalid_0's ndcg@10: 0.964255\tvalid_0's ndcg@20: 0.964302\tvalid_0's ndcg@30: 0.964302\tvalid_0's ndcg@40: 0.964302\tvalid_0's ndcg@50: 0.964302\tvalid_0's ndcg@10: 0.964255\tvalid_0's ndcg@20: 0.964302\tvalid_0's ndcg@30: 0.964302\tvalid_0's ndcg@40: 0.964302\tvalid_0's ndcg@50: 0.964302\n","[45]\tvalid_0's ndcg@10: 0.964226\tvalid_0's ndcg@20: 0.964273\tvalid_0's ndcg@30: 0.964273\tvalid_0's ndcg@40: 0.964273\tvalid_0's ndcg@50: 0.964273\tvalid_0's ndcg@10: 0.964226\tvalid_0's ndcg@20: 0.964273\tvalid_0's ndcg@30: 0.964273\tvalid_0's ndcg@40: 0.964273\tvalid_0's ndcg@50: 0.964273\n","[46]\tvalid_0's ndcg@10: 0.964136\tvalid_0's ndcg@20: 0.964184\tvalid_0's ndcg@30: 0.964184\tvalid_0's ndcg@40: 0.964184\tvalid_0's ndcg@50: 0.964184\tvalid_0's ndcg@10: 0.964136\tvalid_0's ndcg@20: 0.964184\tvalid_0's ndcg@30: 0.964184\tvalid_0's ndcg@40: 0.964184\tvalid_0's ndcg@50: 0.964184\n","[47]\tvalid_0's ndcg@10: 0.964092\tvalid_0's ndcg@20: 0.964139\tvalid_0's ndcg@30: 0.964139\tvalid_0's ndcg@40: 0.964139\tvalid_0's ndcg@50: 0.964139\tvalid_0's ndcg@10: 0.964092\tvalid_0's ndcg@20: 0.964139\tvalid_0's ndcg@30: 0.964139\tvalid_0's ndcg@40: 0.964139\tvalid_0's ndcg@50: 0.964139\n","[48]\tvalid_0's ndcg@10: 0.964371\tvalid_0's ndcg@20: 0.964418\tvalid_0's ndcg@30: 0.964418\tvalid_0's ndcg@40: 0.964418\tvalid_0's ndcg@50: 0.964418\tvalid_0's ndcg@10: 0.964371\tvalid_0's ndcg@20: 0.964418\tvalid_0's ndcg@30: 0.964418\tvalid_0's ndcg@40: 0.964418\tvalid_0's ndcg@50: 0.964418\n","[49]\tvalid_0's ndcg@10: 0.964218\tvalid_0's ndcg@20: 0.964218\tvalid_0's ndcg@30: 0.964218\tvalid_0's ndcg@40: 0.964218\tvalid_0's ndcg@50: 0.964218\tvalid_0's ndcg@10: 0.964218\tvalid_0's ndcg@20: 0.964218\tvalid_0's ndcg@30: 0.964218\tvalid_0's ndcg@40: 0.964218\tvalid_0's ndcg@50: 0.964218\n","[50]\tvalid_0's ndcg@10: 0.964247\tvalid_0's ndcg@20: 0.964247\tvalid_0's ndcg@30: 0.964247\tvalid_0's ndcg@40: 0.964247\tvalid_0's ndcg@50: 0.964247\tvalid_0's ndcg@10: 0.964247\tvalid_0's ndcg@20: 0.964247\tvalid_0's ndcg@30: 0.964247\tvalid_0's ndcg@40: 0.964247\tvalid_0's ndcg@50: 0.964247\n","[51]\tvalid_0's ndcg@10: 0.96431\tvalid_0's ndcg@20: 0.964358\tvalid_0's ndcg@30: 0.964358\tvalid_0's ndcg@40: 0.964358\tvalid_0's ndcg@50: 0.964358\tvalid_0's ndcg@10: 0.96431\tvalid_0's ndcg@20: 0.964358\tvalid_0's ndcg@30: 0.964358\tvalid_0's ndcg@40: 0.964358\tvalid_0's ndcg@50: 0.964358\n","[52]\tvalid_0's ndcg@10: 0.964194\tvalid_0's ndcg@20: 0.964241\tvalid_0's ndcg@30: 0.964241\tvalid_0's ndcg@40: 0.964241\tvalid_0's ndcg@50: 0.964241\tvalid_0's ndcg@10: 0.964194\tvalid_0's ndcg@20: 0.964241\tvalid_0's ndcg@30: 0.964241\tvalid_0's ndcg@40: 0.964241\tvalid_0's ndcg@50: 0.964241\n","[53]\tvalid_0's ndcg@10: 0.964304\tvalid_0's ndcg@20: 0.964352\tvalid_0's ndcg@30: 0.964352\tvalid_0's ndcg@40: 0.964352\tvalid_0's ndcg@50: 0.964352\tvalid_0's ndcg@10: 0.964304\tvalid_0's ndcg@20: 0.964352\tvalid_0's ndcg@30: 0.964352\tvalid_0's ndcg@40: 0.964352\tvalid_0's ndcg@50: 0.964352\n","[54]\tvalid_0's ndcg@10: 0.964115\tvalid_0's ndcg@20: 0.964162\tvalid_0's ndcg@30: 0.964162\tvalid_0's ndcg@40: 0.964162\tvalid_0's ndcg@50: 0.964162\tvalid_0's ndcg@10: 0.964115\tvalid_0's ndcg@20: 0.964162\tvalid_0's ndcg@30: 0.964162\tvalid_0's ndcg@40: 0.964162\tvalid_0's ndcg@50: 0.964162\n","[55]\tvalid_0's ndcg@10: 0.963993\tvalid_0's ndcg@20: 0.96404\tvalid_0's ndcg@30: 0.96404\tvalid_0's ndcg@40: 0.96404\tvalid_0's ndcg@50: 0.96404\tvalid_0's ndcg@10: 0.963993\tvalid_0's ndcg@20: 0.96404\tvalid_0's ndcg@30: 0.96404\tvalid_0's ndcg@40: 0.96404\tvalid_0's ndcg@50: 0.96404\n","[56]\tvalid_0's ndcg@10: 0.964132\tvalid_0's ndcg@20: 0.964179\tvalid_0's ndcg@30: 0.964179\tvalid_0's ndcg@40: 0.964179\tvalid_0's ndcg@50: 0.964179\tvalid_0's ndcg@10: 0.964132\tvalid_0's ndcg@20: 0.964179\tvalid_0's ndcg@30: 0.964179\tvalid_0's ndcg@40: 0.964179\tvalid_0's ndcg@50: 0.964179\n","[57]\tvalid_0's ndcg@10: 0.964163\tvalid_0's ndcg@20: 0.964163\tvalid_0's ndcg@30: 0.964163\tvalid_0's ndcg@40: 0.964163\tvalid_0's ndcg@50: 0.964163\tvalid_0's ndcg@10: 0.964163\tvalid_0's ndcg@20: 0.964163\tvalid_0's ndcg@30: 0.964163\tvalid_0's ndcg@40: 0.964163\tvalid_0's ndcg@50: 0.964163\n","[58]\tvalid_0's ndcg@10: 0.964382\tvalid_0's ndcg@20: 0.964382\tvalid_0's ndcg@30: 0.964382\tvalid_0's ndcg@40: 0.964382\tvalid_0's ndcg@50: 0.964382\tvalid_0's ndcg@10: 0.964382\tvalid_0's ndcg@20: 0.964382\tvalid_0's ndcg@30: 0.964382\tvalid_0's ndcg@40: 0.964382\tvalid_0's ndcg@50: 0.964382\n","[59]\tvalid_0's ndcg@10: 0.964428\tvalid_0's ndcg@20: 0.964428\tvalid_0's ndcg@30: 0.964428\tvalid_0's ndcg@40: 0.964428\tvalid_0's ndcg@50: 0.964428\tvalid_0's ndcg@10: 0.964428\tvalid_0's ndcg@20: 0.964428\tvalid_0's ndcg@30: 0.964428\tvalid_0's ndcg@40: 0.964428\tvalid_0's ndcg@50: 0.964428\n","Early stopping, best iteration is:\n","[9]\tvalid_0's ndcg@10: 0.964761\tvalid_0's ndcg@20: 0.964761\tvalid_0's ndcg@30: 0.964761\tvalid_0's ndcg@40: 0.964761\tvalid_0's ndcg@50: 0.964761\tvalid_0's ndcg@10: 0.964761\tvalid_0's ndcg@20: 0.964761\tvalid_0's ndcg@30: 0.964761\tvalid_0's ndcg@40: 0.964761\tvalid_0's ndcg@50: 0.964761\n","[1]\tvalid_0's ndcg@10: 0.959102\tvalid_0's ndcg@20: 0.959274\tvalid_0's ndcg@30: 0.959347\tvalid_0's ndcg@40: 0.959347\tvalid_0's ndcg@50: 0.959347\tvalid_0's ndcg@10: 0.959102\tvalid_0's ndcg@20: 0.959274\tvalid_0's ndcg@30: 0.959347\tvalid_0's ndcg@40: 0.959347\tvalid_0's ndcg@50: 0.959347\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.96065\tvalid_0's ndcg@20: 0.960914\tvalid_0's ndcg@30: 0.96095\tvalid_0's ndcg@40: 0.96095\tvalid_0's ndcg@50: 0.96095\tvalid_0's ndcg@10: 0.96065\tvalid_0's ndcg@20: 0.960914\tvalid_0's ndcg@30: 0.96095\tvalid_0's ndcg@40: 0.96095\tvalid_0's ndcg@50: 0.96095\n","[3]\tvalid_0's ndcg@10: 0.9607\tvalid_0's ndcg@20: 0.960925\tvalid_0's ndcg@30: 0.960961\tvalid_0's ndcg@40: 0.960961\tvalid_0's ndcg@50: 0.960961\tvalid_0's ndcg@10: 0.9607\tvalid_0's ndcg@20: 0.960925\tvalid_0's ndcg@30: 0.960961\tvalid_0's ndcg@40: 0.960961\tvalid_0's ndcg@50: 0.960961\n","[4]\tvalid_0's ndcg@10: 0.961968\tvalid_0's ndcg@20: 0.962191\tvalid_0's ndcg@30: 0.962227\tvalid_0's ndcg@40: 0.962227\tvalid_0's ndcg@50: 0.962227\tvalid_0's ndcg@10: 0.961968\tvalid_0's ndcg@20: 0.962191\tvalid_0's ndcg@30: 0.962227\tvalid_0's ndcg@40: 0.962227\tvalid_0's ndcg@50: 0.962227\n","[5]\tvalid_0's ndcg@10: 0.962589\tvalid_0's ndcg@20: 0.962811\tvalid_0's ndcg@30: 0.962847\tvalid_0's ndcg@40: 0.962847\tvalid_0's ndcg@50: 0.962847\tvalid_0's ndcg@10: 0.962589\tvalid_0's ndcg@20: 0.962811\tvalid_0's ndcg@30: 0.962847\tvalid_0's ndcg@40: 0.962847\tvalid_0's ndcg@50: 0.962847\n","[6]\tvalid_0's ndcg@10: 0.962954\tvalid_0's ndcg@20: 0.963131\tvalid_0's ndcg@30: 0.963167\tvalid_0's ndcg@40: 0.963167\tvalid_0's ndcg@50: 0.963167\tvalid_0's ndcg@10: 0.962954\tvalid_0's ndcg@20: 0.963131\tvalid_0's ndcg@30: 0.963167\tvalid_0's ndcg@40: 0.963167\tvalid_0's ndcg@50: 0.963167\n","[7]\tvalid_0's ndcg@10: 0.96285\tvalid_0's ndcg@20: 0.962982\tvalid_0's ndcg@30: 0.963018\tvalid_0's ndcg@40: 0.963018\tvalid_0's ndcg@50: 0.963018\tvalid_0's ndcg@10: 0.96285\tvalid_0's ndcg@20: 0.962982\tvalid_0's ndcg@30: 0.963018\tvalid_0's ndcg@40: 0.963018\tvalid_0's ndcg@50: 0.963018\n","[8]\tvalid_0's ndcg@10: 0.962779\tvalid_0's ndcg@20: 0.962911\tvalid_0's ndcg@30: 0.962947\tvalid_0's ndcg@40: 0.962947\tvalid_0's ndcg@50: 0.962947\tvalid_0's ndcg@10: 0.962779\tvalid_0's ndcg@20: 0.962911\tvalid_0's ndcg@30: 0.962947\tvalid_0's ndcg@40: 0.962947\tvalid_0's ndcg@50: 0.962947\n","[9]\tvalid_0's ndcg@10: 0.963397\tvalid_0's ndcg@20: 0.963528\tvalid_0's ndcg@30: 0.963564\tvalid_0's ndcg@40: 0.963564\tvalid_0's ndcg@50: 0.963564\tvalid_0's ndcg@10: 0.963397\tvalid_0's ndcg@20: 0.963528\tvalid_0's ndcg@30: 0.963564\tvalid_0's ndcg@40: 0.963564\tvalid_0's ndcg@50: 0.963564\n","[10]\tvalid_0's ndcg@10: 0.963523\tvalid_0's ndcg@20: 0.963655\tvalid_0's ndcg@30: 0.963691\tvalid_0's ndcg@40: 0.963691\tvalid_0's ndcg@50: 0.963691\tvalid_0's ndcg@10: 0.963523\tvalid_0's ndcg@20: 0.963655\tvalid_0's ndcg@30: 0.963691\tvalid_0's ndcg@40: 0.963691\tvalid_0's ndcg@50: 0.963691\n","[11]\tvalid_0's ndcg@10: 0.962895\tvalid_0's ndcg@20: 0.963027\tvalid_0's ndcg@30: 0.963063\tvalid_0's ndcg@40: 0.963063\tvalid_0's ndcg@50: 0.963063\tvalid_0's ndcg@10: 0.962895\tvalid_0's ndcg@20: 0.963027\tvalid_0's ndcg@30: 0.963063\tvalid_0's ndcg@40: 0.963063\tvalid_0's ndcg@50: 0.963063\n","[12]\tvalid_0's ndcg@10: 0.962937\tvalid_0's ndcg@20: 0.963069\tvalid_0's ndcg@30: 0.963105\tvalid_0's ndcg@40: 0.963105\tvalid_0's ndcg@50: 0.963105\tvalid_0's ndcg@10: 0.962937\tvalid_0's ndcg@20: 0.963069\tvalid_0's ndcg@30: 0.963105\tvalid_0's ndcg@40: 0.963105\tvalid_0's ndcg@50: 0.963105\n","[13]\tvalid_0's ndcg@10: 0.963156\tvalid_0's ndcg@20: 0.963287\tvalid_0's ndcg@30: 0.963323\tvalid_0's ndcg@40: 0.963323\tvalid_0's ndcg@50: 0.963323\tvalid_0's ndcg@10: 0.963156\tvalid_0's ndcg@20: 0.963287\tvalid_0's ndcg@30: 0.963323\tvalid_0's ndcg@40: 0.963323\tvalid_0's ndcg@50: 0.963323\n","[14]\tvalid_0's ndcg@10: 0.963095\tvalid_0's ndcg@20: 0.963227\tvalid_0's ndcg@30: 0.963263\tvalid_0's ndcg@40: 0.963263\tvalid_0's ndcg@50: 0.963263\tvalid_0's ndcg@10: 0.963095\tvalid_0's ndcg@20: 0.963227\tvalid_0's ndcg@30: 0.963263\tvalid_0's ndcg@40: 0.963263\tvalid_0's ndcg@50: 0.963263\n","[15]\tvalid_0's ndcg@10: 0.963264\tvalid_0's ndcg@20: 0.963395\tvalid_0's ndcg@30: 0.963432\tvalid_0's ndcg@40: 0.963432\tvalid_0's ndcg@50: 0.963432\tvalid_0's ndcg@10: 0.963264\tvalid_0's ndcg@20: 0.963395\tvalid_0's ndcg@30: 0.963432\tvalid_0's ndcg@40: 0.963432\tvalid_0's ndcg@50: 0.963432\n","[16]\tvalid_0's ndcg@10: 0.963583\tvalid_0's ndcg@20: 0.963714\tvalid_0's ndcg@30: 0.963751\tvalid_0's ndcg@40: 0.963751\tvalid_0's ndcg@50: 0.963751\tvalid_0's ndcg@10: 0.963583\tvalid_0's ndcg@20: 0.963714\tvalid_0's ndcg@30: 0.963751\tvalid_0's ndcg@40: 0.963751\tvalid_0's ndcg@50: 0.963751\n","[17]\tvalid_0's ndcg@10: 0.963196\tvalid_0's ndcg@20: 0.963328\tvalid_0's ndcg@30: 0.963364\tvalid_0's ndcg@40: 0.963364\tvalid_0's ndcg@50: 0.963364\tvalid_0's ndcg@10: 0.963196\tvalid_0's ndcg@20: 0.963328\tvalid_0's ndcg@30: 0.963364\tvalid_0's ndcg@40: 0.963364\tvalid_0's ndcg@50: 0.963364\n","[18]\tvalid_0's ndcg@10: 0.963077\tvalid_0's ndcg@20: 0.963208\tvalid_0's ndcg@30: 0.963244\tvalid_0's ndcg@40: 0.963244\tvalid_0's ndcg@50: 0.963244\tvalid_0's ndcg@10: 0.963077\tvalid_0's ndcg@20: 0.963208\tvalid_0's ndcg@30: 0.963244\tvalid_0's ndcg@40: 0.963244\tvalid_0's ndcg@50: 0.963244\n","[19]\tvalid_0's ndcg@10: 0.963488\tvalid_0's ndcg@20: 0.963619\tvalid_0's ndcg@30: 0.963655\tvalid_0's ndcg@40: 0.963655\tvalid_0's ndcg@50: 0.963655\tvalid_0's ndcg@10: 0.963488\tvalid_0's ndcg@20: 0.963619\tvalid_0's ndcg@30: 0.963655\tvalid_0's ndcg@40: 0.963655\tvalid_0's ndcg@50: 0.963655\n","[20]\tvalid_0's ndcg@10: 0.963227\tvalid_0's ndcg@20: 0.963359\tvalid_0's ndcg@30: 0.963395\tvalid_0's ndcg@40: 0.963395\tvalid_0's ndcg@50: 0.963395\tvalid_0's ndcg@10: 0.963227\tvalid_0's ndcg@20: 0.963359\tvalid_0's ndcg@30: 0.963395\tvalid_0's ndcg@40: 0.963395\tvalid_0's ndcg@50: 0.963395\n","[21]\tvalid_0's ndcg@10: 0.963285\tvalid_0's ndcg@20: 0.963416\tvalid_0's ndcg@30: 0.963452\tvalid_0's ndcg@40: 0.963452\tvalid_0's ndcg@50: 0.963452\tvalid_0's ndcg@10: 0.963285\tvalid_0's ndcg@20: 0.963416\tvalid_0's ndcg@30: 0.963452\tvalid_0's ndcg@40: 0.963452\tvalid_0's ndcg@50: 0.963452\n","[22]\tvalid_0's ndcg@10: 0.963699\tvalid_0's ndcg@20: 0.963831\tvalid_0's ndcg@30: 0.963867\tvalid_0's ndcg@40: 0.963867\tvalid_0's ndcg@50: 0.963867\tvalid_0's ndcg@10: 0.963699\tvalid_0's ndcg@20: 0.963831\tvalid_0's ndcg@30: 0.963867\tvalid_0's ndcg@40: 0.963867\tvalid_0's ndcg@50: 0.963867\n","[23]\tvalid_0's ndcg@10: 0.963439\tvalid_0's ndcg@20: 0.96357\tvalid_0's ndcg@30: 0.963606\tvalid_0's ndcg@40: 0.963606\tvalid_0's ndcg@50: 0.963606\tvalid_0's ndcg@10: 0.963439\tvalid_0's ndcg@20: 0.96357\tvalid_0's ndcg@30: 0.963606\tvalid_0's ndcg@40: 0.963606\tvalid_0's ndcg@50: 0.963606\n","[24]\tvalid_0's ndcg@10: 0.963278\tvalid_0's ndcg@20: 0.963409\tvalid_0's ndcg@30: 0.963445\tvalid_0's ndcg@40: 0.963445\tvalid_0's ndcg@50: 0.963445\tvalid_0's ndcg@10: 0.963278\tvalid_0's ndcg@20: 0.963409\tvalid_0's ndcg@30: 0.963445\tvalid_0's ndcg@40: 0.963445\tvalid_0's ndcg@50: 0.963445\n","[25]\tvalid_0's ndcg@10: 0.963268\tvalid_0's ndcg@20: 0.9634\tvalid_0's ndcg@30: 0.963436\tvalid_0's ndcg@40: 0.963436\tvalid_0's ndcg@50: 0.963436\tvalid_0's ndcg@10: 0.963268\tvalid_0's ndcg@20: 0.9634\tvalid_0's ndcg@30: 0.963436\tvalid_0's ndcg@40: 0.963436\tvalid_0's ndcg@50: 0.963436\n","[26]\tvalid_0's ndcg@10: 0.963362\tvalid_0's ndcg@20: 0.963494\tvalid_0's ndcg@30: 0.96353\tvalid_0's ndcg@40: 0.96353\tvalid_0's ndcg@50: 0.96353\tvalid_0's ndcg@10: 0.963362\tvalid_0's ndcg@20: 0.963494\tvalid_0's ndcg@30: 0.96353\tvalid_0's ndcg@40: 0.96353\tvalid_0's ndcg@50: 0.96353\n","[27]\tvalid_0's ndcg@10: 0.963266\tvalid_0's ndcg@20: 0.963397\tvalid_0's ndcg@30: 0.963433\tvalid_0's ndcg@40: 0.963433\tvalid_0's ndcg@50: 0.963433\tvalid_0's ndcg@10: 0.963266\tvalid_0's ndcg@20: 0.963397\tvalid_0's ndcg@30: 0.963433\tvalid_0's ndcg@40: 0.963433\tvalid_0's ndcg@50: 0.963433\n","[28]\tvalid_0's ndcg@10: 0.963435\tvalid_0's ndcg@20: 0.963567\tvalid_0's ndcg@30: 0.963603\tvalid_0's ndcg@40: 0.963603\tvalid_0's ndcg@50: 0.963603\tvalid_0's ndcg@10: 0.963435\tvalid_0's ndcg@20: 0.963567\tvalid_0's ndcg@30: 0.963603\tvalid_0's ndcg@40: 0.963603\tvalid_0's ndcg@50: 0.963603\n","[29]\tvalid_0's ndcg@10: 0.963466\tvalid_0's ndcg@20: 0.963598\tvalid_0's ndcg@30: 0.963635\tvalid_0's ndcg@40: 0.963635\tvalid_0's ndcg@50: 0.963635\tvalid_0's ndcg@10: 0.963466\tvalid_0's ndcg@20: 0.963598\tvalid_0's ndcg@30: 0.963635\tvalid_0's ndcg@40: 0.963635\tvalid_0's ndcg@50: 0.963635\n","[30]\tvalid_0's ndcg@10: 0.963794\tvalid_0's ndcg@20: 0.963927\tvalid_0's ndcg@30: 0.963963\tvalid_0's ndcg@40: 0.963963\tvalid_0's ndcg@50: 0.963963\tvalid_0's ndcg@10: 0.963794\tvalid_0's ndcg@20: 0.963927\tvalid_0's ndcg@30: 0.963963\tvalid_0's ndcg@40: 0.963963\tvalid_0's ndcg@50: 0.963963\n","[31]\tvalid_0's ndcg@10: 0.964\tvalid_0's ndcg@20: 0.964133\tvalid_0's ndcg@30: 0.964169\tvalid_0's ndcg@40: 0.964169\tvalid_0's ndcg@50: 0.964169\tvalid_0's ndcg@10: 0.964\tvalid_0's ndcg@20: 0.964133\tvalid_0's ndcg@30: 0.964169\tvalid_0's ndcg@40: 0.964169\tvalid_0's ndcg@50: 0.964169\n","[32]\tvalid_0's ndcg@10: 0.963683\tvalid_0's ndcg@20: 0.963816\tvalid_0's ndcg@30: 0.963852\tvalid_0's ndcg@40: 0.963852\tvalid_0's ndcg@50: 0.963852\tvalid_0's ndcg@10: 0.963683\tvalid_0's ndcg@20: 0.963816\tvalid_0's ndcg@30: 0.963852\tvalid_0's ndcg@40: 0.963852\tvalid_0's ndcg@50: 0.963852\n","[33]\tvalid_0's ndcg@10: 0.963573\tvalid_0's ndcg@20: 0.963706\tvalid_0's ndcg@30: 0.963743\tvalid_0's ndcg@40: 0.963743\tvalid_0's ndcg@50: 0.963743\tvalid_0's ndcg@10: 0.963573\tvalid_0's ndcg@20: 0.963706\tvalid_0's ndcg@30: 0.963743\tvalid_0's ndcg@40: 0.963743\tvalid_0's ndcg@50: 0.963743\n","[34]\tvalid_0's ndcg@10: 0.963961\tvalid_0's ndcg@20: 0.964097\tvalid_0's ndcg@30: 0.964133\tvalid_0's ndcg@40: 0.964133\tvalid_0's ndcg@50: 0.964133\tvalid_0's ndcg@10: 0.963961\tvalid_0's ndcg@20: 0.964097\tvalid_0's ndcg@30: 0.964133\tvalid_0's ndcg@40: 0.964133\tvalid_0's ndcg@50: 0.964133\n","[35]\tvalid_0's ndcg@10: 0.964064\tvalid_0's ndcg@20: 0.9642\tvalid_0's ndcg@30: 0.964236\tvalid_0's ndcg@40: 0.964236\tvalid_0's ndcg@50: 0.964236\tvalid_0's ndcg@10: 0.964064\tvalid_0's ndcg@20: 0.9642\tvalid_0's ndcg@30: 0.964236\tvalid_0's ndcg@40: 0.964236\tvalid_0's ndcg@50: 0.964236\n","[36]\tvalid_0's ndcg@10: 0.963912\tvalid_0's ndcg@20: 0.964048\tvalid_0's ndcg@30: 0.964084\tvalid_0's ndcg@40: 0.964084\tvalid_0's ndcg@50: 0.964084\tvalid_0's ndcg@10: 0.963912\tvalid_0's ndcg@20: 0.964048\tvalid_0's ndcg@30: 0.964084\tvalid_0's ndcg@40: 0.964084\tvalid_0's ndcg@50: 0.964084\n","[37]\tvalid_0's ndcg@10: 0.963994\tvalid_0's ndcg@20: 0.96413\tvalid_0's ndcg@30: 0.964166\tvalid_0's ndcg@40: 0.964166\tvalid_0's ndcg@50: 0.964166\tvalid_0's ndcg@10: 0.963994\tvalid_0's ndcg@20: 0.96413\tvalid_0's ndcg@30: 0.964166\tvalid_0's ndcg@40: 0.964166\tvalid_0's ndcg@50: 0.964166\n","[38]\tvalid_0's ndcg@10: 0.963828\tvalid_0's ndcg@20: 0.963964\tvalid_0's ndcg@30: 0.964\tvalid_0's ndcg@40: 0.964\tvalid_0's ndcg@50: 0.964\tvalid_0's ndcg@10: 0.963828\tvalid_0's ndcg@20: 0.963964\tvalid_0's ndcg@30: 0.964\tvalid_0's ndcg@40: 0.964\tvalid_0's ndcg@50: 0.964\n","[39]\tvalid_0's ndcg@10: 0.963871\tvalid_0's ndcg@20: 0.964006\tvalid_0's ndcg@30: 0.964042\tvalid_0's ndcg@40: 0.964042\tvalid_0's ndcg@50: 0.964042\tvalid_0's ndcg@10: 0.963871\tvalid_0's ndcg@20: 0.964006\tvalid_0's ndcg@30: 0.964042\tvalid_0's ndcg@40: 0.964042\tvalid_0's ndcg@50: 0.964042\n","[40]\tvalid_0's ndcg@10: 0.963871\tvalid_0's ndcg@20: 0.964007\tvalid_0's ndcg@30: 0.964043\tvalid_0's ndcg@40: 0.964043\tvalid_0's ndcg@50: 0.964043\tvalid_0's ndcg@10: 0.963871\tvalid_0's ndcg@20: 0.964007\tvalid_0's ndcg@30: 0.964043\tvalid_0's ndcg@40: 0.964043\tvalid_0's ndcg@50: 0.964043\n","[41]\tvalid_0's ndcg@10: 0.963681\tvalid_0's ndcg@20: 0.963865\tvalid_0's ndcg@30: 0.963901\tvalid_0's ndcg@40: 0.963901\tvalid_0's ndcg@50: 0.963901\tvalid_0's ndcg@10: 0.963681\tvalid_0's ndcg@20: 0.963865\tvalid_0's ndcg@30: 0.963901\tvalid_0's ndcg@40: 0.963901\tvalid_0's ndcg@50: 0.963901\n","[42]\tvalid_0's ndcg@10: 0.963469\tvalid_0's ndcg@20: 0.963652\tvalid_0's ndcg@30: 0.963689\tvalid_0's ndcg@40: 0.963689\tvalid_0's ndcg@50: 0.963689\tvalid_0's ndcg@10: 0.963469\tvalid_0's ndcg@20: 0.963652\tvalid_0's ndcg@30: 0.963689\tvalid_0's ndcg@40: 0.963689\tvalid_0's ndcg@50: 0.963689\n","[43]\tvalid_0's ndcg@10: 0.963385\tvalid_0's ndcg@20: 0.963568\tvalid_0's ndcg@30: 0.963604\tvalid_0's ndcg@40: 0.963604\tvalid_0's ndcg@50: 0.963604\tvalid_0's ndcg@10: 0.963385\tvalid_0's ndcg@20: 0.963568\tvalid_0's ndcg@30: 0.963604\tvalid_0's ndcg@40: 0.963604\tvalid_0's ndcg@50: 0.963604\n","[44]\tvalid_0's ndcg@10: 0.963418\tvalid_0's ndcg@20: 0.963602\tvalid_0's ndcg@30: 0.963638\tvalid_0's ndcg@40: 0.963638\tvalid_0's ndcg@50: 0.963638\tvalid_0's ndcg@10: 0.963418\tvalid_0's ndcg@20: 0.963602\tvalid_0's ndcg@30: 0.963638\tvalid_0's ndcg@40: 0.963638\tvalid_0's ndcg@50: 0.963638\n","[45]\tvalid_0's ndcg@10: 0.963396\tvalid_0's ndcg@20: 0.963579\tvalid_0's ndcg@30: 0.963616\tvalid_0's ndcg@40: 0.963616\tvalid_0's ndcg@50: 0.963616\tvalid_0's ndcg@10: 0.963396\tvalid_0's ndcg@20: 0.963579\tvalid_0's ndcg@30: 0.963616\tvalid_0's ndcg@40: 0.963616\tvalid_0's ndcg@50: 0.963616\n","[46]\tvalid_0's ndcg@10: 0.963542\tvalid_0's ndcg@20: 0.963725\tvalid_0's ndcg@30: 0.963761\tvalid_0's ndcg@40: 0.963761\tvalid_0's ndcg@50: 0.963761\tvalid_0's ndcg@10: 0.963542\tvalid_0's ndcg@20: 0.963725\tvalid_0's ndcg@30: 0.963761\tvalid_0's ndcg@40: 0.963761\tvalid_0's ndcg@50: 0.963761\n","[47]\tvalid_0's ndcg@10: 0.963463\tvalid_0's ndcg@20: 0.963646\tvalid_0's ndcg@30: 0.963682\tvalid_0's ndcg@40: 0.963682\tvalid_0's ndcg@50: 0.963682\tvalid_0's ndcg@10: 0.963463\tvalid_0's ndcg@20: 0.963646\tvalid_0's ndcg@30: 0.963682\tvalid_0's ndcg@40: 0.963682\tvalid_0's ndcg@50: 0.963682\n","[48]\tvalid_0's ndcg@10: 0.963321\tvalid_0's ndcg@20: 0.963504\tvalid_0's ndcg@30: 0.963541\tvalid_0's ndcg@40: 0.963541\tvalid_0's ndcg@50: 0.963541\tvalid_0's ndcg@10: 0.963321\tvalid_0's ndcg@20: 0.963504\tvalid_0's ndcg@30: 0.963541\tvalid_0's ndcg@40: 0.963541\tvalid_0's ndcg@50: 0.963541\n","[49]\tvalid_0's ndcg@10: 0.963476\tvalid_0's ndcg@20: 0.963659\tvalid_0's ndcg@30: 0.963695\tvalid_0's ndcg@40: 0.963695\tvalid_0's ndcg@50: 0.963695\tvalid_0's ndcg@10: 0.963476\tvalid_0's ndcg@20: 0.963659\tvalid_0's ndcg@30: 0.963695\tvalid_0's ndcg@40: 0.963695\tvalid_0's ndcg@50: 0.963695\n","[50]\tvalid_0's ndcg@10: 0.963368\tvalid_0's ndcg@20: 0.963552\tvalid_0's ndcg@30: 0.963588\tvalid_0's ndcg@40: 0.963588\tvalid_0's ndcg@50: 0.963588\tvalid_0's ndcg@10: 0.963368\tvalid_0's ndcg@20: 0.963552\tvalid_0's ndcg@30: 0.963588\tvalid_0's ndcg@40: 0.963588\tvalid_0's ndcg@50: 0.963588\n","[51]\tvalid_0's ndcg@10: 0.963294\tvalid_0's ndcg@20: 0.963477\tvalid_0's ndcg@30: 0.963513\tvalid_0's ndcg@40: 0.963513\tvalid_0's ndcg@50: 0.963513\tvalid_0's ndcg@10: 0.963294\tvalid_0's ndcg@20: 0.963477\tvalid_0's ndcg@30: 0.963513\tvalid_0's ndcg@40: 0.963513\tvalid_0's ndcg@50: 0.963513\n","[52]\tvalid_0's ndcg@10: 0.96334\tvalid_0's ndcg@20: 0.963523\tvalid_0's ndcg@30: 0.96356\tvalid_0's ndcg@40: 0.96356\tvalid_0's ndcg@50: 0.96356\tvalid_0's ndcg@10: 0.96334\tvalid_0's ndcg@20: 0.963523\tvalid_0's ndcg@30: 0.96356\tvalid_0's ndcg@40: 0.96356\tvalid_0's ndcg@50: 0.96356\n","[53]\tvalid_0's ndcg@10: 0.963175\tvalid_0's ndcg@20: 0.963358\tvalid_0's ndcg@30: 0.963396\tvalid_0's ndcg@40: 0.963396\tvalid_0's ndcg@50: 0.963396\tvalid_0's ndcg@10: 0.963175\tvalid_0's ndcg@20: 0.963358\tvalid_0's ndcg@30: 0.963396\tvalid_0's ndcg@40: 0.963396\tvalid_0's ndcg@50: 0.963396\n","[54]\tvalid_0's ndcg@10: 0.963226\tvalid_0's ndcg@20: 0.963409\tvalid_0's ndcg@30: 0.963447\tvalid_0's ndcg@40: 0.963447\tvalid_0's ndcg@50: 0.963447\tvalid_0's ndcg@10: 0.963226\tvalid_0's ndcg@20: 0.963409\tvalid_0's ndcg@30: 0.963447\tvalid_0's ndcg@40: 0.963447\tvalid_0's ndcg@50: 0.963447\n","[55]\tvalid_0's ndcg@10: 0.963201\tvalid_0's ndcg@20: 0.963384\tvalid_0's ndcg@30: 0.963422\tvalid_0's ndcg@40: 0.963422\tvalid_0's ndcg@50: 0.963422\tvalid_0's ndcg@10: 0.963201\tvalid_0's ndcg@20: 0.963384\tvalid_0's ndcg@30: 0.963422\tvalid_0's ndcg@40: 0.963422\tvalid_0's ndcg@50: 0.963422\n","[56]\tvalid_0's ndcg@10: 0.963021\tvalid_0's ndcg@20: 0.963204\tvalid_0's ndcg@30: 0.963242\tvalid_0's ndcg@40: 0.963242\tvalid_0's ndcg@50: 0.963242\tvalid_0's ndcg@10: 0.963021\tvalid_0's ndcg@20: 0.963204\tvalid_0's ndcg@30: 0.963242\tvalid_0's ndcg@40: 0.963242\tvalid_0's ndcg@50: 0.963242\n","[57]\tvalid_0's ndcg@10: 0.963185\tvalid_0's ndcg@20: 0.96332\tvalid_0's ndcg@30: 0.963358\tvalid_0's ndcg@40: 0.963358\tvalid_0's ndcg@50: 0.963358\tvalid_0's ndcg@10: 0.963185\tvalid_0's ndcg@20: 0.96332\tvalid_0's ndcg@30: 0.963358\tvalid_0's ndcg@40: 0.963358\tvalid_0's ndcg@50: 0.963358\n","[58]\tvalid_0's ndcg@10: 0.963013\tvalid_0's ndcg@20: 0.963196\tvalid_0's ndcg@30: 0.963234\tvalid_0's ndcg@40: 0.963234\tvalid_0's ndcg@50: 0.963234\tvalid_0's ndcg@10: 0.963013\tvalid_0's ndcg@20: 0.963196\tvalid_0's ndcg@30: 0.963234\tvalid_0's ndcg@40: 0.963234\tvalid_0's ndcg@50: 0.963234\n","[59]\tvalid_0's ndcg@10: 0.963106\tvalid_0's ndcg@20: 0.963289\tvalid_0's ndcg@30: 0.963326\tvalid_0's ndcg@40: 0.963326\tvalid_0's ndcg@50: 0.963326\tvalid_0's ndcg@10: 0.963106\tvalid_0's ndcg@20: 0.963289\tvalid_0's ndcg@30: 0.963326\tvalid_0's ndcg@40: 0.963326\tvalid_0's ndcg@50: 0.963326\n","[60]\tvalid_0's ndcg@10: 0.963043\tvalid_0's ndcg@20: 0.963226\tvalid_0's ndcg@30: 0.963264\tvalid_0's ndcg@40: 0.963264\tvalid_0's ndcg@50: 0.963264\tvalid_0's ndcg@10: 0.963043\tvalid_0's ndcg@20: 0.963226\tvalid_0's ndcg@30: 0.963264\tvalid_0's ndcg@40: 0.963264\tvalid_0's ndcg@50: 0.963264\n","[61]\tvalid_0's ndcg@10: 0.963229\tvalid_0's ndcg@20: 0.963412\tvalid_0's ndcg@30: 0.96345\tvalid_0's ndcg@40: 0.96345\tvalid_0's ndcg@50: 0.96345\tvalid_0's ndcg@10: 0.963229\tvalid_0's ndcg@20: 0.963412\tvalid_0's ndcg@30: 0.96345\tvalid_0's ndcg@40: 0.96345\tvalid_0's ndcg@50: 0.96345\n","[62]\tvalid_0's ndcg@10: 0.963133\tvalid_0's ndcg@20: 0.963316\tvalid_0's ndcg@30: 0.963354\tvalid_0's ndcg@40: 0.963354\tvalid_0's ndcg@50: 0.963354\tvalid_0's ndcg@10: 0.963133\tvalid_0's ndcg@20: 0.963316\tvalid_0's ndcg@30: 0.963354\tvalid_0's ndcg@40: 0.963354\tvalid_0's ndcg@50: 0.963354\n","[63]\tvalid_0's ndcg@10: 0.963147\tvalid_0's ndcg@20: 0.96333\tvalid_0's ndcg@30: 0.963368\tvalid_0's ndcg@40: 0.963368\tvalid_0's ndcg@50: 0.963368\tvalid_0's ndcg@10: 0.963147\tvalid_0's ndcg@20: 0.96333\tvalid_0's ndcg@30: 0.963368\tvalid_0's ndcg@40: 0.963368\tvalid_0's ndcg@50: 0.963368\n","[64]\tvalid_0's ndcg@10: 0.963076\tvalid_0's ndcg@20: 0.963259\tvalid_0's ndcg@30: 0.963296\tvalid_0's ndcg@40: 0.963296\tvalid_0's ndcg@50: 0.963296\tvalid_0's ndcg@10: 0.963076\tvalid_0's ndcg@20: 0.963259\tvalid_0's ndcg@30: 0.963296\tvalid_0's ndcg@40: 0.963296\tvalid_0's ndcg@50: 0.963296\n","[65]\tvalid_0's ndcg@10: 0.963178\tvalid_0's ndcg@20: 0.963361\tvalid_0's ndcg@30: 0.963399\tvalid_0's ndcg@40: 0.963399\tvalid_0's ndcg@50: 0.963399\tvalid_0's ndcg@10: 0.963178\tvalid_0's ndcg@20: 0.963361\tvalid_0's ndcg@30: 0.963399\tvalid_0's ndcg@40: 0.963399\tvalid_0's ndcg@50: 0.963399\n","[66]\tvalid_0's ndcg@10: 0.963412\tvalid_0's ndcg@20: 0.963595\tvalid_0's ndcg@30: 0.963633\tvalid_0's ndcg@40: 0.963633\tvalid_0's ndcg@50: 0.963633\tvalid_0's ndcg@10: 0.963412\tvalid_0's ndcg@20: 0.963595\tvalid_0's ndcg@30: 0.963633\tvalid_0's ndcg@40: 0.963633\tvalid_0's ndcg@50: 0.963633\n","[67]\tvalid_0's ndcg@10: 0.963415\tvalid_0's ndcg@20: 0.963598\tvalid_0's ndcg@30: 0.963635\tvalid_0's ndcg@40: 0.963635\tvalid_0's ndcg@50: 0.963635\tvalid_0's ndcg@10: 0.963415\tvalid_0's ndcg@20: 0.963598\tvalid_0's ndcg@30: 0.963635\tvalid_0's ndcg@40: 0.963635\tvalid_0's ndcg@50: 0.963635\n","[68]\tvalid_0's ndcg@10: 0.963432\tvalid_0's ndcg@20: 0.963615\tvalid_0's ndcg@30: 0.963653\tvalid_0's ndcg@40: 0.963653\tvalid_0's ndcg@50: 0.963653\tvalid_0's ndcg@10: 0.963432\tvalid_0's ndcg@20: 0.963615\tvalid_0's ndcg@30: 0.963653\tvalid_0's ndcg@40: 0.963653\tvalid_0's ndcg@50: 0.963653\n","[69]\tvalid_0's ndcg@10: 0.963545\tvalid_0's ndcg@20: 0.963728\tvalid_0's ndcg@30: 0.963765\tvalid_0's ndcg@40: 0.963765\tvalid_0's ndcg@50: 0.963765\tvalid_0's ndcg@10: 0.963545\tvalid_0's ndcg@20: 0.963728\tvalid_0's ndcg@30: 0.963765\tvalid_0's ndcg@40: 0.963765\tvalid_0's ndcg@50: 0.963765\n","[70]\tvalid_0's ndcg@10: 0.963407\tvalid_0's ndcg@20: 0.96359\tvalid_0's ndcg@30: 0.963627\tvalid_0's ndcg@40: 0.963627\tvalid_0's ndcg@50: 0.963627\tvalid_0's ndcg@10: 0.963407\tvalid_0's ndcg@20: 0.96359\tvalid_0's ndcg@30: 0.963627\tvalid_0's ndcg@40: 0.963627\tvalid_0's ndcg@50: 0.963627\n","[71]\tvalid_0's ndcg@10: 0.963511\tvalid_0's ndcg@20: 0.963694\tvalid_0's ndcg@30: 0.963731\tvalid_0's ndcg@40: 0.963731\tvalid_0's ndcg@50: 0.963731\tvalid_0's ndcg@10: 0.963511\tvalid_0's ndcg@20: 0.963694\tvalid_0's ndcg@30: 0.963731\tvalid_0's ndcg@40: 0.963731\tvalid_0's ndcg@50: 0.963731\n","[72]\tvalid_0's ndcg@10: 0.963419\tvalid_0's ndcg@20: 0.963601\tvalid_0's ndcg@30: 0.963639\tvalid_0's ndcg@40: 0.963639\tvalid_0's ndcg@50: 0.963639\tvalid_0's ndcg@10: 0.963419\tvalid_0's ndcg@20: 0.963601\tvalid_0's ndcg@30: 0.963639\tvalid_0's ndcg@40: 0.963639\tvalid_0's ndcg@50: 0.963639\n","[73]\tvalid_0's ndcg@10: 0.963625\tvalid_0's ndcg@20: 0.963807\tvalid_0's ndcg@30: 0.963845\tvalid_0's ndcg@40: 0.963845\tvalid_0's ndcg@50: 0.963845\tvalid_0's ndcg@10: 0.963625\tvalid_0's ndcg@20: 0.963807\tvalid_0's ndcg@30: 0.963845\tvalid_0's ndcg@40: 0.963845\tvalid_0's ndcg@50: 0.963845\n","[74]\tvalid_0's ndcg@10: 0.963558\tvalid_0's ndcg@20: 0.96374\tvalid_0's ndcg@30: 0.963778\tvalid_0's ndcg@40: 0.963778\tvalid_0's ndcg@50: 0.963778\tvalid_0's ndcg@10: 0.963558\tvalid_0's ndcg@20: 0.96374\tvalid_0's ndcg@30: 0.963778\tvalid_0's ndcg@40: 0.963778\tvalid_0's ndcg@50: 0.963778\n","[75]\tvalid_0's ndcg@10: 0.963436\tvalid_0's ndcg@20: 0.963617\tvalid_0's ndcg@30: 0.963654\tvalid_0's ndcg@40: 0.963654\tvalid_0's ndcg@50: 0.963654\tvalid_0's ndcg@10: 0.963436\tvalid_0's ndcg@20: 0.963617\tvalid_0's ndcg@30: 0.963654\tvalid_0's ndcg@40: 0.963654\tvalid_0's ndcg@50: 0.963654\n","[76]\tvalid_0's ndcg@10: 0.963399\tvalid_0's ndcg@20: 0.963581\tvalid_0's ndcg@30: 0.963619\tvalid_0's ndcg@40: 0.963619\tvalid_0's ndcg@50: 0.963619\tvalid_0's ndcg@10: 0.963399\tvalid_0's ndcg@20: 0.963581\tvalid_0's ndcg@30: 0.963619\tvalid_0's ndcg@40: 0.963619\tvalid_0's ndcg@50: 0.963619\n","[77]\tvalid_0's ndcg@10: 0.96345\tvalid_0's ndcg@20: 0.963632\tvalid_0's ndcg@30: 0.96367\tvalid_0's ndcg@40: 0.96367\tvalid_0's ndcg@50: 0.96367\tvalid_0's ndcg@10: 0.96345\tvalid_0's ndcg@20: 0.963632\tvalid_0's ndcg@30: 0.96367\tvalid_0's ndcg@40: 0.96367\tvalid_0's ndcg@50: 0.96367\n","[78]\tvalid_0's ndcg@10: 0.963483\tvalid_0's ndcg@20: 0.963665\tvalid_0's ndcg@30: 0.963703\tvalid_0's ndcg@40: 0.963703\tvalid_0's ndcg@50: 0.963703\tvalid_0's ndcg@10: 0.963483\tvalid_0's ndcg@20: 0.963665\tvalid_0's ndcg@30: 0.963703\tvalid_0's ndcg@40: 0.963703\tvalid_0's ndcg@50: 0.963703\n","[79]\tvalid_0's ndcg@10: 0.963499\tvalid_0's ndcg@20: 0.963681\tvalid_0's ndcg@30: 0.963719\tvalid_0's ndcg@40: 0.963719\tvalid_0's ndcg@50: 0.963719\tvalid_0's ndcg@10: 0.963499\tvalid_0's ndcg@20: 0.963681\tvalid_0's ndcg@30: 0.963719\tvalid_0's ndcg@40: 0.963719\tvalid_0's ndcg@50: 0.963719\n","[80]\tvalid_0's ndcg@10: 0.963408\tvalid_0's ndcg@20: 0.963589\tvalid_0's ndcg@30: 0.963627\tvalid_0's ndcg@40: 0.963627\tvalid_0's ndcg@50: 0.963627\tvalid_0's ndcg@10: 0.963408\tvalid_0's ndcg@20: 0.963589\tvalid_0's ndcg@30: 0.963627\tvalid_0's ndcg@40: 0.963627\tvalid_0's ndcg@50: 0.963627\n","[81]\tvalid_0's ndcg@10: 0.963494\tvalid_0's ndcg@20: 0.963676\tvalid_0's ndcg@30: 0.963714\tvalid_0's ndcg@40: 0.963714\tvalid_0's ndcg@50: 0.963714\tvalid_0's ndcg@10: 0.963494\tvalid_0's ndcg@20: 0.963676\tvalid_0's ndcg@30: 0.963714\tvalid_0's ndcg@40: 0.963714\tvalid_0's ndcg@50: 0.963714\n","[82]\tvalid_0's ndcg@10: 0.963489\tvalid_0's ndcg@20: 0.963671\tvalid_0's ndcg@30: 0.963709\tvalid_0's ndcg@40: 0.963709\tvalid_0's ndcg@50: 0.963709\tvalid_0's ndcg@10: 0.963489\tvalid_0's ndcg@20: 0.963671\tvalid_0's ndcg@30: 0.963709\tvalid_0's ndcg@40: 0.963709\tvalid_0's ndcg@50: 0.963709\n","[83]\tvalid_0's ndcg@10: 0.963575\tvalid_0's ndcg@20: 0.963757\tvalid_0's ndcg@30: 0.963795\tvalid_0's ndcg@40: 0.963795\tvalid_0's ndcg@50: 0.963795\tvalid_0's ndcg@10: 0.963575\tvalid_0's ndcg@20: 0.963757\tvalid_0's ndcg@30: 0.963795\tvalid_0's ndcg@40: 0.963795\tvalid_0's ndcg@50: 0.963795\n","[84]\tvalid_0's ndcg@10: 0.963443\tvalid_0's ndcg@20: 0.963625\tvalid_0's ndcg@30: 0.963663\tvalid_0's ndcg@40: 0.963663\tvalid_0's ndcg@50: 0.963663\tvalid_0's ndcg@10: 0.963443\tvalid_0's ndcg@20: 0.963625\tvalid_0's ndcg@30: 0.963663\tvalid_0's ndcg@40: 0.963663\tvalid_0's ndcg@50: 0.963663\n","[85]\tvalid_0's ndcg@10: 0.963221\tvalid_0's ndcg@20: 0.963403\tvalid_0's ndcg@30: 0.963441\tvalid_0's ndcg@40: 0.963441\tvalid_0's ndcg@50: 0.963441\tvalid_0's ndcg@10: 0.963221\tvalid_0's ndcg@20: 0.963403\tvalid_0's ndcg@30: 0.963441\tvalid_0's ndcg@40: 0.963441\tvalid_0's ndcg@50: 0.963441\n","Early stopping, best iteration is:\n","[35]\tvalid_0's ndcg@10: 0.964064\tvalid_0's ndcg@20: 0.9642\tvalid_0's ndcg@30: 0.964236\tvalid_0's ndcg@40: 0.964236\tvalid_0's ndcg@50: 0.964236\tvalid_0's ndcg@10: 0.964064\tvalid_0's ndcg@20: 0.9642\tvalid_0's ndcg@30: 0.964236\tvalid_0's ndcg@40: 0.964236\tvalid_0's ndcg@50: 0.964236\n","[1]\tvalid_0's ndcg@10: 0.959809\tvalid_0's ndcg@20: 0.960131\tvalid_0's ndcg@30: 0.960169\tvalid_0's ndcg@40: 0.960169\tvalid_0's ndcg@50: 0.960169\tvalid_0's ndcg@10: 0.959809\tvalid_0's ndcg@20: 0.960131\tvalid_0's ndcg@30: 0.960169\tvalid_0's ndcg@40: 0.960169\tvalid_0's ndcg@50: 0.960169\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.961207\tvalid_0's ndcg@20: 0.96139\tvalid_0's ndcg@30: 0.961428\tvalid_0's ndcg@40: 0.961428\tvalid_0's ndcg@50: 0.961428\tvalid_0's ndcg@10: 0.961207\tvalid_0's ndcg@20: 0.96139\tvalid_0's ndcg@30: 0.961428\tvalid_0's ndcg@40: 0.961428\tvalid_0's ndcg@50: 0.961428\n","[3]\tvalid_0's ndcg@10: 0.962337\tvalid_0's ndcg@20: 0.962472\tvalid_0's ndcg@30: 0.96251\tvalid_0's ndcg@40: 0.96251\tvalid_0's ndcg@50: 0.96251\tvalid_0's ndcg@10: 0.962337\tvalid_0's ndcg@20: 0.962472\tvalid_0's ndcg@30: 0.96251\tvalid_0's ndcg@40: 0.96251\tvalid_0's ndcg@50: 0.96251\n","[4]\tvalid_0's ndcg@10: 0.962872\tvalid_0's ndcg@20: 0.963099\tvalid_0's ndcg@30: 0.963099\tvalid_0's ndcg@40: 0.963099\tvalid_0's ndcg@50: 0.963099\tvalid_0's ndcg@10: 0.962872\tvalid_0's ndcg@20: 0.963099\tvalid_0's ndcg@30: 0.963099\tvalid_0's ndcg@40: 0.963099\tvalid_0's ndcg@50: 0.963099\n","[5]\tvalid_0's ndcg@10: 0.962321\tvalid_0's ndcg@20: 0.962594\tvalid_0's ndcg@30: 0.962594\tvalid_0's ndcg@40: 0.962594\tvalid_0's ndcg@50: 0.962594\tvalid_0's ndcg@10: 0.962321\tvalid_0's ndcg@20: 0.962594\tvalid_0's ndcg@30: 0.962594\tvalid_0's ndcg@40: 0.962594\tvalid_0's ndcg@50: 0.962594\n","[6]\tvalid_0's ndcg@10: 0.963292\tvalid_0's ndcg@20: 0.963467\tvalid_0's ndcg@30: 0.963467\tvalid_0's ndcg@40: 0.963467\tvalid_0's ndcg@50: 0.963467\tvalid_0's ndcg@10: 0.963292\tvalid_0's ndcg@20: 0.963467\tvalid_0's ndcg@30: 0.963467\tvalid_0's ndcg@40: 0.963467\tvalid_0's ndcg@50: 0.963467\n","[7]\tvalid_0's ndcg@10: 0.962924\tvalid_0's ndcg@20: 0.963098\tvalid_0's ndcg@30: 0.963098\tvalid_0's ndcg@40: 0.963098\tvalid_0's ndcg@50: 0.963098\tvalid_0's ndcg@10: 0.962924\tvalid_0's ndcg@20: 0.963098\tvalid_0's ndcg@30: 0.963098\tvalid_0's ndcg@40: 0.963098\tvalid_0's ndcg@50: 0.963098\n","[8]\tvalid_0's ndcg@10: 0.963258\tvalid_0's ndcg@20: 0.963478\tvalid_0's ndcg@30: 0.963478\tvalid_0's ndcg@40: 0.963478\tvalid_0's ndcg@50: 0.963478\tvalid_0's ndcg@10: 0.963258\tvalid_0's ndcg@20: 0.963478\tvalid_0's ndcg@30: 0.963478\tvalid_0's ndcg@40: 0.963478\tvalid_0's ndcg@50: 0.963478\n","[9]\tvalid_0's ndcg@10: 0.963268\tvalid_0's ndcg@20: 0.963442\tvalid_0's ndcg@30: 0.963442\tvalid_0's ndcg@40: 0.963442\tvalid_0's ndcg@50: 0.963442\tvalid_0's ndcg@10: 0.963268\tvalid_0's ndcg@20: 0.963442\tvalid_0's ndcg@30: 0.963442\tvalid_0's ndcg@40: 0.963442\tvalid_0's ndcg@50: 0.963442\n","[10]\tvalid_0's ndcg@10: 0.963365\tvalid_0's ndcg@20: 0.963539\tvalid_0's ndcg@30: 0.963539\tvalid_0's ndcg@40: 0.963539\tvalid_0's ndcg@50: 0.963539\tvalid_0's ndcg@10: 0.963365\tvalid_0's ndcg@20: 0.963539\tvalid_0's ndcg@30: 0.963539\tvalid_0's ndcg@40: 0.963539\tvalid_0's ndcg@50: 0.963539\n","[11]\tvalid_0's ndcg@10: 0.9636\tvalid_0's ndcg@20: 0.963821\tvalid_0's ndcg@30: 0.963821\tvalid_0's ndcg@40: 0.963821\tvalid_0's ndcg@50: 0.963821\tvalid_0's ndcg@10: 0.9636\tvalid_0's ndcg@20: 0.963821\tvalid_0's ndcg@30: 0.963821\tvalid_0's ndcg@40: 0.963821\tvalid_0's ndcg@50: 0.963821\n","[12]\tvalid_0's ndcg@10: 0.963682\tvalid_0's ndcg@20: 0.963903\tvalid_0's ndcg@30: 0.963903\tvalid_0's ndcg@40: 0.963903\tvalid_0's ndcg@50: 0.963903\tvalid_0's ndcg@10: 0.963682\tvalid_0's ndcg@20: 0.963903\tvalid_0's ndcg@30: 0.963903\tvalid_0's ndcg@40: 0.963903\tvalid_0's ndcg@50: 0.963903\n","[13]\tvalid_0's ndcg@10: 0.963721\tvalid_0's ndcg@20: 0.963943\tvalid_0's ndcg@30: 0.963943\tvalid_0's ndcg@40: 0.963943\tvalid_0's ndcg@50: 0.963943\tvalid_0's ndcg@10: 0.963721\tvalid_0's ndcg@20: 0.963943\tvalid_0's ndcg@30: 0.963943\tvalid_0's ndcg@40: 0.963943\tvalid_0's ndcg@50: 0.963943\n","[14]\tvalid_0's ndcg@10: 0.963551\tvalid_0's ndcg@20: 0.963772\tvalid_0's ndcg@30: 0.963772\tvalid_0's ndcg@40: 0.963772\tvalid_0's ndcg@50: 0.963772\tvalid_0's ndcg@10: 0.963551\tvalid_0's ndcg@20: 0.963772\tvalid_0's ndcg@30: 0.963772\tvalid_0's ndcg@40: 0.963772\tvalid_0's ndcg@50: 0.963772\n","[15]\tvalid_0's ndcg@10: 0.963627\tvalid_0's ndcg@20: 0.963849\tvalid_0's ndcg@30: 0.963849\tvalid_0's ndcg@40: 0.963849\tvalid_0's ndcg@50: 0.963849\tvalid_0's ndcg@10: 0.963627\tvalid_0's ndcg@20: 0.963849\tvalid_0's ndcg@30: 0.963849\tvalid_0's ndcg@40: 0.963849\tvalid_0's ndcg@50: 0.963849\n","[16]\tvalid_0's ndcg@10: 0.963618\tvalid_0's ndcg@20: 0.963794\tvalid_0's ndcg@30: 0.963794\tvalid_0's ndcg@40: 0.963794\tvalid_0's ndcg@50: 0.963794\tvalid_0's ndcg@10: 0.963618\tvalid_0's ndcg@20: 0.963794\tvalid_0's ndcg@30: 0.963794\tvalid_0's ndcg@40: 0.963794\tvalid_0's ndcg@50: 0.963794\n","[17]\tvalid_0's ndcg@10: 0.963635\tvalid_0's ndcg@20: 0.963811\tvalid_0's ndcg@30: 0.963811\tvalid_0's ndcg@40: 0.963811\tvalid_0's ndcg@50: 0.963811\tvalid_0's ndcg@10: 0.963635\tvalid_0's ndcg@20: 0.963811\tvalid_0's ndcg@30: 0.963811\tvalid_0's ndcg@40: 0.963811\tvalid_0's ndcg@50: 0.963811\n","[18]\tvalid_0's ndcg@10: 0.963716\tvalid_0's ndcg@20: 0.963892\tvalid_0's ndcg@30: 0.963892\tvalid_0's ndcg@40: 0.963892\tvalid_0's ndcg@50: 0.963892\tvalid_0's ndcg@10: 0.963716\tvalid_0's ndcg@20: 0.963892\tvalid_0's ndcg@30: 0.963892\tvalid_0's ndcg@40: 0.963892\tvalid_0's ndcg@50: 0.963892\n","[19]\tvalid_0's ndcg@10: 0.963698\tvalid_0's ndcg@20: 0.963872\tvalid_0's ndcg@30: 0.963872\tvalid_0's ndcg@40: 0.963872\tvalid_0's ndcg@50: 0.963872\tvalid_0's ndcg@10: 0.963698\tvalid_0's ndcg@20: 0.963872\tvalid_0's ndcg@30: 0.963872\tvalid_0's ndcg@40: 0.963872\tvalid_0's ndcg@50: 0.963872\n","[20]\tvalid_0's ndcg@10: 0.963491\tvalid_0's ndcg@20: 0.963667\tvalid_0's ndcg@30: 0.963667\tvalid_0's ndcg@40: 0.963667\tvalid_0's ndcg@50: 0.963667\tvalid_0's ndcg@10: 0.963491\tvalid_0's ndcg@20: 0.963667\tvalid_0's ndcg@30: 0.963667\tvalid_0's ndcg@40: 0.963667\tvalid_0's ndcg@50: 0.963667\n","[21]\tvalid_0's ndcg@10: 0.963814\tvalid_0's ndcg@20: 0.96399\tvalid_0's ndcg@30: 0.96399\tvalid_0's ndcg@40: 0.96399\tvalid_0's ndcg@50: 0.96399\tvalid_0's ndcg@10: 0.963814\tvalid_0's ndcg@20: 0.96399\tvalid_0's ndcg@30: 0.96399\tvalid_0's ndcg@40: 0.96399\tvalid_0's ndcg@50: 0.96399\n","[22]\tvalid_0's ndcg@10: 0.963917\tvalid_0's ndcg@20: 0.964139\tvalid_0's ndcg@30: 0.964139\tvalid_0's ndcg@40: 0.964139\tvalid_0's ndcg@50: 0.964139\tvalid_0's ndcg@10: 0.963917\tvalid_0's ndcg@20: 0.964139\tvalid_0's ndcg@30: 0.964139\tvalid_0's ndcg@40: 0.964139\tvalid_0's ndcg@50: 0.964139\n","[23]\tvalid_0's ndcg@10: 0.963841\tvalid_0's ndcg@20: 0.964067\tvalid_0's ndcg@30: 0.964067\tvalid_0's ndcg@40: 0.964067\tvalid_0's ndcg@50: 0.964067\tvalid_0's ndcg@10: 0.963841\tvalid_0's ndcg@20: 0.964067\tvalid_0's ndcg@30: 0.964067\tvalid_0's ndcg@40: 0.964067\tvalid_0's ndcg@50: 0.964067\n","[24]\tvalid_0's ndcg@10: 0.963532\tvalid_0's ndcg@20: 0.963757\tvalid_0's ndcg@30: 0.963757\tvalid_0's ndcg@40: 0.963757\tvalid_0's ndcg@50: 0.963757\tvalid_0's ndcg@10: 0.963532\tvalid_0's ndcg@20: 0.963757\tvalid_0's ndcg@30: 0.963757\tvalid_0's ndcg@40: 0.963757\tvalid_0's ndcg@50: 0.963757\n","[25]\tvalid_0's ndcg@10: 0.963722\tvalid_0's ndcg@20: 0.963901\tvalid_0's ndcg@30: 0.963901\tvalid_0's ndcg@40: 0.963901\tvalid_0's ndcg@50: 0.963901\tvalid_0's ndcg@10: 0.963722\tvalid_0's ndcg@20: 0.963901\tvalid_0's ndcg@30: 0.963901\tvalid_0's ndcg@40: 0.963901\tvalid_0's ndcg@50: 0.963901\n","[26]\tvalid_0's ndcg@10: 0.963569\tvalid_0's ndcg@20: 0.963748\tvalid_0's ndcg@30: 0.963748\tvalid_0's ndcg@40: 0.963748\tvalid_0's ndcg@50: 0.963748\tvalid_0's ndcg@10: 0.963569\tvalid_0's ndcg@20: 0.963748\tvalid_0's ndcg@30: 0.963748\tvalid_0's ndcg@40: 0.963748\tvalid_0's ndcg@50: 0.963748\n","[27]\tvalid_0's ndcg@10: 0.963816\tvalid_0's ndcg@20: 0.963992\tvalid_0's ndcg@30: 0.963992\tvalid_0's ndcg@40: 0.963992\tvalid_0's ndcg@50: 0.963992\tvalid_0's ndcg@10: 0.963816\tvalid_0's ndcg@20: 0.963992\tvalid_0's ndcg@30: 0.963992\tvalid_0's ndcg@40: 0.963992\tvalid_0's ndcg@50: 0.963992\n","[28]\tvalid_0's ndcg@10: 0.963729\tvalid_0's ndcg@20: 0.963906\tvalid_0's ndcg@30: 0.963906\tvalid_0's ndcg@40: 0.963906\tvalid_0's ndcg@50: 0.963906\tvalid_0's ndcg@10: 0.963729\tvalid_0's ndcg@20: 0.963906\tvalid_0's ndcg@30: 0.963906\tvalid_0's ndcg@40: 0.963906\tvalid_0's ndcg@50: 0.963906\n","[29]\tvalid_0's ndcg@10: 0.963489\tvalid_0's ndcg@20: 0.963668\tvalid_0's ndcg@30: 0.963668\tvalid_0's ndcg@40: 0.963668\tvalid_0's ndcg@50: 0.963668\tvalid_0's ndcg@10: 0.963489\tvalid_0's ndcg@20: 0.963668\tvalid_0's ndcg@30: 0.963668\tvalid_0's ndcg@40: 0.963668\tvalid_0's ndcg@50: 0.963668\n","[30]\tvalid_0's ndcg@10: 0.96346\tvalid_0's ndcg@20: 0.963638\tvalid_0's ndcg@30: 0.963638\tvalid_0's ndcg@40: 0.963638\tvalid_0's ndcg@50: 0.963638\tvalid_0's ndcg@10: 0.96346\tvalid_0's ndcg@20: 0.963638\tvalid_0's ndcg@30: 0.963638\tvalid_0's ndcg@40: 0.963638\tvalid_0's ndcg@50: 0.963638\n","[31]\tvalid_0's ndcg@10: 0.963568\tvalid_0's ndcg@20: 0.963745\tvalid_0's ndcg@30: 0.963745\tvalid_0's ndcg@40: 0.963745\tvalid_0's ndcg@50: 0.963745\tvalid_0's ndcg@10: 0.963568\tvalid_0's ndcg@20: 0.963745\tvalid_0's ndcg@30: 0.963745\tvalid_0's ndcg@40: 0.963745\tvalid_0's ndcg@50: 0.963745\n","[32]\tvalid_0's ndcg@10: 0.963494\tvalid_0's ndcg@20: 0.963718\tvalid_0's ndcg@30: 0.963718\tvalid_0's ndcg@40: 0.963718\tvalid_0's ndcg@50: 0.963718\tvalid_0's ndcg@10: 0.963494\tvalid_0's ndcg@20: 0.963718\tvalid_0's ndcg@30: 0.963718\tvalid_0's ndcg@40: 0.963718\tvalid_0's ndcg@50: 0.963718\n","[33]\tvalid_0's ndcg@10: 0.963557\tvalid_0's ndcg@20: 0.963783\tvalid_0's ndcg@30: 0.963783\tvalid_0's ndcg@40: 0.963783\tvalid_0's ndcg@50: 0.963783\tvalid_0's ndcg@10: 0.963557\tvalid_0's ndcg@20: 0.963783\tvalid_0's ndcg@30: 0.963783\tvalid_0's ndcg@40: 0.963783\tvalid_0's ndcg@50: 0.963783\n","[34]\tvalid_0's ndcg@10: 0.963514\tvalid_0's ndcg@20: 0.96374\tvalid_0's ndcg@30: 0.96374\tvalid_0's ndcg@40: 0.96374\tvalid_0's ndcg@50: 0.96374\tvalid_0's ndcg@10: 0.963514\tvalid_0's ndcg@20: 0.96374\tvalid_0's ndcg@30: 0.96374\tvalid_0's ndcg@40: 0.96374\tvalid_0's ndcg@50: 0.96374\n","[35]\tvalid_0's ndcg@10: 0.963462\tvalid_0's ndcg@20: 0.963687\tvalid_0's ndcg@30: 0.963687\tvalid_0's ndcg@40: 0.963687\tvalid_0's ndcg@50: 0.963687\tvalid_0's ndcg@10: 0.963462\tvalid_0's ndcg@20: 0.963687\tvalid_0's ndcg@30: 0.963687\tvalid_0's ndcg@40: 0.963687\tvalid_0's ndcg@50: 0.963687\n","[36]\tvalid_0's ndcg@10: 0.963257\tvalid_0's ndcg@20: 0.963482\tvalid_0's ndcg@30: 0.963482\tvalid_0's ndcg@40: 0.963482\tvalid_0's ndcg@50: 0.963482\tvalid_0's ndcg@10: 0.963257\tvalid_0's ndcg@20: 0.963482\tvalid_0's ndcg@30: 0.963482\tvalid_0's ndcg@40: 0.963482\tvalid_0's ndcg@50: 0.963482\n","[37]\tvalid_0's ndcg@10: 0.963393\tvalid_0's ndcg@20: 0.963571\tvalid_0's ndcg@30: 0.963571\tvalid_0's ndcg@40: 0.963571\tvalid_0's ndcg@50: 0.963571\tvalid_0's ndcg@10: 0.963393\tvalid_0's ndcg@20: 0.963571\tvalid_0's ndcg@30: 0.963571\tvalid_0's ndcg@40: 0.963571\tvalid_0's ndcg@50: 0.963571\n","[38]\tvalid_0's ndcg@10: 0.963378\tvalid_0's ndcg@20: 0.963556\tvalid_0's ndcg@30: 0.963556\tvalid_0's ndcg@40: 0.963556\tvalid_0's ndcg@50: 0.963556\tvalid_0's ndcg@10: 0.963378\tvalid_0's ndcg@20: 0.963556\tvalid_0's ndcg@30: 0.963556\tvalid_0's ndcg@40: 0.963556\tvalid_0's ndcg@50: 0.963556\n","[39]\tvalid_0's ndcg@10: 0.96334\tvalid_0's ndcg@20: 0.963518\tvalid_0's ndcg@30: 0.963518\tvalid_0's ndcg@40: 0.963518\tvalid_0's ndcg@50: 0.963518\tvalid_0's ndcg@10: 0.96334\tvalid_0's ndcg@20: 0.963518\tvalid_0's ndcg@30: 0.963518\tvalid_0's ndcg@40: 0.963518\tvalid_0's ndcg@50: 0.963518\n","[40]\tvalid_0's ndcg@10: 0.963091\tvalid_0's ndcg@20: 0.963269\tvalid_0's ndcg@30: 0.963269\tvalid_0's ndcg@40: 0.963269\tvalid_0's ndcg@50: 0.963269\tvalid_0's ndcg@10: 0.963091\tvalid_0's ndcg@20: 0.963269\tvalid_0's ndcg@30: 0.963269\tvalid_0's ndcg@40: 0.963269\tvalid_0's ndcg@50: 0.963269\n","[41]\tvalid_0's ndcg@10: 0.963145\tvalid_0's ndcg@20: 0.963324\tvalid_0's ndcg@30: 0.963324\tvalid_0's ndcg@40: 0.963324\tvalid_0's ndcg@50: 0.963324\tvalid_0's ndcg@10: 0.963145\tvalid_0's ndcg@20: 0.963324\tvalid_0's ndcg@30: 0.963324\tvalid_0's ndcg@40: 0.963324\tvalid_0's ndcg@50: 0.963324\n","[42]\tvalid_0's ndcg@10: 0.963274\tvalid_0's ndcg@20: 0.963453\tvalid_0's ndcg@30: 0.963453\tvalid_0's ndcg@40: 0.963453\tvalid_0's ndcg@50: 0.963453\tvalid_0's ndcg@10: 0.963274\tvalid_0's ndcg@20: 0.963453\tvalid_0's ndcg@30: 0.963453\tvalid_0's ndcg@40: 0.963453\tvalid_0's ndcg@50: 0.963453\n","[43]\tvalid_0's ndcg@10: 0.963265\tvalid_0's ndcg@20: 0.963444\tvalid_0's ndcg@30: 0.963444\tvalid_0's ndcg@40: 0.963444\tvalid_0's ndcg@50: 0.963444\tvalid_0's ndcg@10: 0.963265\tvalid_0's ndcg@20: 0.963444\tvalid_0's ndcg@30: 0.963444\tvalid_0's ndcg@40: 0.963444\tvalid_0's ndcg@50: 0.963444\n","[44]\tvalid_0's ndcg@10: 0.963399\tvalid_0's ndcg@20: 0.963577\tvalid_0's ndcg@30: 0.963577\tvalid_0's ndcg@40: 0.963577\tvalid_0's ndcg@50: 0.963577\tvalid_0's ndcg@10: 0.963399\tvalid_0's ndcg@20: 0.963577\tvalid_0's ndcg@30: 0.963577\tvalid_0's ndcg@40: 0.963577\tvalid_0's ndcg@50: 0.963577\n","[45]\tvalid_0's ndcg@10: 0.96329\tvalid_0's ndcg@20: 0.963468\tvalid_0's ndcg@30: 0.963468\tvalid_0's ndcg@40: 0.963468\tvalid_0's ndcg@50: 0.963468\tvalid_0's ndcg@10: 0.96329\tvalid_0's ndcg@20: 0.963468\tvalid_0's ndcg@30: 0.963468\tvalid_0's ndcg@40: 0.963468\tvalid_0's ndcg@50: 0.963468\n","[46]\tvalid_0's ndcg@10: 0.963237\tvalid_0's ndcg@20: 0.963415\tvalid_0's ndcg@30: 0.963415\tvalid_0's ndcg@40: 0.963415\tvalid_0's ndcg@50: 0.963415\tvalid_0's ndcg@10: 0.963237\tvalid_0's ndcg@20: 0.963415\tvalid_0's ndcg@30: 0.963415\tvalid_0's ndcg@40: 0.963415\tvalid_0's ndcg@50: 0.963415\n","[47]\tvalid_0's ndcg@10: 0.963313\tvalid_0's ndcg@20: 0.963491\tvalid_0's ndcg@30: 0.963491\tvalid_0's ndcg@40: 0.963491\tvalid_0's ndcg@50: 0.963491\tvalid_0's ndcg@10: 0.963313\tvalid_0's ndcg@20: 0.963491\tvalid_0's ndcg@30: 0.963491\tvalid_0's ndcg@40: 0.963491\tvalid_0's ndcg@50: 0.963491\n","[48]\tvalid_0's ndcg@10: 0.963446\tvalid_0's ndcg@20: 0.963623\tvalid_0's ndcg@30: 0.963623\tvalid_0's ndcg@40: 0.963623\tvalid_0's ndcg@50: 0.963623\tvalid_0's ndcg@10: 0.963446\tvalid_0's ndcg@20: 0.963623\tvalid_0's ndcg@30: 0.963623\tvalid_0's ndcg@40: 0.963623\tvalid_0's ndcg@50: 0.963623\n","[49]\tvalid_0's ndcg@10: 0.963299\tvalid_0's ndcg@20: 0.963476\tvalid_0's ndcg@30: 0.963476\tvalid_0's ndcg@40: 0.963476\tvalid_0's ndcg@50: 0.963476\tvalid_0's ndcg@10: 0.963299\tvalid_0's ndcg@20: 0.963476\tvalid_0's ndcg@30: 0.963476\tvalid_0's ndcg@40: 0.963476\tvalid_0's ndcg@50: 0.963476\n","[50]\tvalid_0's ndcg@10: 0.963433\tvalid_0's ndcg@20: 0.96361\tvalid_0's ndcg@30: 0.96361\tvalid_0's ndcg@40: 0.96361\tvalid_0's ndcg@50: 0.96361\tvalid_0's ndcg@10: 0.963433\tvalid_0's ndcg@20: 0.96361\tvalid_0's ndcg@30: 0.96361\tvalid_0's ndcg@40: 0.96361\tvalid_0's ndcg@50: 0.96361\n","[51]\tvalid_0's ndcg@10: 0.963548\tvalid_0's ndcg@20: 0.963725\tvalid_0's ndcg@30: 0.963725\tvalid_0's ndcg@40: 0.963725\tvalid_0's ndcg@50: 0.963725\tvalid_0's ndcg@10: 0.963548\tvalid_0's ndcg@20: 0.963725\tvalid_0's ndcg@30: 0.963725\tvalid_0's ndcg@40: 0.963725\tvalid_0's ndcg@50: 0.963725\n","[52]\tvalid_0's ndcg@10: 0.963461\tvalid_0's ndcg@20: 0.963639\tvalid_0's ndcg@30: 0.963639\tvalid_0's ndcg@40: 0.963639\tvalid_0's ndcg@50: 0.963639\tvalid_0's ndcg@10: 0.963461\tvalid_0's ndcg@20: 0.963639\tvalid_0's ndcg@30: 0.963639\tvalid_0's ndcg@40: 0.963639\tvalid_0's ndcg@50: 0.963639\n","[53]\tvalid_0's ndcg@10: 0.963599\tvalid_0's ndcg@20: 0.963777\tvalid_0's ndcg@30: 0.963777\tvalid_0's ndcg@40: 0.963777\tvalid_0's ndcg@50: 0.963777\tvalid_0's ndcg@10: 0.963599\tvalid_0's ndcg@20: 0.963777\tvalid_0's ndcg@30: 0.963777\tvalid_0's ndcg@40: 0.963777\tvalid_0's ndcg@50: 0.963777\n","[54]\tvalid_0's ndcg@10: 0.963447\tvalid_0's ndcg@20: 0.963625\tvalid_0's ndcg@30: 0.963625\tvalid_0's ndcg@40: 0.963625\tvalid_0's ndcg@50: 0.963625\tvalid_0's ndcg@10: 0.963447\tvalid_0's ndcg@20: 0.963625\tvalid_0's ndcg@30: 0.963625\tvalid_0's ndcg@40: 0.963625\tvalid_0's ndcg@50: 0.963625\n","[55]\tvalid_0's ndcg@10: 0.963515\tvalid_0's ndcg@20: 0.963693\tvalid_0's ndcg@30: 0.963693\tvalid_0's ndcg@40: 0.963693\tvalid_0's ndcg@50: 0.963693\tvalid_0's ndcg@10: 0.963515\tvalid_0's ndcg@20: 0.963693\tvalid_0's ndcg@30: 0.963693\tvalid_0's ndcg@40: 0.963693\tvalid_0's ndcg@50: 0.963693\n","[56]\tvalid_0's ndcg@10: 0.963485\tvalid_0's ndcg@20: 0.963664\tvalid_0's ndcg@30: 0.963664\tvalid_0's ndcg@40: 0.963664\tvalid_0's ndcg@50: 0.963664\tvalid_0's ndcg@10: 0.963485\tvalid_0's ndcg@20: 0.963664\tvalid_0's ndcg@30: 0.963664\tvalid_0's ndcg@40: 0.963664\tvalid_0's ndcg@50: 0.963664\n","[57]\tvalid_0's ndcg@10: 0.963418\tvalid_0's ndcg@20: 0.963596\tvalid_0's ndcg@30: 0.963596\tvalid_0's ndcg@40: 0.963596\tvalid_0's ndcg@50: 0.963596\tvalid_0's ndcg@10: 0.963418\tvalid_0's ndcg@20: 0.963596\tvalid_0's ndcg@30: 0.963596\tvalid_0's ndcg@40: 0.963596\tvalid_0's ndcg@50: 0.963596\n","[58]\tvalid_0's ndcg@10: 0.963519\tvalid_0's ndcg@20: 0.963696\tvalid_0's ndcg@30: 0.963696\tvalid_0's ndcg@40: 0.963696\tvalid_0's ndcg@50: 0.963696\tvalid_0's ndcg@10: 0.963519\tvalid_0's ndcg@20: 0.963696\tvalid_0's ndcg@30: 0.963696\tvalid_0's ndcg@40: 0.963696\tvalid_0's ndcg@50: 0.963696\n","[59]\tvalid_0's ndcg@10: 0.963725\tvalid_0's ndcg@20: 0.963901\tvalid_0's ndcg@30: 0.963901\tvalid_0's ndcg@40: 0.963901\tvalid_0's ndcg@50: 0.963901\tvalid_0's ndcg@10: 0.963725\tvalid_0's ndcg@20: 0.963901\tvalid_0's ndcg@30: 0.963901\tvalid_0's ndcg@40: 0.963901\tvalid_0's ndcg@50: 0.963901\n","[60]\tvalid_0's ndcg@10: 0.963723\tvalid_0's ndcg@20: 0.963901\tvalid_0's ndcg@30: 0.963901\tvalid_0's ndcg@40: 0.963901\tvalid_0's ndcg@50: 0.963901\tvalid_0's ndcg@10: 0.963723\tvalid_0's ndcg@20: 0.963901\tvalid_0's ndcg@30: 0.963901\tvalid_0's ndcg@40: 0.963901\tvalid_0's ndcg@50: 0.963901\n","[61]\tvalid_0's ndcg@10: 0.96369\tvalid_0's ndcg@20: 0.963868\tvalid_0's ndcg@30: 0.963868\tvalid_0's ndcg@40: 0.963868\tvalid_0's ndcg@50: 0.963868\tvalid_0's ndcg@10: 0.96369\tvalid_0's ndcg@20: 0.963868\tvalid_0's ndcg@30: 0.963868\tvalid_0's ndcg@40: 0.963868\tvalid_0's ndcg@50: 0.963868\n","[62]\tvalid_0's ndcg@10: 0.963604\tvalid_0's ndcg@20: 0.963782\tvalid_0's ndcg@30: 0.963782\tvalid_0's ndcg@40: 0.963782\tvalid_0's ndcg@50: 0.963782\tvalid_0's ndcg@10: 0.963604\tvalid_0's ndcg@20: 0.963782\tvalid_0's ndcg@30: 0.963782\tvalid_0's ndcg@40: 0.963782\tvalid_0's ndcg@50: 0.963782\n","[63]\tvalid_0's ndcg@10: 0.963533\tvalid_0's ndcg@20: 0.963709\tvalid_0's ndcg@30: 0.963709\tvalid_0's ndcg@40: 0.963709\tvalid_0's ndcg@50: 0.963709\tvalid_0's ndcg@10: 0.963533\tvalid_0's ndcg@20: 0.963709\tvalid_0's ndcg@30: 0.963709\tvalid_0's ndcg@40: 0.963709\tvalid_0's ndcg@50: 0.963709\n","[64]\tvalid_0's ndcg@10: 0.963533\tvalid_0's ndcg@20: 0.963709\tvalid_0's ndcg@30: 0.963709\tvalid_0's ndcg@40: 0.963709\tvalid_0's ndcg@50: 0.963709\tvalid_0's ndcg@10: 0.963533\tvalid_0's ndcg@20: 0.963709\tvalid_0's ndcg@30: 0.963709\tvalid_0's ndcg@40: 0.963709\tvalid_0's ndcg@50: 0.963709\n","[65]\tvalid_0's ndcg@10: 0.963772\tvalid_0's ndcg@20: 0.963948\tvalid_0's ndcg@30: 0.963948\tvalid_0's ndcg@40: 0.963948\tvalid_0's ndcg@50: 0.963948\tvalid_0's ndcg@10: 0.963772\tvalid_0's ndcg@20: 0.963948\tvalid_0's ndcg@30: 0.963948\tvalid_0's ndcg@40: 0.963948\tvalid_0's ndcg@50: 0.963948\n","[66]\tvalid_0's ndcg@10: 0.963947\tvalid_0's ndcg@20: 0.964123\tvalid_0's ndcg@30: 0.964123\tvalid_0's ndcg@40: 0.964123\tvalid_0's ndcg@50: 0.964123\tvalid_0's ndcg@10: 0.963947\tvalid_0's ndcg@20: 0.964123\tvalid_0's ndcg@30: 0.964123\tvalid_0's ndcg@40: 0.964123\tvalid_0's ndcg@50: 0.964123\n","[67]\tvalid_0's ndcg@10: 0.963694\tvalid_0's ndcg@20: 0.96387\tvalid_0's ndcg@30: 0.96387\tvalid_0's ndcg@40: 0.96387\tvalid_0's ndcg@50: 0.96387\tvalid_0's ndcg@10: 0.963694\tvalid_0's ndcg@20: 0.96387\tvalid_0's ndcg@30: 0.96387\tvalid_0's ndcg@40: 0.96387\tvalid_0's ndcg@50: 0.96387\n","[68]\tvalid_0's ndcg@10: 0.963599\tvalid_0's ndcg@20: 0.963775\tvalid_0's ndcg@30: 0.963775\tvalid_0's ndcg@40: 0.963775\tvalid_0's ndcg@50: 0.963775\tvalid_0's ndcg@10: 0.963599\tvalid_0's ndcg@20: 0.963775\tvalid_0's ndcg@30: 0.963775\tvalid_0's ndcg@40: 0.963775\tvalid_0's ndcg@50: 0.963775\n","[69]\tvalid_0's ndcg@10: 0.963586\tvalid_0's ndcg@20: 0.963762\tvalid_0's ndcg@30: 0.963762\tvalid_0's ndcg@40: 0.963762\tvalid_0's ndcg@50: 0.963762\tvalid_0's ndcg@10: 0.963586\tvalid_0's ndcg@20: 0.963762\tvalid_0's ndcg@30: 0.963762\tvalid_0's ndcg@40: 0.963762\tvalid_0's ndcg@50: 0.963762\n","[70]\tvalid_0's ndcg@10: 0.963513\tvalid_0's ndcg@20: 0.963689\tvalid_0's ndcg@30: 0.963689\tvalid_0's ndcg@40: 0.963689\tvalid_0's ndcg@50: 0.963689\tvalid_0's ndcg@10: 0.963513\tvalid_0's ndcg@20: 0.963689\tvalid_0's ndcg@30: 0.963689\tvalid_0's ndcg@40: 0.963689\tvalid_0's ndcg@50: 0.963689\n","[71]\tvalid_0's ndcg@10: 0.963576\tvalid_0's ndcg@20: 0.963752\tvalid_0's ndcg@30: 0.963752\tvalid_0's ndcg@40: 0.963752\tvalid_0's ndcg@50: 0.963752\tvalid_0's ndcg@10: 0.963576\tvalid_0's ndcg@20: 0.963752\tvalid_0's ndcg@30: 0.963752\tvalid_0's ndcg@40: 0.963752\tvalid_0's ndcg@50: 0.963752\n","[72]\tvalid_0's ndcg@10: 0.963627\tvalid_0's ndcg@20: 0.963803\tvalid_0's ndcg@30: 0.963803\tvalid_0's ndcg@40: 0.963803\tvalid_0's ndcg@50: 0.963803\tvalid_0's ndcg@10: 0.963627\tvalid_0's ndcg@20: 0.963803\tvalid_0's ndcg@30: 0.963803\tvalid_0's ndcg@40: 0.963803\tvalid_0's ndcg@50: 0.963803\n","Early stopping, best iteration is:\n","[22]\tvalid_0's ndcg@10: 0.963917\tvalid_0's ndcg@20: 0.964139\tvalid_0's ndcg@30: 0.964139\tvalid_0's ndcg@40: 0.964139\tvalid_0's ndcg@50: 0.964139\tvalid_0's ndcg@10: 0.963917\tvalid_0's ndcg@20: 0.964139\tvalid_0's ndcg@30: 0.964139\tvalid_0's ndcg@40: 0.964139\tvalid_0's ndcg@50: 0.964139\n","[1]\tvalid_0's ndcg@10: 0.959335\tvalid_0's ndcg@20: 0.959555\tvalid_0's ndcg@30: 0.959592\tvalid_0's ndcg@40: 0.959592\tvalid_0's ndcg@50: 0.959592\tvalid_0's ndcg@10: 0.959335\tvalid_0's ndcg@20: 0.959555\tvalid_0's ndcg@30: 0.959592\tvalid_0's ndcg@40: 0.959592\tvalid_0's ndcg@50: 0.959592\n","Training until validation scores don't improve for 50 rounds.\n","[2]\tvalid_0's ndcg@10: 0.960525\tvalid_0's ndcg@20: 0.960705\tvalid_0's ndcg@30: 0.960705\tvalid_0's ndcg@40: 0.960705\tvalid_0's ndcg@50: 0.960705\tvalid_0's ndcg@10: 0.960525\tvalid_0's ndcg@20: 0.960705\tvalid_0's ndcg@30: 0.960705\tvalid_0's ndcg@40: 0.960705\tvalid_0's ndcg@50: 0.960705\n","[3]\tvalid_0's ndcg@10: 0.961651\tvalid_0's ndcg@20: 0.961784\tvalid_0's ndcg@30: 0.961784\tvalid_0's ndcg@40: 0.961784\tvalid_0's ndcg@50: 0.961784\tvalid_0's ndcg@10: 0.961651\tvalid_0's ndcg@20: 0.961784\tvalid_0's ndcg@30: 0.961784\tvalid_0's ndcg@40: 0.961784\tvalid_0's ndcg@50: 0.961784\n","[4]\tvalid_0's ndcg@10: 0.962156\tvalid_0's ndcg@20: 0.962288\tvalid_0's ndcg@30: 0.962288\tvalid_0's ndcg@40: 0.962288\tvalid_0's ndcg@50: 0.962288\tvalid_0's ndcg@10: 0.962156\tvalid_0's ndcg@20: 0.962288\tvalid_0's ndcg@30: 0.962288\tvalid_0's ndcg@40: 0.962288\tvalid_0's ndcg@50: 0.962288\n","[5]\tvalid_0's ndcg@10: 0.962393\tvalid_0's ndcg@20: 0.962574\tvalid_0's ndcg@30: 0.962574\tvalid_0's ndcg@40: 0.962574\tvalid_0's ndcg@50: 0.962574\tvalid_0's ndcg@10: 0.962393\tvalid_0's ndcg@20: 0.962574\tvalid_0's ndcg@30: 0.962574\tvalid_0's ndcg@40: 0.962574\tvalid_0's ndcg@50: 0.962574\n","[6]\tvalid_0's ndcg@10: 0.961648\tvalid_0's ndcg@20: 0.961829\tvalid_0's ndcg@30: 0.961829\tvalid_0's ndcg@40: 0.961829\tvalid_0's ndcg@50: 0.961829\tvalid_0's ndcg@10: 0.961648\tvalid_0's ndcg@20: 0.961829\tvalid_0's ndcg@30: 0.961829\tvalid_0's ndcg@40: 0.961829\tvalid_0's ndcg@50: 0.961829\n","[7]\tvalid_0's ndcg@10: 0.961699\tvalid_0's ndcg@20: 0.961882\tvalid_0's ndcg@30: 0.961882\tvalid_0's ndcg@40: 0.961882\tvalid_0's ndcg@50: 0.961882\tvalid_0's ndcg@10: 0.961699\tvalid_0's ndcg@20: 0.961882\tvalid_0's ndcg@30: 0.961882\tvalid_0's ndcg@40: 0.961882\tvalid_0's ndcg@50: 0.961882\n","[8]\tvalid_0's ndcg@10: 0.961561\tvalid_0's ndcg@20: 0.961697\tvalid_0's ndcg@30: 0.961697\tvalid_0's ndcg@40: 0.961697\tvalid_0's ndcg@50: 0.961697\tvalid_0's ndcg@10: 0.961561\tvalid_0's ndcg@20: 0.961697\tvalid_0's ndcg@30: 0.961697\tvalid_0's ndcg@40: 0.961697\tvalid_0's ndcg@50: 0.961697\n","[9]\tvalid_0's ndcg@10: 0.96257\tvalid_0's ndcg@20: 0.962708\tvalid_0's ndcg@30: 0.962708\tvalid_0's ndcg@40: 0.962708\tvalid_0's ndcg@50: 0.962708\tvalid_0's ndcg@10: 0.96257\tvalid_0's ndcg@20: 0.962708\tvalid_0's ndcg@30: 0.962708\tvalid_0's ndcg@40: 0.962708\tvalid_0's ndcg@50: 0.962708\n","[10]\tvalid_0's ndcg@10: 0.963005\tvalid_0's ndcg@20: 0.963142\tvalid_0's ndcg@30: 0.963142\tvalid_0's ndcg@40: 0.963142\tvalid_0's ndcg@50: 0.963142\tvalid_0's ndcg@10: 0.963005\tvalid_0's ndcg@20: 0.963142\tvalid_0's ndcg@30: 0.963142\tvalid_0's ndcg@40: 0.963142\tvalid_0's ndcg@50: 0.963142\n","[11]\tvalid_0's ndcg@10: 0.962695\tvalid_0's ndcg@20: 0.962832\tvalid_0's ndcg@30: 0.962832\tvalid_0's ndcg@40: 0.962832\tvalid_0's ndcg@50: 0.962832\tvalid_0's ndcg@10: 0.962695\tvalid_0's ndcg@20: 0.962832\tvalid_0's ndcg@30: 0.962832\tvalid_0's ndcg@40: 0.962832\tvalid_0's ndcg@50: 0.962832\n","[12]\tvalid_0's ndcg@10: 0.963043\tvalid_0's ndcg@20: 0.963133\tvalid_0's ndcg@30: 0.963133\tvalid_0's ndcg@40: 0.963133\tvalid_0's ndcg@50: 0.963133\tvalid_0's ndcg@10: 0.963043\tvalid_0's ndcg@20: 0.963133\tvalid_0's ndcg@30: 0.963133\tvalid_0's ndcg@40: 0.963133\tvalid_0's ndcg@50: 0.963133\n","[13]\tvalid_0's ndcg@10: 0.962623\tvalid_0's ndcg@20: 0.962761\tvalid_0's ndcg@30: 0.962761\tvalid_0's ndcg@40: 0.962761\tvalid_0's ndcg@50: 0.962761\tvalid_0's ndcg@10: 0.962623\tvalid_0's ndcg@20: 0.962761\tvalid_0's ndcg@30: 0.962761\tvalid_0's ndcg@40: 0.962761\tvalid_0's ndcg@50: 0.962761\n","[14]\tvalid_0's ndcg@10: 0.963077\tvalid_0's ndcg@20: 0.96312\tvalid_0's ndcg@30: 0.96312\tvalid_0's ndcg@40: 0.96312\tvalid_0's ndcg@50: 0.96312\tvalid_0's ndcg@10: 0.963077\tvalid_0's ndcg@20: 0.96312\tvalid_0's ndcg@30: 0.96312\tvalid_0's ndcg@40: 0.96312\tvalid_0's ndcg@50: 0.96312\n","[15]\tvalid_0's ndcg@10: 0.96291\tvalid_0's ndcg@20: 0.962952\tvalid_0's ndcg@30: 0.962952\tvalid_0's ndcg@40: 0.962952\tvalid_0's ndcg@50: 0.962952\tvalid_0's ndcg@10: 0.96291\tvalid_0's ndcg@20: 0.962952\tvalid_0's ndcg@30: 0.962952\tvalid_0's ndcg@40: 0.962952\tvalid_0's ndcg@50: 0.962952\n","[16]\tvalid_0's ndcg@10: 0.96294\tvalid_0's ndcg@20: 0.962984\tvalid_0's ndcg@30: 0.962984\tvalid_0's ndcg@40: 0.962984\tvalid_0's ndcg@50: 0.962984\tvalid_0's ndcg@10: 0.96294\tvalid_0's ndcg@20: 0.962984\tvalid_0's ndcg@30: 0.962984\tvalid_0's ndcg@40: 0.962984\tvalid_0's ndcg@50: 0.962984\n","[17]\tvalid_0's ndcg@10: 0.962786\tvalid_0's ndcg@20: 0.96283\tvalid_0's ndcg@30: 0.96283\tvalid_0's ndcg@40: 0.96283\tvalid_0's ndcg@50: 0.96283\tvalid_0's ndcg@10: 0.962786\tvalid_0's ndcg@20: 0.96283\tvalid_0's ndcg@30: 0.96283\tvalid_0's ndcg@40: 0.96283\tvalid_0's ndcg@50: 0.96283\n","[18]\tvalid_0's ndcg@10: 0.963085\tvalid_0's ndcg@20: 0.963127\tvalid_0's ndcg@30: 0.963127\tvalid_0's ndcg@40: 0.963127\tvalid_0's ndcg@50: 0.963127\tvalid_0's ndcg@10: 0.963085\tvalid_0's ndcg@20: 0.963127\tvalid_0's ndcg@30: 0.963127\tvalid_0's ndcg@40: 0.963127\tvalid_0's ndcg@50: 0.963127\n","[19]\tvalid_0's ndcg@10: 0.962965\tvalid_0's ndcg@20: 0.963007\tvalid_0's ndcg@30: 0.963007\tvalid_0's ndcg@40: 0.963007\tvalid_0's ndcg@50: 0.963007\tvalid_0's ndcg@10: 0.962965\tvalid_0's ndcg@20: 0.963007\tvalid_0's ndcg@30: 0.963007\tvalid_0's ndcg@40: 0.963007\tvalid_0's ndcg@50: 0.963007\n","[20]\tvalid_0's ndcg@10: 0.9632\tvalid_0's ndcg@20: 0.963288\tvalid_0's ndcg@30: 0.963288\tvalid_0's ndcg@40: 0.963288\tvalid_0's ndcg@50: 0.963288\tvalid_0's ndcg@10: 0.9632\tvalid_0's ndcg@20: 0.963288\tvalid_0's ndcg@30: 0.963288\tvalid_0's ndcg@40: 0.963288\tvalid_0's ndcg@50: 0.963288\n","[21]\tvalid_0's ndcg@10: 0.963042\tvalid_0's ndcg@20: 0.963129\tvalid_0's ndcg@30: 0.963129\tvalid_0's ndcg@40: 0.963129\tvalid_0's ndcg@50: 0.963129\tvalid_0's ndcg@10: 0.963042\tvalid_0's ndcg@20: 0.963129\tvalid_0's ndcg@30: 0.963129\tvalid_0's ndcg@40: 0.963129\tvalid_0's ndcg@50: 0.963129\n","[22]\tvalid_0's ndcg@10: 0.96327\tvalid_0's ndcg@20: 0.963358\tvalid_0's ndcg@30: 0.963358\tvalid_0's ndcg@40: 0.963358\tvalid_0's ndcg@50: 0.963358\tvalid_0's ndcg@10: 0.96327\tvalid_0's ndcg@20: 0.963358\tvalid_0's ndcg@30: 0.963358\tvalid_0's ndcg@40: 0.963358\tvalid_0's ndcg@50: 0.963358\n","[23]\tvalid_0's ndcg@10: 0.96314\tvalid_0's ndcg@20: 0.96323\tvalid_0's ndcg@30: 0.96323\tvalid_0's ndcg@40: 0.96323\tvalid_0's ndcg@50: 0.96323\tvalid_0's ndcg@10: 0.96314\tvalid_0's ndcg@20: 0.96323\tvalid_0's ndcg@30: 0.96323\tvalid_0's ndcg@40: 0.96323\tvalid_0's ndcg@50: 0.96323\n","[24]\tvalid_0's ndcg@10: 0.963132\tvalid_0's ndcg@20: 0.963222\tvalid_0's ndcg@30: 0.963222\tvalid_0's ndcg@40: 0.963222\tvalid_0's ndcg@50: 0.963222\tvalid_0's ndcg@10: 0.963132\tvalid_0's ndcg@20: 0.963222\tvalid_0's ndcg@30: 0.963222\tvalid_0's ndcg@40: 0.963222\tvalid_0's ndcg@50: 0.963222\n","[25]\tvalid_0's ndcg@10: 0.963487\tvalid_0's ndcg@20: 0.96353\tvalid_0's ndcg@30: 0.96353\tvalid_0's ndcg@40: 0.96353\tvalid_0's ndcg@50: 0.96353\tvalid_0's ndcg@10: 0.963487\tvalid_0's ndcg@20: 0.96353\tvalid_0's ndcg@30: 0.96353\tvalid_0's ndcg@40: 0.96353\tvalid_0's ndcg@50: 0.96353\n","[26]\tvalid_0's ndcg@10: 0.963467\tvalid_0's ndcg@20: 0.96351\tvalid_0's ndcg@30: 0.96351\tvalid_0's ndcg@40: 0.96351\tvalid_0's ndcg@50: 0.96351\tvalid_0's ndcg@10: 0.963467\tvalid_0's ndcg@20: 0.96351\tvalid_0's ndcg@30: 0.96351\tvalid_0's ndcg@40: 0.96351\tvalid_0's ndcg@50: 0.96351\n","[27]\tvalid_0's ndcg@10: 0.963473\tvalid_0's ndcg@20: 0.963515\tvalid_0's ndcg@30: 0.963515\tvalid_0's ndcg@40: 0.963515\tvalid_0's ndcg@50: 0.963515\tvalid_0's ndcg@10: 0.963473\tvalid_0's ndcg@20: 0.963515\tvalid_0's ndcg@30: 0.963515\tvalid_0's ndcg@40: 0.963515\tvalid_0's ndcg@50: 0.963515\n","[28]\tvalid_0's ndcg@10: 0.963444\tvalid_0's ndcg@20: 0.963534\tvalid_0's ndcg@30: 0.963534\tvalid_0's ndcg@40: 0.963534\tvalid_0's ndcg@50: 0.963534\tvalid_0's ndcg@10: 0.963444\tvalid_0's ndcg@20: 0.963534\tvalid_0's ndcg@30: 0.963534\tvalid_0's ndcg@40: 0.963534\tvalid_0's ndcg@50: 0.963534\n","[29]\tvalid_0's ndcg@10: 0.963492\tvalid_0's ndcg@20: 0.963534\tvalid_0's ndcg@30: 0.963534\tvalid_0's ndcg@40: 0.963534\tvalid_0's ndcg@50: 0.963534\tvalid_0's ndcg@10: 0.963492\tvalid_0's ndcg@20: 0.963534\tvalid_0's ndcg@30: 0.963534\tvalid_0's ndcg@40: 0.963534\tvalid_0's ndcg@50: 0.963534\n","[30]\tvalid_0's ndcg@10: 0.963494\tvalid_0's ndcg@20: 0.963584\tvalid_0's ndcg@30: 0.963584\tvalid_0's ndcg@40: 0.963584\tvalid_0's ndcg@50: 0.963584\tvalid_0's ndcg@10: 0.963494\tvalid_0's ndcg@20: 0.963584\tvalid_0's ndcg@30: 0.963584\tvalid_0's ndcg@40: 0.963584\tvalid_0's ndcg@50: 0.963584\n","[31]\tvalid_0's ndcg@10: 0.963371\tvalid_0's ndcg@20: 0.963415\tvalid_0's ndcg@30: 0.963415\tvalid_0's ndcg@40: 0.963415\tvalid_0's ndcg@50: 0.963415\tvalid_0's ndcg@10: 0.963371\tvalid_0's ndcg@20: 0.963415\tvalid_0's ndcg@30: 0.963415\tvalid_0's ndcg@40: 0.963415\tvalid_0's ndcg@50: 0.963415\n","[32]\tvalid_0's ndcg@10: 0.963313\tvalid_0's ndcg@20: 0.963404\tvalid_0's ndcg@30: 0.963404\tvalid_0's ndcg@40: 0.963404\tvalid_0's ndcg@50: 0.963404\tvalid_0's ndcg@10: 0.963313\tvalid_0's ndcg@20: 0.963404\tvalid_0's ndcg@30: 0.963404\tvalid_0's ndcg@40: 0.963404\tvalid_0's ndcg@50: 0.963404\n","[33]\tvalid_0's ndcg@10: 0.963435\tvalid_0's ndcg@20: 0.963526\tvalid_0's ndcg@30: 0.963526\tvalid_0's ndcg@40: 0.963526\tvalid_0's ndcg@50: 0.963526\tvalid_0's ndcg@10: 0.963435\tvalid_0's ndcg@20: 0.963526\tvalid_0's ndcg@30: 0.963526\tvalid_0's ndcg@40: 0.963526\tvalid_0's ndcg@50: 0.963526\n","[34]\tvalid_0's ndcg@10: 0.963623\tvalid_0's ndcg@20: 0.963666\tvalid_0's ndcg@30: 0.963666\tvalid_0's ndcg@40: 0.963666\tvalid_0's ndcg@50: 0.963666\tvalid_0's ndcg@10: 0.963623\tvalid_0's ndcg@20: 0.963666\tvalid_0's ndcg@30: 0.963666\tvalid_0's ndcg@40: 0.963666\tvalid_0's ndcg@50: 0.963666\n","[35]\tvalid_0's ndcg@10: 0.963537\tvalid_0's ndcg@20: 0.963628\tvalid_0's ndcg@30: 0.963628\tvalid_0's ndcg@40: 0.963628\tvalid_0's ndcg@50: 0.963628\tvalid_0's ndcg@10: 0.963537\tvalid_0's ndcg@20: 0.963628\tvalid_0's ndcg@30: 0.963628\tvalid_0's ndcg@40: 0.963628\tvalid_0's ndcg@50: 0.963628\n","[36]\tvalid_0's ndcg@10: 0.963384\tvalid_0's ndcg@20: 0.963475\tvalid_0's ndcg@30: 0.963475\tvalid_0's ndcg@40: 0.963475\tvalid_0's ndcg@50: 0.963475\tvalid_0's ndcg@10: 0.963384\tvalid_0's ndcg@20: 0.963475\tvalid_0's ndcg@30: 0.963475\tvalid_0's ndcg@40: 0.963475\tvalid_0's ndcg@50: 0.963475\n","[37]\tvalid_0's ndcg@10: 0.963431\tvalid_0's ndcg@20: 0.963522\tvalid_0's ndcg@30: 0.963522\tvalid_0's ndcg@40: 0.963522\tvalid_0's ndcg@50: 0.963522\tvalid_0's ndcg@10: 0.963431\tvalid_0's ndcg@20: 0.963522\tvalid_0's ndcg@30: 0.963522\tvalid_0's ndcg@40: 0.963522\tvalid_0's ndcg@50: 0.963522\n","[38]\tvalid_0's ndcg@10: 0.96313\tvalid_0's ndcg@20: 0.963268\tvalid_0's ndcg@30: 0.963268\tvalid_0's ndcg@40: 0.963268\tvalid_0's ndcg@50: 0.963268\tvalid_0's ndcg@10: 0.96313\tvalid_0's ndcg@20: 0.963268\tvalid_0's ndcg@30: 0.963268\tvalid_0's ndcg@40: 0.963268\tvalid_0's ndcg@50: 0.963268\n","[39]\tvalid_0's ndcg@10: 0.963402\tvalid_0's ndcg@20: 0.963493\tvalid_0's ndcg@30: 0.963493\tvalid_0's ndcg@40: 0.963493\tvalid_0's ndcg@50: 0.963493\tvalid_0's ndcg@10: 0.963402\tvalid_0's ndcg@20: 0.963493\tvalid_0's ndcg@30: 0.963493\tvalid_0's ndcg@40: 0.963493\tvalid_0's ndcg@50: 0.963493\n","[40]\tvalid_0's ndcg@10: 0.963356\tvalid_0's ndcg@20: 0.963447\tvalid_0's ndcg@30: 0.963447\tvalid_0's ndcg@40: 0.963447\tvalid_0's ndcg@50: 0.963447\tvalid_0's ndcg@10: 0.963356\tvalid_0's ndcg@20: 0.963447\tvalid_0's ndcg@30: 0.963447\tvalid_0's ndcg@40: 0.963447\tvalid_0's ndcg@50: 0.963447\n","[41]\tvalid_0's ndcg@10: 0.963368\tvalid_0's ndcg@20: 0.963411\tvalid_0's ndcg@30: 0.963411\tvalid_0's ndcg@40: 0.963411\tvalid_0's ndcg@50: 0.963411\tvalid_0's ndcg@10: 0.963368\tvalid_0's ndcg@20: 0.963411\tvalid_0's ndcg@30: 0.963411\tvalid_0's ndcg@40: 0.963411\tvalid_0's ndcg@50: 0.963411\n","[42]\tvalid_0's ndcg@10: 0.963257\tvalid_0's ndcg@20: 0.963347\tvalid_0's ndcg@30: 0.963347\tvalid_0's ndcg@40: 0.963347\tvalid_0's ndcg@50: 0.963347\tvalid_0's ndcg@10: 0.963257\tvalid_0's ndcg@20: 0.963347\tvalid_0's ndcg@30: 0.963347\tvalid_0's ndcg@40: 0.963347\tvalid_0's ndcg@50: 0.963347\n","[43]\tvalid_0's ndcg@10: 0.96322\tvalid_0's ndcg@20: 0.96331\tvalid_0's ndcg@30: 0.96331\tvalid_0's ndcg@40: 0.96331\tvalid_0's ndcg@50: 0.96331\tvalid_0's ndcg@10: 0.96322\tvalid_0's ndcg@20: 0.96331\tvalid_0's ndcg@30: 0.96331\tvalid_0's ndcg@40: 0.96331\tvalid_0's ndcg@50: 0.96331\n","[44]\tvalid_0's ndcg@10: 0.963201\tvalid_0's ndcg@20: 0.963339\tvalid_0's ndcg@30: 0.963339\tvalid_0's ndcg@40: 0.963339\tvalid_0's ndcg@50: 0.963339\tvalid_0's ndcg@10: 0.963201\tvalid_0's ndcg@20: 0.963339\tvalid_0's ndcg@30: 0.963339\tvalid_0's ndcg@40: 0.963339\tvalid_0's ndcg@50: 0.963339\n","[45]\tvalid_0's ndcg@10: 0.963559\tvalid_0's ndcg@20: 0.963697\tvalid_0's ndcg@30: 0.963697\tvalid_0's ndcg@40: 0.963697\tvalid_0's ndcg@50: 0.963697\tvalid_0's ndcg@10: 0.963559\tvalid_0's ndcg@20: 0.963697\tvalid_0's ndcg@30: 0.963697\tvalid_0's ndcg@40: 0.963697\tvalid_0's ndcg@50: 0.963697\n","[46]\tvalid_0's ndcg@10: 0.963513\tvalid_0's ndcg@20: 0.963604\tvalid_0's ndcg@30: 0.963604\tvalid_0's ndcg@40: 0.963604\tvalid_0's ndcg@50: 0.963604\tvalid_0's ndcg@10: 0.963513\tvalid_0's ndcg@20: 0.963604\tvalid_0's ndcg@30: 0.963604\tvalid_0's ndcg@40: 0.963604\tvalid_0's ndcg@50: 0.963604\n","[47]\tvalid_0's ndcg@10: 0.963465\tvalid_0's ndcg@20: 0.963603\tvalid_0's ndcg@30: 0.963603\tvalid_0's ndcg@40: 0.963603\tvalid_0's ndcg@50: 0.963603\tvalid_0's ndcg@10: 0.963465\tvalid_0's ndcg@20: 0.963603\tvalid_0's ndcg@30: 0.963603\tvalid_0's ndcg@40: 0.963603\tvalid_0's ndcg@50: 0.963603\n","[48]\tvalid_0's ndcg@10: 0.96369\tvalid_0's ndcg@20: 0.963781\tvalid_0's ndcg@30: 0.963781\tvalid_0's ndcg@40: 0.963781\tvalid_0's ndcg@50: 0.963781\tvalid_0's ndcg@10: 0.96369\tvalid_0's ndcg@20: 0.963781\tvalid_0's ndcg@30: 0.963781\tvalid_0's ndcg@40: 0.963781\tvalid_0's ndcg@50: 0.963781\n","[49]\tvalid_0's ndcg@10: 0.963564\tvalid_0's ndcg@20: 0.963654\tvalid_0's ndcg@30: 0.963654\tvalid_0's ndcg@40: 0.963654\tvalid_0's ndcg@50: 0.963654\tvalid_0's ndcg@10: 0.963564\tvalid_0's ndcg@20: 0.963654\tvalid_0's ndcg@30: 0.963654\tvalid_0's ndcg@40: 0.963654\tvalid_0's ndcg@50: 0.963654\n","[50]\tvalid_0's ndcg@10: 0.963607\tvalid_0's ndcg@20: 0.963697\tvalid_0's ndcg@30: 0.963697\tvalid_0's ndcg@40: 0.963697\tvalid_0's ndcg@50: 0.963697\tvalid_0's ndcg@10: 0.963607\tvalid_0's ndcg@20: 0.963697\tvalid_0's ndcg@30: 0.963697\tvalid_0's ndcg@40: 0.963697\tvalid_0's ndcg@50: 0.963697\n","[51]\tvalid_0's ndcg@10: 0.963761\tvalid_0's ndcg@20: 0.963851\tvalid_0's ndcg@30: 0.963851\tvalid_0's ndcg@40: 0.963851\tvalid_0's ndcg@50: 0.963851\tvalid_0's ndcg@10: 0.963761\tvalid_0's ndcg@20: 0.963851\tvalid_0's ndcg@30: 0.963851\tvalid_0's ndcg@40: 0.963851\tvalid_0's ndcg@50: 0.963851\n","[52]\tvalid_0's ndcg@10: 0.963673\tvalid_0's ndcg@20: 0.963763\tvalid_0's ndcg@30: 0.963763\tvalid_0's ndcg@40: 0.963763\tvalid_0's ndcg@50: 0.963763\tvalid_0's ndcg@10: 0.963673\tvalid_0's ndcg@20: 0.963763\tvalid_0's ndcg@30: 0.963763\tvalid_0's ndcg@40: 0.963763\tvalid_0's ndcg@50: 0.963763\n","[53]\tvalid_0's ndcg@10: 0.963723\tvalid_0's ndcg@20: 0.963812\tvalid_0's ndcg@30: 0.963812\tvalid_0's ndcg@40: 0.963812\tvalid_0's ndcg@50: 0.963812\tvalid_0's ndcg@10: 0.963723\tvalid_0's ndcg@20: 0.963812\tvalid_0's ndcg@30: 0.963812\tvalid_0's ndcg@40: 0.963812\tvalid_0's ndcg@50: 0.963812\n","[54]\tvalid_0's ndcg@10: 0.963705\tvalid_0's ndcg@20: 0.963795\tvalid_0's ndcg@30: 0.963795\tvalid_0's ndcg@40: 0.963795\tvalid_0's ndcg@50: 0.963795\tvalid_0's ndcg@10: 0.963705\tvalid_0's ndcg@20: 0.963795\tvalid_0's ndcg@30: 0.963795\tvalid_0's ndcg@40: 0.963795\tvalid_0's ndcg@50: 0.963795\n","[55]\tvalid_0's ndcg@10: 0.963724\tvalid_0's ndcg@20: 0.963814\tvalid_0's ndcg@30: 0.963814\tvalid_0's ndcg@40: 0.963814\tvalid_0's ndcg@50: 0.963814\tvalid_0's ndcg@10: 0.963724\tvalid_0's ndcg@20: 0.963814\tvalid_0's ndcg@30: 0.963814\tvalid_0's ndcg@40: 0.963814\tvalid_0's ndcg@50: 0.963814\n","[56]\tvalid_0's ndcg@10: 0.963773\tvalid_0's ndcg@20: 0.963863\tvalid_0's ndcg@30: 0.963863\tvalid_0's ndcg@40: 0.963863\tvalid_0's ndcg@50: 0.963863\tvalid_0's ndcg@10: 0.963773\tvalid_0's ndcg@20: 0.963863\tvalid_0's ndcg@30: 0.963863\tvalid_0's ndcg@40: 0.963863\tvalid_0's ndcg@50: 0.963863\n","[57]\tvalid_0's ndcg@10: 0.963799\tvalid_0's ndcg@20: 0.96389\tvalid_0's ndcg@30: 0.96389\tvalid_0's ndcg@40: 0.96389\tvalid_0's ndcg@50: 0.96389\tvalid_0's ndcg@10: 0.963799\tvalid_0's ndcg@20: 0.96389\tvalid_0's ndcg@30: 0.96389\tvalid_0's ndcg@40: 0.96389\tvalid_0's ndcg@50: 0.96389\n","[58]\tvalid_0's ndcg@10: 0.963503\tvalid_0's ndcg@20: 0.963641\tvalid_0's ndcg@30: 0.963641\tvalid_0's ndcg@40: 0.963641\tvalid_0's ndcg@50: 0.963641\tvalid_0's ndcg@10: 0.963503\tvalid_0's ndcg@20: 0.963641\tvalid_0's ndcg@30: 0.963641\tvalid_0's ndcg@40: 0.963641\tvalid_0's ndcg@50: 0.963641\n","[59]\tvalid_0's ndcg@10: 0.963796\tvalid_0's ndcg@20: 0.963933\tvalid_0's ndcg@30: 0.963933\tvalid_0's ndcg@40: 0.963933\tvalid_0's ndcg@50: 0.963933\tvalid_0's ndcg@10: 0.963796\tvalid_0's ndcg@20: 0.963933\tvalid_0's ndcg@30: 0.963933\tvalid_0's ndcg@40: 0.963933\tvalid_0's ndcg@50: 0.963933\n","[60]\tvalid_0's ndcg@10: 0.963745\tvalid_0's ndcg@20: 0.963882\tvalid_0's ndcg@30: 0.963882\tvalid_0's ndcg@40: 0.963882\tvalid_0's ndcg@50: 0.963882\tvalid_0's ndcg@10: 0.963745\tvalid_0's ndcg@20: 0.963882\tvalid_0's ndcg@30: 0.963882\tvalid_0's ndcg@40: 0.963882\tvalid_0's ndcg@50: 0.963882\n","[61]\tvalid_0's ndcg@10: 0.963752\tvalid_0's ndcg@20: 0.96389\tvalid_0's ndcg@30: 0.96389\tvalid_0's ndcg@40: 0.96389\tvalid_0's ndcg@50: 0.96389\tvalid_0's ndcg@10: 0.963752\tvalid_0's ndcg@20: 0.96389\tvalid_0's ndcg@30: 0.96389\tvalid_0's ndcg@40: 0.96389\tvalid_0's ndcg@50: 0.96389\n","[62]\tvalid_0's ndcg@10: 0.963698\tvalid_0's ndcg@20: 0.963835\tvalid_0's ndcg@30: 0.963835\tvalid_0's ndcg@40: 0.963835\tvalid_0's ndcg@50: 0.963835\tvalid_0's ndcg@10: 0.963698\tvalid_0's ndcg@20: 0.963835\tvalid_0's ndcg@30: 0.963835\tvalid_0's ndcg@40: 0.963835\tvalid_0's ndcg@50: 0.963835\n","[63]\tvalid_0's ndcg@10: 0.963553\tvalid_0's ndcg@20: 0.96369\tvalid_0's ndcg@30: 0.96369\tvalid_0's ndcg@40: 0.96369\tvalid_0's ndcg@50: 0.96369\tvalid_0's ndcg@10: 0.963553\tvalid_0's ndcg@20: 0.96369\tvalid_0's ndcg@30: 0.96369\tvalid_0's ndcg@40: 0.96369\tvalid_0's ndcg@50: 0.96369\n","[64]\tvalid_0's ndcg@10: 0.963584\tvalid_0's ndcg@20: 0.963721\tvalid_0's ndcg@30: 0.963721\tvalid_0's ndcg@40: 0.963721\tvalid_0's ndcg@50: 0.963721\tvalid_0's ndcg@10: 0.963584\tvalid_0's ndcg@20: 0.963721\tvalid_0's ndcg@30: 0.963721\tvalid_0's ndcg@40: 0.963721\tvalid_0's ndcg@50: 0.963721\n","[65]\tvalid_0's ndcg@10: 0.963735\tvalid_0's ndcg@20: 0.963872\tvalid_0's ndcg@30: 0.963872\tvalid_0's ndcg@40: 0.963872\tvalid_0's ndcg@50: 0.963872\tvalid_0's ndcg@10: 0.963735\tvalid_0's ndcg@20: 0.963872\tvalid_0's ndcg@30: 0.963872\tvalid_0's ndcg@40: 0.963872\tvalid_0's ndcg@50: 0.963872\n","[66]\tvalid_0's ndcg@10: 0.963473\tvalid_0's ndcg@20: 0.963611\tvalid_0's ndcg@30: 0.963611\tvalid_0's ndcg@40: 0.963611\tvalid_0's ndcg@50: 0.963611\tvalid_0's ndcg@10: 0.963473\tvalid_0's ndcg@20: 0.963611\tvalid_0's ndcg@30: 0.963611\tvalid_0's ndcg@40: 0.963611\tvalid_0's ndcg@50: 0.963611\n","[67]\tvalid_0's ndcg@10: 0.963374\tvalid_0's ndcg@20: 0.963511\tvalid_0's ndcg@30: 0.963511\tvalid_0's ndcg@40: 0.963511\tvalid_0's ndcg@50: 0.963511\tvalid_0's ndcg@10: 0.963374\tvalid_0's ndcg@20: 0.963511\tvalid_0's ndcg@30: 0.963511\tvalid_0's ndcg@40: 0.963511\tvalid_0's ndcg@50: 0.963511\n","[68]\tvalid_0's ndcg@10: 0.963268\tvalid_0's ndcg@20: 0.963405\tvalid_0's ndcg@30: 0.963405\tvalid_0's ndcg@40: 0.963405\tvalid_0's ndcg@50: 0.963405\tvalid_0's ndcg@10: 0.963268\tvalid_0's ndcg@20: 0.963405\tvalid_0's ndcg@30: 0.963405\tvalid_0's ndcg@40: 0.963405\tvalid_0's ndcg@50: 0.963405\n","[69]\tvalid_0's ndcg@10: 0.963401\tvalid_0's ndcg@20: 0.963538\tvalid_0's ndcg@30: 0.963538\tvalid_0's ndcg@40: 0.963538\tvalid_0's ndcg@50: 0.963538\tvalid_0's ndcg@10: 0.963401\tvalid_0's ndcg@20: 0.963538\tvalid_0's ndcg@30: 0.963538\tvalid_0's ndcg@40: 0.963538\tvalid_0's ndcg@50: 0.963538\n","[70]\tvalid_0's ndcg@10: 0.963512\tvalid_0's ndcg@20: 0.963649\tvalid_0's ndcg@30: 0.963649\tvalid_0's ndcg@40: 0.963649\tvalid_0's ndcg@50: 0.963649\tvalid_0's ndcg@10: 0.963512\tvalid_0's ndcg@20: 0.963649\tvalid_0's ndcg@30: 0.963649\tvalid_0's ndcg@40: 0.963649\tvalid_0's ndcg@50: 0.963649\n","[71]\tvalid_0's ndcg@10: 0.963434\tvalid_0's ndcg@20: 0.963524\tvalid_0's ndcg@30: 0.963524\tvalid_0's ndcg@40: 0.963524\tvalid_0's ndcg@50: 0.963524\tvalid_0's ndcg@10: 0.963434\tvalid_0's ndcg@20: 0.963524\tvalid_0's ndcg@30: 0.963524\tvalid_0's ndcg@40: 0.963524\tvalid_0's ndcg@50: 0.963524\n","[72]\tvalid_0's ndcg@10: 0.96343\tvalid_0's ndcg@20: 0.96352\tvalid_0's ndcg@30: 0.96352\tvalid_0's ndcg@40: 0.96352\tvalid_0's ndcg@50: 0.96352\tvalid_0's ndcg@10: 0.96343\tvalid_0's ndcg@20: 0.96352\tvalid_0's ndcg@30: 0.96352\tvalid_0's ndcg@40: 0.96352\tvalid_0's ndcg@50: 0.96352\n","[73]\tvalid_0's ndcg@10: 0.96362\tvalid_0's ndcg@20: 0.96371\tvalid_0's ndcg@30: 0.96371\tvalid_0's ndcg@40: 0.96371\tvalid_0's ndcg@50: 0.96371\tvalid_0's ndcg@10: 0.96362\tvalid_0's ndcg@20: 0.96371\tvalid_0's ndcg@30: 0.96371\tvalid_0's ndcg@40: 0.96371\tvalid_0's ndcg@50: 0.96371\n","[74]\tvalid_0's ndcg@10: 0.963621\tvalid_0's ndcg@20: 0.963757\tvalid_0's ndcg@30: 0.963757\tvalid_0's ndcg@40: 0.963757\tvalid_0's ndcg@50: 0.963757\tvalid_0's ndcg@10: 0.963621\tvalid_0's ndcg@20: 0.963757\tvalid_0's ndcg@30: 0.963757\tvalid_0's ndcg@40: 0.963757\tvalid_0's ndcg@50: 0.963757\n","[75]\tvalid_0's ndcg@10: 0.963553\tvalid_0's ndcg@20: 0.963689\tvalid_0's ndcg@30: 0.963689\tvalid_0's ndcg@40: 0.963689\tvalid_0's ndcg@50: 0.963689\tvalid_0's ndcg@10: 0.963553\tvalid_0's ndcg@20: 0.963689\tvalid_0's ndcg@30: 0.963689\tvalid_0's ndcg@40: 0.963689\tvalid_0's ndcg@50: 0.963689\n","[76]\tvalid_0's ndcg@10: 0.963501\tvalid_0's ndcg@20: 0.963637\tvalid_0's ndcg@30: 0.963637\tvalid_0's ndcg@40: 0.963637\tvalid_0's ndcg@50: 0.963637\tvalid_0's ndcg@10: 0.963501\tvalid_0's ndcg@20: 0.963637\tvalid_0's ndcg@30: 0.963637\tvalid_0's ndcg@40: 0.963637\tvalid_0's ndcg@50: 0.963637\n","[77]\tvalid_0's ndcg@10: 0.963634\tvalid_0's ndcg@20: 0.96377\tvalid_0's ndcg@30: 0.96377\tvalid_0's ndcg@40: 0.96377\tvalid_0's ndcg@50: 0.96377\tvalid_0's ndcg@10: 0.963634\tvalid_0's ndcg@20: 0.96377\tvalid_0's ndcg@30: 0.96377\tvalid_0's ndcg@40: 0.96377\tvalid_0's ndcg@50: 0.96377\n","[78]\tvalid_0's ndcg@10: 0.963701\tvalid_0's ndcg@20: 0.963837\tvalid_0's ndcg@30: 0.963837\tvalid_0's ndcg@40: 0.963837\tvalid_0's ndcg@50: 0.963837\tvalid_0's ndcg@10: 0.963701\tvalid_0's ndcg@20: 0.963837\tvalid_0's ndcg@30: 0.963837\tvalid_0's ndcg@40: 0.963837\tvalid_0's ndcg@50: 0.963837\n","[79]\tvalid_0's ndcg@10: 0.963658\tvalid_0's ndcg@20: 0.963794\tvalid_0's ndcg@30: 0.963794\tvalid_0's ndcg@40: 0.963794\tvalid_0's ndcg@50: 0.963794\tvalid_0's ndcg@10: 0.963658\tvalid_0's ndcg@20: 0.963794\tvalid_0's ndcg@30: 0.963794\tvalid_0's ndcg@40: 0.963794\tvalid_0's ndcg@50: 0.963794\n","[80]\tvalid_0's ndcg@10: 0.963695\tvalid_0's ndcg@20: 0.963831\tvalid_0's ndcg@30: 0.963831\tvalid_0's ndcg@40: 0.963831\tvalid_0's ndcg@50: 0.963831\tvalid_0's ndcg@10: 0.963695\tvalid_0's ndcg@20: 0.963831\tvalid_0's ndcg@30: 0.963831\tvalid_0's ndcg@40: 0.963831\tvalid_0's ndcg@50: 0.963831\n","[81]\tvalid_0's ndcg@10: 0.963553\tvalid_0's ndcg@20: 0.963688\tvalid_0's ndcg@30: 0.963688\tvalid_0's ndcg@40: 0.963688\tvalid_0's ndcg@50: 0.963688\tvalid_0's ndcg@10: 0.963553\tvalid_0's ndcg@20: 0.963688\tvalid_0's ndcg@30: 0.963688\tvalid_0's ndcg@40: 0.963688\tvalid_0's ndcg@50: 0.963688\n","[82]\tvalid_0's ndcg@10: 0.963579\tvalid_0's ndcg@20: 0.963715\tvalid_0's ndcg@30: 0.963715\tvalid_0's ndcg@40: 0.963715\tvalid_0's ndcg@50: 0.963715\tvalid_0's ndcg@10: 0.963579\tvalid_0's ndcg@20: 0.963715\tvalid_0's ndcg@30: 0.963715\tvalid_0's ndcg@40: 0.963715\tvalid_0's ndcg@50: 0.963715\n","[83]\tvalid_0's ndcg@10: 0.963587\tvalid_0's ndcg@20: 0.963723\tvalid_0's ndcg@30: 0.963723\tvalid_0's ndcg@40: 0.963723\tvalid_0's ndcg@50: 0.963723\tvalid_0's ndcg@10: 0.963587\tvalid_0's ndcg@20: 0.963723\tvalid_0's ndcg@30: 0.963723\tvalid_0's ndcg@40: 0.963723\tvalid_0's ndcg@50: 0.963723\n","[84]\tvalid_0's ndcg@10: 0.963439\tvalid_0's ndcg@20: 0.963575\tvalid_0's ndcg@30: 0.963575\tvalid_0's ndcg@40: 0.963575\tvalid_0's ndcg@50: 0.963575\tvalid_0's ndcg@10: 0.963439\tvalid_0's ndcg@20: 0.963575\tvalid_0's ndcg@30: 0.963575\tvalid_0's ndcg@40: 0.963575\tvalid_0's ndcg@50: 0.963575\n","[85]\tvalid_0's ndcg@10: 0.963575\tvalid_0's ndcg@20: 0.963711\tvalid_0's ndcg@30: 0.963711\tvalid_0's ndcg@40: 0.963711\tvalid_0's ndcg@50: 0.963711\tvalid_0's ndcg@10: 0.963575\tvalid_0's ndcg@20: 0.963711\tvalid_0's ndcg@30: 0.963711\tvalid_0's ndcg@40: 0.963711\tvalid_0's ndcg@50: 0.963711\n","[86]\tvalid_0's ndcg@10: 0.963472\tvalid_0's ndcg@20: 0.963608\tvalid_0's ndcg@30: 0.963608\tvalid_0's ndcg@40: 0.963608\tvalid_0's ndcg@50: 0.963608\tvalid_0's ndcg@10: 0.963472\tvalid_0's ndcg@20: 0.963608\tvalid_0's ndcg@30: 0.963608\tvalid_0's ndcg@40: 0.963608\tvalid_0's ndcg@50: 0.963608\n","[87]\tvalid_0's ndcg@10: 0.963447\tvalid_0's ndcg@20: 0.963582\tvalid_0's ndcg@30: 0.963582\tvalid_0's ndcg@40: 0.963582\tvalid_0's ndcg@50: 0.963582\tvalid_0's ndcg@10: 0.963447\tvalid_0's ndcg@20: 0.963582\tvalid_0's ndcg@30: 0.963582\tvalid_0's ndcg@40: 0.963582\tvalid_0's ndcg@50: 0.963582\n","[88]\tvalid_0's ndcg@10: 0.963572\tvalid_0's ndcg@20: 0.963708\tvalid_0's ndcg@30: 0.963708\tvalid_0's ndcg@40: 0.963708\tvalid_0's ndcg@50: 0.963708\tvalid_0's ndcg@10: 0.963572\tvalid_0's ndcg@20: 0.963708\tvalid_0's ndcg@30: 0.963708\tvalid_0's ndcg@40: 0.963708\tvalid_0's ndcg@50: 0.963708\n","[89]\tvalid_0's ndcg@10: 0.963479\tvalid_0's ndcg@20: 0.963615\tvalid_0's ndcg@30: 0.963615\tvalid_0's ndcg@40: 0.963615\tvalid_0's ndcg@50: 0.963615\tvalid_0's ndcg@10: 0.963479\tvalid_0's ndcg@20: 0.963615\tvalid_0's ndcg@30: 0.963615\tvalid_0's ndcg@40: 0.963615\tvalid_0's ndcg@50: 0.963615\n","[90]\tvalid_0's ndcg@10: 0.9634\tvalid_0's ndcg@20: 0.963535\tvalid_0's ndcg@30: 0.963535\tvalid_0's ndcg@40: 0.963535\tvalid_0's ndcg@50: 0.963535\tvalid_0's ndcg@10: 0.9634\tvalid_0's ndcg@20: 0.963535\tvalid_0's ndcg@30: 0.963535\tvalid_0's ndcg@40: 0.963535\tvalid_0's ndcg@50: 0.963535\n","[91]\tvalid_0's ndcg@10: 0.963262\tvalid_0's ndcg@20: 0.963398\tvalid_0's ndcg@30: 0.963398\tvalid_0's ndcg@40: 0.963398\tvalid_0's ndcg@50: 0.963398\tvalid_0's ndcg@10: 0.963262\tvalid_0's ndcg@20: 0.963398\tvalid_0's ndcg@30: 0.963398\tvalid_0's ndcg@40: 0.963398\tvalid_0's ndcg@50: 0.963398\n","[92]\tvalid_0's ndcg@10: 0.963266\tvalid_0's ndcg@20: 0.963402\tvalid_0's ndcg@30: 0.963402\tvalid_0's ndcg@40: 0.963402\tvalid_0's ndcg@50: 0.963402\tvalid_0's ndcg@10: 0.963266\tvalid_0's ndcg@20: 0.963402\tvalid_0's ndcg@30: 0.963402\tvalid_0's ndcg@40: 0.963402\tvalid_0's ndcg@50: 0.963402\n","[93]\tvalid_0's ndcg@10: 0.963129\tvalid_0's ndcg@20: 0.963264\tvalid_0's ndcg@30: 0.963264\tvalid_0's ndcg@40: 0.963264\tvalid_0's ndcg@50: 0.963264\tvalid_0's ndcg@10: 0.963129\tvalid_0's ndcg@20: 0.963264\tvalid_0's ndcg@30: 0.963264\tvalid_0's ndcg@40: 0.963264\tvalid_0's ndcg@50: 0.963264\n","[94]\tvalid_0's ndcg@10: 0.963167\tvalid_0's ndcg@20: 0.963302\tvalid_0's ndcg@30: 0.963302\tvalid_0's ndcg@40: 0.963302\tvalid_0's ndcg@50: 0.963302\tvalid_0's ndcg@10: 0.963167\tvalid_0's ndcg@20: 0.963302\tvalid_0's ndcg@30: 0.963302\tvalid_0's ndcg@40: 0.963302\tvalid_0's ndcg@50: 0.963302\n","[95]\tvalid_0's ndcg@10: 0.963344\tvalid_0's ndcg@20: 0.96348\tvalid_0's ndcg@30: 0.96348\tvalid_0's ndcg@40: 0.96348\tvalid_0's ndcg@50: 0.96348\tvalid_0's ndcg@10: 0.963344\tvalid_0's ndcg@20: 0.96348\tvalid_0's ndcg@30: 0.96348\tvalid_0's ndcg@40: 0.96348\tvalid_0's ndcg@50: 0.96348\n","[96]\tvalid_0's ndcg@10: 0.963493\tvalid_0's ndcg@20: 0.963629\tvalid_0's ndcg@30: 0.963629\tvalid_0's ndcg@40: 0.963629\tvalid_0's ndcg@50: 0.963629\tvalid_0's ndcg@10: 0.963493\tvalid_0's ndcg@20: 0.963629\tvalid_0's ndcg@30: 0.963629\tvalid_0's ndcg@40: 0.963629\tvalid_0's ndcg@50: 0.963629\n","[97]\tvalid_0's ndcg@10: 0.963556\tvalid_0's ndcg@20: 0.963691\tvalid_0's ndcg@30: 0.963691\tvalid_0's ndcg@40: 0.963691\tvalid_0's ndcg@50: 0.963691\tvalid_0's ndcg@10: 0.963556\tvalid_0's ndcg@20: 0.963691\tvalid_0's ndcg@30: 0.963691\tvalid_0's ndcg@40: 0.963691\tvalid_0's ndcg@50: 0.963691\n","[98]\tvalid_0's ndcg@10: 0.96343\tvalid_0's ndcg@20: 0.963566\tvalid_0's ndcg@30: 0.963566\tvalid_0's ndcg@40: 0.963566\tvalid_0's ndcg@50: 0.963566\tvalid_0's ndcg@10: 0.96343\tvalid_0's ndcg@20: 0.963566\tvalid_0's ndcg@30: 0.963566\tvalid_0's ndcg@40: 0.963566\tvalid_0's ndcg@50: 0.963566\n","[99]\tvalid_0's ndcg@10: 0.963578\tvalid_0's ndcg@20: 0.963713\tvalid_0's ndcg@30: 0.963713\tvalid_0's ndcg@40: 0.963713\tvalid_0's ndcg@50: 0.963713\tvalid_0's ndcg@10: 0.963578\tvalid_0's ndcg@20: 0.963713\tvalid_0's ndcg@30: 0.963713\tvalid_0's ndcg@40: 0.963713\tvalid_0's ndcg@50: 0.963713\n","[100]\tvalid_0's ndcg@10: 0.963493\tvalid_0's ndcg@20: 0.963629\tvalid_0's ndcg@30: 0.963629\tvalid_0's ndcg@40: 0.963629\tvalid_0's ndcg@50: 0.963629\tvalid_0's ndcg@10: 0.963493\tvalid_0's ndcg@20: 0.963629\tvalid_0's ndcg@30: 0.963629\tvalid_0's ndcg@40: 0.963629\tvalid_0's ndcg@50: 0.963629\n","Did not meet early stopping. Best iteration is:\n","[57]\tvalid_0's ndcg@10: 0.963799\tvalid_0's ndcg@20: 0.96389\tvalid_0's ndcg@30: 0.96389\tvalid_0's ndcg@40: 0.96389\tvalid_0's ndcg@50: 0.96389\tvalid_0's ndcg@10: 0.963799\tvalid_0's ndcg@20: 0.96389\tvalid_0's ndcg@30: 0.96389\tvalid_0's ndcg@40: 0.96389\tvalid_0's ndcg@50: 0.96389\n"]}]},{"cell_type":"markdown","source":["2. Use the LGBMClassifier model to sort the candidate set returned by each model"],"metadata":{"id":"x3ZGSgSB4EMS"}},{"cell_type":"code","source":["!python src/rank/GBDT_classifier.py --task itemcf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZCuHgWzj4Bbt","executionInfo":{"status":"ok","timestamp":1639076352087,"user_tz":-480,"elapsed":42606,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"75641ed6-7955-48e3-92bf-f772d3dd8d6a"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 2592, number of negative: 117002\n","[LightGBM] [Info] Total Bins 3568\n","[LightGBM] [Info] Number of data: 119594, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.021673 -> initscore=-3.809761\n","[LightGBM] [Info] Start training from score -3.809761\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 11\n","[1]\tvalid_0's auc: 0.815618\tvalid_0's binary_logloss: 0.103457\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 13\n","[2]\tvalid_0's auc: 0.828115\tvalid_0's binary_logloss: 0.102898\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 10\n","[3]\tvalid_0's auc: 0.847709\tvalid_0's binary_logloss: 0.10231\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 10\n","[4]\tvalid_0's auc: 0.852981\tvalid_0's binary_logloss: 0.101661\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[5]\tvalid_0's auc: 0.853171\tvalid_0's binary_logloss: 0.101169\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 12\n","[6]\tvalid_0's auc: 0.854884\tvalid_0's binary_logloss: 0.100667\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 10\n","[7]\tvalid_0's auc: 0.857823\tvalid_0's binary_logloss: 0.100166\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 12\n","[8]\tvalid_0's auc: 0.861806\tvalid_0's binary_logloss: 0.0998002\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 10\n","[9]\tvalid_0's auc: 0.861047\tvalid_0's binary_logloss: 0.0993635\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[10]\tvalid_0's auc: 0.865606\tvalid_0's binary_logloss: 0.0987264\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 10\n","[11]\tvalid_0's auc: 0.866841\tvalid_0's binary_logloss: 0.0981702\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 8\n","[12]\tvalid_0's auc: 0.867252\tvalid_0's binary_logloss: 0.0976067\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[13]\tvalid_0's auc: 0.868005\tvalid_0's binary_logloss: 0.0970448\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 8\n","[14]\tvalid_0's auc: 0.869177\tvalid_0's binary_logloss: 0.0964794\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 12\n","[15]\tvalid_0's auc: 0.869182\tvalid_0's binary_logloss: 0.096046\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[16]\tvalid_0's auc: 0.869084\tvalid_0's binary_logloss: 0.0956313\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[17]\tvalid_0's auc: 0.869442\tvalid_0's binary_logloss: 0.0952269\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 11\n","[18]\tvalid_0's auc: 0.870027\tvalid_0's binary_logloss: 0.0947902\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[19]\tvalid_0's auc: 0.870142\tvalid_0's binary_logloss: 0.0943384\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 8\n","[20]\tvalid_0's auc: 0.869879\tvalid_0's binary_logloss: 0.0939283\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 10\n","[21]\tvalid_0's auc: 0.869884\tvalid_0's binary_logloss: 0.0935801\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 7\n","[22]\tvalid_0's auc: 0.869704\tvalid_0's binary_logloss: 0.0931843\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[23]\tvalid_0's auc: 0.870314\tvalid_0's binary_logloss: 0.0929508\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 11\n","[24]\tvalid_0's auc: 0.870424\tvalid_0's binary_logloss: 0.0927244\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[25]\tvalid_0's auc: 0.870632\tvalid_0's binary_logloss: 0.0923761\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[26]\tvalid_0's auc: 0.870334\tvalid_0's binary_logloss: 0.0920031\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[27]\tvalid_0's auc: 0.870302\tvalid_0's binary_logloss: 0.0917027\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 29 and max_depth = 11\n","[28]\tvalid_0's auc: 0.871246\tvalid_0's binary_logloss: 0.0914906\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 9\n","[29]\tvalid_0's auc: 0.871413\tvalid_0's binary_logloss: 0.0911721\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 12\n","[30]\tvalid_0's auc: 0.871284\tvalid_0's binary_logloss: 0.090906\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 9\n","[31]\tvalid_0's auc: 0.871171\tvalid_0's binary_logloss: 0.0905957\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[32]\tvalid_0's auc: 0.871215\tvalid_0's binary_logloss: 0.0902713\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[33]\tvalid_0's auc: 0.871773\tvalid_0's binary_logloss: 0.0900909\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[34]\tvalid_0's auc: 0.871706\tvalid_0's binary_logloss: 0.0898418\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[35]\tvalid_0's auc: 0.87189\tvalid_0's binary_logloss: 0.0895417\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[36]\tvalid_0's auc: 0.871918\tvalid_0's binary_logloss: 0.0892561\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[37]\tvalid_0's auc: 0.871878\tvalid_0's binary_logloss: 0.0890163\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[38]\tvalid_0's auc: 0.871715\tvalid_0's binary_logloss: 0.0887398\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 9\n","[39]\tvalid_0's auc: 0.871778\tvalid_0's binary_logloss: 0.0884881\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 8\n","[40]\tvalid_0's auc: 0.871622\tvalid_0's binary_logloss: 0.0882377\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 7\n","[41]\tvalid_0's auc: 0.871632\tvalid_0's binary_logloss: 0.0879928\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 8\n","[42]\tvalid_0's auc: 0.871657\tvalid_0's binary_logloss: 0.0877671\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[43]\tvalid_0's auc: 0.871786\tvalid_0's binary_logloss: 0.0875183\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[44]\tvalid_0's auc: 0.871599\tvalid_0's binary_logloss: 0.0872998\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[45]\tvalid_0's auc: 0.871711\tvalid_0's binary_logloss: 0.0870815\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[46]\tvalid_0's auc: 0.871638\tvalid_0's binary_logloss: 0.0868664\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[47]\tvalid_0's auc: 0.87166\tvalid_0's binary_logloss: 0.0866552\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[48]\tvalid_0's auc: 0.871998\tvalid_0's binary_logloss: 0.0864454\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[49]\tvalid_0's auc: 0.872239\tvalid_0's binary_logloss: 0.0863067\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[50]\tvalid_0's auc: 0.872297\tvalid_0's binary_logloss: 0.0861109\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 10\n","[51]\tvalid_0's auc: 0.872682\tvalid_0's binary_logloss: 0.0859793\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[52]\tvalid_0's auc: 0.872859\tvalid_0's binary_logloss: 0.0857825\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[53]\tvalid_0's auc: 0.872966\tvalid_0's binary_logloss: 0.0855906\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[54]\tvalid_0's auc: 0.872839\tvalid_0's binary_logloss: 0.0854013\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[55]\tvalid_0's auc: 0.872812\tvalid_0's binary_logloss: 0.0852274\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 8\n","[56]\tvalid_0's auc: 0.872729\tvalid_0's binary_logloss: 0.0850556\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 11\n","[57]\tvalid_0's auc: 0.872843\tvalid_0's binary_logloss: 0.084905\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[58]\tvalid_0's auc: 0.87292\tvalid_0's binary_logloss: 0.0847663\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[59]\tvalid_0's auc: 0.872931\tvalid_0's binary_logloss: 0.0845873\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[60]\tvalid_0's auc: 0.872898\tvalid_0's binary_logloss: 0.0844408\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 8\n","[61]\tvalid_0's auc: 0.87298\tvalid_0's binary_logloss: 0.0842753\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 28 and max_depth = 11\n","[62]\tvalid_0's auc: 0.873192\tvalid_0's binary_logloss: 0.0841684\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 28 and max_depth = 10\n","[63]\tvalid_0's auc: 0.873383\tvalid_0's binary_logloss: 0.0840535\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 8\n","[64]\tvalid_0's auc: 0.873485\tvalid_0's binary_logloss: 0.083905\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[65]\tvalid_0's auc: 0.873551\tvalid_0's binary_logloss: 0.0837711\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[66]\tvalid_0's auc: 0.873607\tvalid_0's binary_logloss: 0.0836351\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[67]\tvalid_0's auc: 0.873583\tvalid_0's binary_logloss: 0.0834896\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[68]\tvalid_0's auc: 0.873525\tvalid_0's binary_logloss: 0.0833725\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[69]\tvalid_0's auc: 0.873516\tvalid_0's binary_logloss: 0.083238\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[70]\tvalid_0's auc: 0.873756\tvalid_0's binary_logloss: 0.0830832\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[71]\tvalid_0's auc: 0.873846\tvalid_0's binary_logloss: 0.0829425\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 8\n","[72]\tvalid_0's auc: 0.873846\tvalid_0's binary_logloss: 0.0828296\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[73]\tvalid_0's auc: 0.873868\tvalid_0's binary_logloss: 0.082744\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[74]\tvalid_0's auc: 0.874087\tvalid_0's binary_logloss: 0.0826065\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[75]\tvalid_0's auc: 0.874279\tvalid_0's binary_logloss: 0.082525\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[76]\tvalid_0's auc: 0.874504\tvalid_0's binary_logloss: 0.0824238\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[77]\tvalid_0's auc: 0.874584\tvalid_0's binary_logloss: 0.0823134\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[78]\tvalid_0's auc: 0.8745\tvalid_0's binary_logloss: 0.0821965\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[79]\tvalid_0's auc: 0.874582\tvalid_0's binary_logloss: 0.0820727\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[80]\tvalid_0's auc: 0.874657\tvalid_0's binary_logloss: 0.0819443\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[81]\tvalid_0's auc: 0.874553\tvalid_0's binary_logloss: 0.0818363\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[82]\tvalid_0's auc: 0.874593\tvalid_0's binary_logloss: 0.081715\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 8\n","[83]\tvalid_0's auc: 0.874592\tvalid_0's binary_logloss: 0.0816175\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 8\n","[84]\tvalid_0's auc: 0.874502\tvalid_0's binary_logloss: 0.0815306\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[85]\tvalid_0's auc: 0.874611\tvalid_0's binary_logloss: 0.081442\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[86]\tvalid_0's auc: 0.874713\tvalid_0's binary_logloss: 0.0813302\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 8\n","[87]\tvalid_0's auc: 0.874732\tvalid_0's binary_logloss: 0.0812216\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[88]\tvalid_0's auc: 0.874767\tvalid_0's binary_logloss: 0.0811371\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[89]\tvalid_0's auc: 0.874675\tvalid_0's binary_logloss: 0.0810381\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[90]\tvalid_0's auc: 0.874611\tvalid_0's binary_logloss: 0.0809365\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 7\n","[91]\tvalid_0's auc: 0.874634\tvalid_0's binary_logloss: 0.0808452\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[92]\tvalid_0's auc: 0.874642\tvalid_0's binary_logloss: 0.0807468\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[93]\tvalid_0's auc: 0.874543\tvalid_0's binary_logloss: 0.0806744\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[94]\tvalid_0's auc: 0.874591\tvalid_0's binary_logloss: 0.0805956\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[95]\tvalid_0's auc: 0.87466\tvalid_0's binary_logloss: 0.0804997\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 8\n","[96]\tvalid_0's auc: 0.874802\tvalid_0's binary_logloss: 0.080401\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 28 and max_depth = 7\n","[97]\tvalid_0's auc: 0.874888\tvalid_0's binary_logloss: 0.0803093\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[98]\tvalid_0's auc: 0.874953\tvalid_0's binary_logloss: 0.0802371\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[99]\tvalid_0's auc: 0.87507\tvalid_0's binary_logloss: 0.0801489\n","[LightGBM] [Debug] Re-bagging, using 83708 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 8\n","[100]\tvalid_0's auc: 0.875062\tvalid_0's binary_logloss: 0.0800657\n","Did not meet early stopping. Best iteration is:\n","[99]\tvalid_0's auc: 0.87507\tvalid_0's binary_logloss: 0.0801489\n","[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 2593, number of negative: 117219\n","[LightGBM] [Info] Total Bins 3569\n","[LightGBM] [Info] Number of data: 119812, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.021642 -> initscore=-3.811228\n","[LightGBM] [Info] Start training from score -3.811228\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 10\n","[1]\tvalid_0's auc: 0.815581\tvalid_0's binary_logloss: 0.10393\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 11\n","[2]\tvalid_0's auc: 0.821191\tvalid_0's binary_logloss: 0.103389\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 9\n","[3]\tvalid_0's auc: 0.838845\tvalid_0's binary_logloss: 0.102826\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 11\n","[4]\tvalid_0's auc: 0.841331\tvalid_0's binary_logloss: 0.102184\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[5]\tvalid_0's auc: 0.842417\tvalid_0's binary_logloss: 0.101672\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[6]\tvalid_0's auc: 0.846182\tvalid_0's binary_logloss: 0.101078\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 9\n","[7]\tvalid_0's auc: 0.848235\tvalid_0's binary_logloss: 0.100585\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 12\n","[8]\tvalid_0's auc: 0.849804\tvalid_0's binary_logloss: 0.100256\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 12\n","[9]\tvalid_0's auc: 0.849308\tvalid_0's binary_logloss: 0.099738\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 10\n","[10]\tvalid_0's auc: 0.852257\tvalid_0's binary_logloss: 0.0991243\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[11]\tvalid_0's auc: 0.853718\tvalid_0's binary_logloss: 0.098539\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[12]\tvalid_0's auc: 0.853517\tvalid_0's binary_logloss: 0.0980129\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 10\n","[13]\tvalid_0's auc: 0.853987\tvalid_0's binary_logloss: 0.097529\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 10\n","[14]\tvalid_0's auc: 0.855243\tvalid_0's binary_logloss: 0.097027\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[15]\tvalid_0's auc: 0.854989\tvalid_0's binary_logloss: 0.0966376\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 11\n","[16]\tvalid_0's auc: 0.854972\tvalid_0's binary_logloss: 0.0962482\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 9\n","[17]\tvalid_0's auc: 0.854909\tvalid_0's binary_logloss: 0.0958486\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[18]\tvalid_0's auc: 0.855025\tvalid_0's binary_logloss: 0.0954117\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[19]\tvalid_0's auc: 0.854977\tvalid_0's binary_logloss: 0.0950409\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[20]\tvalid_0's auc: 0.854978\tvalid_0's binary_logloss: 0.0946575\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[21]\tvalid_0's auc: 0.854968\tvalid_0's binary_logloss: 0.0943505\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[22]\tvalid_0's auc: 0.855077\tvalid_0's binary_logloss: 0.093956\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[23]\tvalid_0's auc: 0.855497\tvalid_0's binary_logloss: 0.093721\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[24]\tvalid_0's auc: 0.855889\tvalid_0's binary_logloss: 0.093499\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[25]\tvalid_0's auc: 0.856119\tvalid_0's binary_logloss: 0.0931639\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[26]\tvalid_0's auc: 0.856174\tvalid_0's binary_logloss: 0.0928108\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 9\n","[27]\tvalid_0's auc: 0.856194\tvalid_0's binary_logloss: 0.0925354\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[28]\tvalid_0's auc: 0.856367\tvalid_0's binary_logloss: 0.0923289\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 8\n","[29]\tvalid_0's auc: 0.856303\tvalid_0's binary_logloss: 0.0920335\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[30]\tvalid_0's auc: 0.856162\tvalid_0's binary_logloss: 0.0917916\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[31]\tvalid_0's auc: 0.856185\tvalid_0's binary_logloss: 0.0915149\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[32]\tvalid_0's auc: 0.856213\tvalid_0's binary_logloss: 0.0912204\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[33]\tvalid_0's auc: 0.856783\tvalid_0's binary_logloss: 0.0910417\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[34]\tvalid_0's auc: 0.856608\tvalid_0's binary_logloss: 0.0908146\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 8\n","[35]\tvalid_0's auc: 0.856811\tvalid_0's binary_logloss: 0.0905418\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[36]\tvalid_0's auc: 0.856878\tvalid_0's binary_logloss: 0.0902723\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 11\n","[37]\tvalid_0's auc: 0.857019\tvalid_0's binary_logloss: 0.090051\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[38]\tvalid_0's auc: 0.857132\tvalid_0's binary_logloss: 0.0897912\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[39]\tvalid_0's auc: 0.857049\tvalid_0's binary_logloss: 0.0895653\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[40]\tvalid_0's auc: 0.857208\tvalid_0's binary_logloss: 0.0893279\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[41]\tvalid_0's auc: 0.8573\tvalid_0's binary_logloss: 0.0891104\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[42]\tvalid_0's auc: 0.857309\tvalid_0's binary_logloss: 0.0888967\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[43]\tvalid_0's auc: 0.857356\tvalid_0's binary_logloss: 0.088682\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[44]\tvalid_0's auc: 0.85751\tvalid_0's binary_logloss: 0.0884739\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[45]\tvalid_0's auc: 0.857415\tvalid_0's binary_logloss: 0.0882684\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[46]\tvalid_0's auc: 0.8574\tvalid_0's binary_logloss: 0.08807\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[47]\tvalid_0's auc: 0.857597\tvalid_0's binary_logloss: 0.0878509\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[48]\tvalid_0's auc: 0.857709\tvalid_0's binary_logloss: 0.0876648\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 11\n","[49]\tvalid_0's auc: 0.857793\tvalid_0's binary_logloss: 0.0875406\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[50]\tvalid_0's auc: 0.857883\tvalid_0's binary_logloss: 0.0873651\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 28 and max_depth = 10\n","[51]\tvalid_0's auc: 0.858348\tvalid_0's binary_logloss: 0.0872399\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[52]\tvalid_0's auc: 0.858477\tvalid_0's binary_logloss: 0.0870515\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[53]\tvalid_0's auc: 0.858509\tvalid_0's binary_logloss: 0.0868719\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 7\n","[54]\tvalid_0's auc: 0.858486\tvalid_0's binary_logloss: 0.0867058\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[55]\tvalid_0's auc: 0.858519\tvalid_0's binary_logloss: 0.0865328\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[56]\tvalid_0's auc: 0.858591\tvalid_0's binary_logloss: 0.0863682\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[57]\tvalid_0's auc: 0.858591\tvalid_0's binary_logloss: 0.086224\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 10\n","[58]\tvalid_0's auc: 0.858698\tvalid_0's binary_logloss: 0.0860954\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[59]\tvalid_0's auc: 0.858782\tvalid_0's binary_logloss: 0.0859321\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 29 and max_depth = 10\n","[60]\tvalid_0's auc: 0.8588\tvalid_0's binary_logloss: 0.0857933\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[61]\tvalid_0's auc: 0.858744\tvalid_0's binary_logloss: 0.0856556\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 10\n","[62]\tvalid_0's auc: 0.858697\tvalid_0's binary_logloss: 0.0855568\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[63]\tvalid_0's auc: 0.858918\tvalid_0's binary_logloss: 0.0854384\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 8\n","[64]\tvalid_0's auc: 0.858971\tvalid_0's binary_logloss: 0.0852991\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 28 and max_depth = 10\n","[65]\tvalid_0's auc: 0.859039\tvalid_0's binary_logloss: 0.0851784\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[66]\tvalid_0's auc: 0.859063\tvalid_0's binary_logloss: 0.0850612\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[67]\tvalid_0's auc: 0.859174\tvalid_0's binary_logloss: 0.084925\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[68]\tvalid_0's auc: 0.859177\tvalid_0's binary_logloss: 0.0848114\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[69]\tvalid_0's auc: 0.859107\tvalid_0's binary_logloss: 0.0846941\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[70]\tvalid_0's auc: 0.859265\tvalid_0's binary_logloss: 0.0845724\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[71]\tvalid_0's auc: 0.859298\tvalid_0's binary_logloss: 0.0844416\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[72]\tvalid_0's auc: 0.859429\tvalid_0's binary_logloss: 0.0843354\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[73]\tvalid_0's auc: 0.859455\tvalid_0's binary_logloss: 0.0842491\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[74]\tvalid_0's auc: 0.859436\tvalid_0's binary_logloss: 0.0841346\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[75]\tvalid_0's auc: 0.859373\tvalid_0's binary_logloss: 0.0840614\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[76]\tvalid_0's auc: 0.859453\tvalid_0's binary_logloss: 0.0839736\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[77]\tvalid_0's auc: 0.859406\tvalid_0's binary_logloss: 0.0838844\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 8\n","[78]\tvalid_0's auc: 0.859396\tvalid_0's binary_logloss: 0.0837662\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[79]\tvalid_0's auc: 0.85937\tvalid_0's binary_logloss: 0.083666\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[80]\tvalid_0's auc: 0.859439\tvalid_0's binary_logloss: 0.0835599\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[81]\tvalid_0's auc: 0.859454\tvalid_0's binary_logloss: 0.0834579\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[82]\tvalid_0's auc: 0.859437\tvalid_0's binary_logloss: 0.0833545\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[83]\tvalid_0's auc: 0.859443\tvalid_0's binary_logloss: 0.0832671\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[84]\tvalid_0's auc: 0.859478\tvalid_0's binary_logloss: 0.0831803\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 8\n","[85]\tvalid_0's auc: 0.859436\tvalid_0's binary_logloss: 0.0831025\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[86]\tvalid_0's auc: 0.859444\tvalid_0's binary_logloss: 0.0830037\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[87]\tvalid_0's auc: 0.85941\tvalid_0's binary_logloss: 0.0829149\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[88]\tvalid_0's auc: 0.859447\tvalid_0's binary_logloss: 0.0828402\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[89]\tvalid_0's auc: 0.859461\tvalid_0's binary_logloss: 0.082753\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 8\n","[90]\tvalid_0's auc: 0.85947\tvalid_0's binary_logloss: 0.0826705\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[91]\tvalid_0's auc: 0.859446\tvalid_0's binary_logloss: 0.0825918\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[92]\tvalid_0's auc: 0.85945\tvalid_0's binary_logloss: 0.0825077\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[93]\tvalid_0's auc: 0.859432\tvalid_0's binary_logloss: 0.0824411\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[94]\tvalid_0's auc: 0.85944\tvalid_0's binary_logloss: 0.082371\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[95]\tvalid_0's auc: 0.859431\tvalid_0's binary_logloss: 0.0822864\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[96]\tvalid_0's auc: 0.859428\tvalid_0's binary_logloss: 0.082207\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 7\n","[97]\tvalid_0's auc: 0.85945\tvalid_0's binary_logloss: 0.0821353\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[98]\tvalid_0's auc: 0.859519\tvalid_0's binary_logloss: 0.0820707\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 8\n","[99]\tvalid_0's auc: 0.859522\tvalid_0's binary_logloss: 0.0819936\n","[LightGBM] [Debug] Re-bagging, using 83863 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[100]\tvalid_0's auc: 0.859541\tvalid_0's binary_logloss: 0.0819243\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's auc: 0.859541\tvalid_0's binary_logloss: 0.0819243\n","[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 2593, number of negative: 117213\n","[LightGBM] [Info] Total Bins 3569\n","[LightGBM] [Info] Number of data: 119806, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.021643 -> initscore=-3.811177\n","[LightGBM] [Info] Start training from score -3.811177\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 11\n","[1]\tvalid_0's auc: 0.805548\tvalid_0's binary_logloss: 0.103909\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 10\n","[2]\tvalid_0's auc: 0.81156\tvalid_0's binary_logloss: 0.103368\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 10\n","[3]\tvalid_0's auc: 0.82915\tvalid_0's binary_logloss: 0.102797\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 10\n","[4]\tvalid_0's auc: 0.831055\tvalid_0's binary_logloss: 0.102201\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[5]\tvalid_0's auc: 0.83241\tvalid_0's binary_logloss: 0.10167\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 11\n","[6]\tvalid_0's auc: 0.835543\tvalid_0's binary_logloss: 0.101167\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 8\n","[7]\tvalid_0's auc: 0.838717\tvalid_0's binary_logloss: 0.100511\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[8]\tvalid_0's auc: 0.841486\tvalid_0's binary_logloss: 0.100158\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 20 and max_depth = 11\n","[9]\tvalid_0's auc: 0.841718\tvalid_0's binary_logloss: 0.0995963\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 9\n","[10]\tvalid_0's auc: 0.842703\tvalid_0's binary_logloss: 0.0990491\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 9\n","[11]\tvalid_0's auc: 0.84461\tvalid_0's binary_logloss: 0.098479\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 8\n","[12]\tvalid_0's auc: 0.845479\tvalid_0's binary_logloss: 0.0978936\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 8\n","[13]\tvalid_0's auc: 0.846488\tvalid_0's binary_logloss: 0.097387\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[14]\tvalid_0's auc: 0.846986\tvalid_0's binary_logloss: 0.096885\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[15]\tvalid_0's auc: 0.846961\tvalid_0's binary_logloss: 0.0964385\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[16]\tvalid_0's auc: 0.846531\tvalid_0's binary_logloss: 0.0960162\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[17]\tvalid_0's auc: 0.846763\tvalid_0's binary_logloss: 0.0955796\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 9\n","[18]\tvalid_0's auc: 0.847138\tvalid_0's binary_logloss: 0.095128\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[19]\tvalid_0's auc: 0.847269\tvalid_0's binary_logloss: 0.0947195\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 9\n","[20]\tvalid_0's auc: 0.847151\tvalid_0's binary_logloss: 0.0943258\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[21]\tvalid_0's auc: 0.84733\tvalid_0's binary_logloss: 0.0939661\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[22]\tvalid_0's auc: 0.847467\tvalid_0's binary_logloss: 0.0935728\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 10\n","[23]\tvalid_0's auc: 0.847818\tvalid_0's binary_logloss: 0.0933427\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[24]\tvalid_0's auc: 0.847816\tvalid_0's binary_logloss: 0.0931037\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[25]\tvalid_0's auc: 0.847886\tvalid_0's binary_logloss: 0.0927682\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 7\n","[26]\tvalid_0's auc: 0.848031\tvalid_0's binary_logloss: 0.0923694\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 11\n","[27]\tvalid_0's auc: 0.848817\tvalid_0's binary_logloss: 0.0920425\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[28]\tvalid_0's auc: 0.84926\tvalid_0's binary_logloss: 0.091839\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[29]\tvalid_0's auc: 0.849297\tvalid_0's binary_logloss: 0.0915338\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[30]\tvalid_0's auc: 0.849447\tvalid_0's binary_logloss: 0.0912363\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[31]\tvalid_0's auc: 0.849327\tvalid_0's binary_logloss: 0.0909399\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[32]\tvalid_0's auc: 0.849334\tvalid_0's binary_logloss: 0.0906268\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[33]\tvalid_0's auc: 0.849351\tvalid_0's binary_logloss: 0.0904579\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 10\n","[34]\tvalid_0's auc: 0.849222\tvalid_0's binary_logloss: 0.0901936\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[35]\tvalid_0's auc: 0.84937\tvalid_0's binary_logloss: 0.0899175\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 7\n","[36]\tvalid_0's auc: 0.849342\tvalid_0's binary_logloss: 0.0896581\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 9\n","[37]\tvalid_0's auc: 0.849377\tvalid_0's binary_logloss: 0.0894084\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[38]\tvalid_0's auc: 0.849323\tvalid_0's binary_logloss: 0.0891348\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 8\n","[39]\tvalid_0's auc: 0.849405\tvalid_0's binary_logloss: 0.0889149\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[40]\tvalid_0's auc: 0.849503\tvalid_0's binary_logloss: 0.0886801\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[41]\tvalid_0's auc: 0.849501\tvalid_0's binary_logloss: 0.0884619\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[42]\tvalid_0's auc: 0.849822\tvalid_0's binary_logloss: 0.0882077\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[43]\tvalid_0's auc: 0.849847\tvalid_0's binary_logloss: 0.0879754\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 7\n","[44]\tvalid_0's auc: 0.849731\tvalid_0's binary_logloss: 0.0877514\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[45]\tvalid_0's auc: 0.84976\tvalid_0's binary_logloss: 0.0875538\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[46]\tvalid_0's auc: 0.849722\tvalid_0's binary_logloss: 0.0873271\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 11\n","[47]\tvalid_0's auc: 0.849784\tvalid_0's binary_logloss: 0.0871022\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 8\n","[48]\tvalid_0's auc: 0.850164\tvalid_0's binary_logloss: 0.0869052\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 8\n","[49]\tvalid_0's auc: 0.850294\tvalid_0's binary_logloss: 0.0867676\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[50]\tvalid_0's auc: 0.850371\tvalid_0's binary_logloss: 0.0865773\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[51]\tvalid_0's auc: 0.850417\tvalid_0's binary_logloss: 0.0864619\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[52]\tvalid_0's auc: 0.850318\tvalid_0's binary_logloss: 0.0862812\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[53]\tvalid_0's auc: 0.850237\tvalid_0's binary_logloss: 0.0861074\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 6\n","[54]\tvalid_0's auc: 0.850369\tvalid_0's binary_logloss: 0.0859211\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[55]\tvalid_0's auc: 0.850481\tvalid_0's binary_logloss: 0.0857302\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[56]\tvalid_0's auc: 0.850546\tvalid_0's binary_logloss: 0.0855456\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[57]\tvalid_0's auc: 0.85075\tvalid_0's binary_logloss: 0.0853908\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[58]\tvalid_0's auc: 0.850819\tvalid_0's binary_logloss: 0.0852537\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[59]\tvalid_0's auc: 0.850854\tvalid_0's binary_logloss: 0.0850864\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 7\n","[60]\tvalid_0's auc: 0.850864\tvalid_0's binary_logloss: 0.0849374\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[61]\tvalid_0's auc: 0.850821\tvalid_0's binary_logloss: 0.0847879\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 11\n","[62]\tvalid_0's auc: 0.850995\tvalid_0's binary_logloss: 0.0846873\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[63]\tvalid_0's auc: 0.851176\tvalid_0's binary_logloss: 0.0845677\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 8\n","[64]\tvalid_0's auc: 0.851161\tvalid_0's binary_logloss: 0.0844209\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[65]\tvalid_0's auc: 0.851275\tvalid_0's binary_logloss: 0.084289\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[66]\tvalid_0's auc: 0.851291\tvalid_0's binary_logloss: 0.0841595\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[67]\tvalid_0's auc: 0.851375\tvalid_0's binary_logloss: 0.0840019\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[68]\tvalid_0's auc: 0.851422\tvalid_0's binary_logloss: 0.0838822\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 8\n","[69]\tvalid_0's auc: 0.851417\tvalid_0's binary_logloss: 0.0837561\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[70]\tvalid_0's auc: 0.851489\tvalid_0's binary_logloss: 0.0836172\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[71]\tvalid_0's auc: 0.851463\tvalid_0's binary_logloss: 0.0834725\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[72]\tvalid_0's auc: 0.851489\tvalid_0's binary_logloss: 0.0833578\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[73]\tvalid_0's auc: 0.851479\tvalid_0's binary_logloss: 0.0832695\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 28 and max_depth = 8\n","[74]\tvalid_0's auc: 0.851539\tvalid_0's binary_logloss: 0.0831499\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 11\n","[75]\tvalid_0's auc: 0.851588\tvalid_0's binary_logloss: 0.0830663\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[76]\tvalid_0's auc: 0.851696\tvalid_0's binary_logloss: 0.0829668\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 11\n","[77]\tvalid_0's auc: 0.851798\tvalid_0's binary_logloss: 0.0828556\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[78]\tvalid_0's auc: 0.851925\tvalid_0's binary_logloss: 0.0827352\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[79]\tvalid_0's auc: 0.852009\tvalid_0's binary_logloss: 0.0826268\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[80]\tvalid_0's auc: 0.852115\tvalid_0's binary_logloss: 0.0825026\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 8\n","[81]\tvalid_0's auc: 0.851982\tvalid_0's binary_logloss: 0.0824011\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[82]\tvalid_0's auc: 0.851993\tvalid_0's binary_logloss: 0.082295\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[83]\tvalid_0's auc: 0.851975\tvalid_0's binary_logloss: 0.0822001\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[84]\tvalid_0's auc: 0.852081\tvalid_0's binary_logloss: 0.0821043\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[85]\tvalid_0's auc: 0.852124\tvalid_0's binary_logloss: 0.0820157\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[86]\tvalid_0's auc: 0.852185\tvalid_0's binary_logloss: 0.0819182\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 28 and max_depth = 8\n","[87]\tvalid_0's auc: 0.852261\tvalid_0's binary_logloss: 0.0818139\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[88]\tvalid_0's auc: 0.852393\tvalid_0's binary_logloss: 0.0817277\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 7\n","[89]\tvalid_0's auc: 0.852462\tvalid_0's binary_logloss: 0.0816315\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 8\n","[90]\tvalid_0's auc: 0.852622\tvalid_0's binary_logloss: 0.0815317\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[91]\tvalid_0's auc: 0.852644\tvalid_0's binary_logloss: 0.0814452\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[92]\tvalid_0's auc: 0.852673\tvalid_0's binary_logloss: 0.0813522\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[93]\tvalid_0's auc: 0.852661\tvalid_0's binary_logloss: 0.0812845\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[94]\tvalid_0's auc: 0.852566\tvalid_0's binary_logloss: 0.0812166\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[95]\tvalid_0's auc: 0.852509\tvalid_0's binary_logloss: 0.081135\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 7\n","[96]\tvalid_0's auc: 0.852491\tvalid_0's binary_logloss: 0.0810546\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[97]\tvalid_0's auc: 0.852513\tvalid_0's binary_logloss: 0.0809779\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[98]\tvalid_0's auc: 0.85254\tvalid_0's binary_logloss: 0.0809075\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[99]\tvalid_0's auc: 0.852519\tvalid_0's binary_logloss: 0.0808272\n","[LightGBM] [Debug] Re-bagging, using 83855 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 7\n","[100]\tvalid_0's auc: 0.852444\tvalid_0's binary_logloss: 0.0807532\n","Did not meet early stopping. Best iteration is:\n","[92]\tvalid_0's auc: 0.852673\tvalid_0's binary_logloss: 0.0813522\n","[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 2593, number of negative: 117195\n","[LightGBM] [Info] Total Bins 3570\n","[LightGBM] [Info] Number of data: 119788, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.021647 -> initscore=-3.811024\n","[LightGBM] [Info] Start training from score -3.811024\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 10\n","[1]\tvalid_0's auc: 0.806903\tvalid_0's binary_logloss: 0.103896\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 11\n","[2]\tvalid_0's auc: 0.818885\tvalid_0's binary_logloss: 0.103321\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 11\n","[3]\tvalid_0's auc: 0.837844\tvalid_0's binary_logloss: 0.102761\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 13\n","[4]\tvalid_0's auc: 0.844353\tvalid_0's binary_logloss: 0.102179\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 10\n","[5]\tvalid_0's auc: 0.843868\tvalid_0's binary_logloss: 0.10172\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[6]\tvalid_0's auc: 0.846199\tvalid_0's binary_logloss: 0.101239\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 10\n","[7]\tvalid_0's auc: 0.848983\tvalid_0's binary_logloss: 0.100763\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 28 and max_depth = 10\n","[8]\tvalid_0's auc: 0.849096\tvalid_0's binary_logloss: 0.100402\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 14\n","[9]\tvalid_0's auc: 0.848543\tvalid_0's binary_logloss: 0.099906\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 8\n","[10]\tvalid_0's auc: 0.850633\tvalid_0's binary_logloss: 0.0993934\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 10\n","[11]\tvalid_0's auc: 0.85492\tvalid_0's binary_logloss: 0.0987768\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[12]\tvalid_0's auc: 0.856687\tvalid_0's binary_logloss: 0.0981662\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 9\n","[13]\tvalid_0's auc: 0.857442\tvalid_0's binary_logloss: 0.0976464\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 10\n","[14]\tvalid_0's auc: 0.858375\tvalid_0's binary_logloss: 0.0971295\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 9\n","[15]\tvalid_0's auc: 0.859213\tvalid_0's binary_logloss: 0.0966882\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[16]\tvalid_0's auc: 0.85882\tvalid_0's binary_logloss: 0.0962935\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 9\n","[17]\tvalid_0's auc: 0.85934\tvalid_0's binary_logloss: 0.0958977\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 9\n","[18]\tvalid_0's auc: 0.859896\tvalid_0's binary_logloss: 0.0954373\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[19]\tvalid_0's auc: 0.860773\tvalid_0's binary_logloss: 0.0950025\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 8\n","[20]\tvalid_0's auc: 0.86065\tvalid_0's binary_logloss: 0.0946223\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 11\n","[21]\tvalid_0's auc: 0.86057\tvalid_0's binary_logloss: 0.0942929\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[22]\tvalid_0's auc: 0.860712\tvalid_0's binary_logloss: 0.0939159\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[23]\tvalid_0's auc: 0.860552\tvalid_0's binary_logloss: 0.093692\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[24]\tvalid_0's auc: 0.860377\tvalid_0's binary_logloss: 0.0934536\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[25]\tvalid_0's auc: 0.860546\tvalid_0's binary_logloss: 0.093093\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[26]\tvalid_0's auc: 0.860704\tvalid_0's binary_logloss: 0.092758\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[27]\tvalid_0's auc: 0.86074\tvalid_0's binary_logloss: 0.092452\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[28]\tvalid_0's auc: 0.860235\tvalid_0's binary_logloss: 0.0922606\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 10\n","[29]\tvalid_0's auc: 0.860508\tvalid_0's binary_logloss: 0.0919375\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[30]\tvalid_0's auc: 0.860722\tvalid_0's binary_logloss: 0.0916796\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 8\n","[31]\tvalid_0's auc: 0.860695\tvalid_0's binary_logloss: 0.091379\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[32]\tvalid_0's auc: 0.860911\tvalid_0's binary_logloss: 0.0910656\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[33]\tvalid_0's auc: 0.860801\tvalid_0's binary_logloss: 0.090893\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 12\n","[34]\tvalid_0's auc: 0.86088\tvalid_0's binary_logloss: 0.0906388\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[35]\tvalid_0's auc: 0.861057\tvalid_0's binary_logloss: 0.0903474\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[36]\tvalid_0's auc: 0.86111\tvalid_0's binary_logloss: 0.0900809\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[37]\tvalid_0's auc: 0.86119\tvalid_0's binary_logloss: 0.0898474\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[38]\tvalid_0's auc: 0.861275\tvalid_0's binary_logloss: 0.0895906\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 7\n","[39]\tvalid_0's auc: 0.861299\tvalid_0's binary_logloss: 0.0893806\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 8\n","[40]\tvalid_0's auc: 0.86132\tvalid_0's binary_logloss: 0.0891379\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[41]\tvalid_0's auc: 0.861631\tvalid_0's binary_logloss: 0.0889078\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[42]\tvalid_0's auc: 0.861941\tvalid_0's binary_logloss: 0.0886691\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[43]\tvalid_0's auc: 0.861917\tvalid_0's binary_logloss: 0.0884542\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[44]\tvalid_0's auc: 0.861932\tvalid_0's binary_logloss: 0.0882354\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[45]\tvalid_0's auc: 0.861963\tvalid_0's binary_logloss: 0.0880186\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[46]\tvalid_0's auc: 0.862164\tvalid_0's binary_logloss: 0.0878106\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[47]\tvalid_0's auc: 0.862274\tvalid_0's binary_logloss: 0.0875977\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[48]\tvalid_0's auc: 0.862314\tvalid_0's binary_logloss: 0.0874062\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 10\n","[49]\tvalid_0's auc: 0.862236\tvalid_0's binary_logloss: 0.0872831\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[50]\tvalid_0's auc: 0.862361\tvalid_0's binary_logloss: 0.0870933\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[51]\tvalid_0's auc: 0.862593\tvalid_0's binary_logloss: 0.0869686\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 7\n","[52]\tvalid_0's auc: 0.862534\tvalid_0's binary_logloss: 0.086781\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 28 and max_depth = 10\n","[53]\tvalid_0's auc: 0.862551\tvalid_0's binary_logloss: 0.0865915\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[54]\tvalid_0's auc: 0.862538\tvalid_0's binary_logloss: 0.0864177\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[55]\tvalid_0's auc: 0.862606\tvalid_0's binary_logloss: 0.086241\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[56]\tvalid_0's auc: 0.862693\tvalid_0's binary_logloss: 0.0860692\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[57]\tvalid_0's auc: 0.862714\tvalid_0's binary_logloss: 0.0859233\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 10\n","[58]\tvalid_0's auc: 0.862801\tvalid_0's binary_logloss: 0.085789\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 8\n","[59]\tvalid_0's auc: 0.862974\tvalid_0's binary_logloss: 0.085625\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[60]\tvalid_0's auc: 0.863048\tvalid_0's binary_logloss: 0.0854853\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[61]\tvalid_0's auc: 0.863135\tvalid_0's binary_logloss: 0.0853276\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 10\n","[62]\tvalid_0's auc: 0.863153\tvalid_0's binary_logloss: 0.0852338\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[63]\tvalid_0's auc: 0.863287\tvalid_0's binary_logloss: 0.0851149\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 7\n","[64]\tvalid_0's auc: 0.863398\tvalid_0's binary_logloss: 0.0849689\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[65]\tvalid_0's auc: 0.863326\tvalid_0's binary_logloss: 0.0848488\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[66]\tvalid_0's auc: 0.863266\tvalid_0's binary_logloss: 0.0847234\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[67]\tvalid_0's auc: 0.863476\tvalid_0's binary_logloss: 0.0845759\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[68]\tvalid_0's auc: 0.86344\tvalid_0's binary_logloss: 0.0844577\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[69]\tvalid_0's auc: 0.863461\tvalid_0's binary_logloss: 0.084335\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[70]\tvalid_0's auc: 0.863412\tvalid_0's binary_logloss: 0.0842035\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[71]\tvalid_0's auc: 0.863479\tvalid_0's binary_logloss: 0.0840657\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[72]\tvalid_0's auc: 0.863418\tvalid_0's binary_logloss: 0.0839489\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[73]\tvalid_0's auc: 0.863458\tvalid_0's binary_logloss: 0.0838602\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[74]\tvalid_0's auc: 0.863492\tvalid_0's binary_logloss: 0.0837439\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 11\n","[75]\tvalid_0's auc: 0.863602\tvalid_0's binary_logloss: 0.0836685\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[76]\tvalid_0's auc: 0.863621\tvalid_0's binary_logloss: 0.0835663\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[77]\tvalid_0's auc: 0.863534\tvalid_0's binary_logloss: 0.0834618\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[78]\tvalid_0's auc: 0.863615\tvalid_0's binary_logloss: 0.0833463\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 6\n","[79]\tvalid_0's auc: 0.863642\tvalid_0's binary_logloss: 0.0832324\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 8\n","[80]\tvalid_0's auc: 0.863714\tvalid_0's binary_logloss: 0.0831188\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 7\n","[81]\tvalid_0's auc: 0.863735\tvalid_0's binary_logloss: 0.0830129\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 12\n","[82]\tvalid_0's auc: 0.863769\tvalid_0's binary_logloss: 0.0829007\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[83]\tvalid_0's auc: 0.86372\tvalid_0's binary_logloss: 0.0828086\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 8\n","[84]\tvalid_0's auc: 0.863718\tvalid_0's binary_logloss: 0.0827146\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[85]\tvalid_0's auc: 0.863634\tvalid_0's binary_logloss: 0.0826302\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[86]\tvalid_0's auc: 0.863645\tvalid_0's binary_logloss: 0.0825291\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[87]\tvalid_0's auc: 0.863692\tvalid_0's binary_logloss: 0.0824335\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[88]\tvalid_0's auc: 0.86363\tvalid_0's binary_logloss: 0.0823495\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 7\n","[89]\tvalid_0's auc: 0.863694\tvalid_0's binary_logloss: 0.0822619\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[90]\tvalid_0's auc: 0.863697\tvalid_0's binary_logloss: 0.0821744\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[91]\tvalid_0's auc: 0.863779\tvalid_0's binary_logloss: 0.0820868\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[92]\tvalid_0's auc: 0.863805\tvalid_0's binary_logloss: 0.0819946\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 8\n","[93]\tvalid_0's auc: 0.863809\tvalid_0's binary_logloss: 0.081922\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[94]\tvalid_0's auc: 0.863855\tvalid_0's binary_logloss: 0.0818459\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[95]\tvalid_0's auc: 0.863892\tvalid_0's binary_logloss: 0.0817692\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 7\n","[96]\tvalid_0's auc: 0.864006\tvalid_0's binary_logloss: 0.0816781\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[97]\tvalid_0's auc: 0.864035\tvalid_0's binary_logloss: 0.0815964\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[98]\tvalid_0's auc: 0.864061\tvalid_0's binary_logloss: 0.0815272\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[99]\tvalid_0's auc: 0.86414\tvalid_0's binary_logloss: 0.0814414\n","[LightGBM] [Debug] Re-bagging, using 83838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[100]\tvalid_0's auc: 0.864196\tvalid_0's binary_logloss: 0.0813599\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's auc: 0.864196\tvalid_0's binary_logloss: 0.0813599\n","[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 2593, number of negative: 117239\n","[LightGBM] [Info] Total Bins 3568\n","[LightGBM] [Info] Number of data: 119832, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.021639 -> initscore=-3.811399\n","[LightGBM] [Info] Start training from score -3.811399\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 10\n","[1]\tvalid_0's auc: 0.802936\tvalid_0's binary_logloss: 0.104031\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 12\n","[2]\tvalid_0's auc: 0.813078\tvalid_0's binary_logloss: 0.103443\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 11\n","[3]\tvalid_0's auc: 0.836273\tvalid_0's binary_logloss: 0.102866\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 10\n","[4]\tvalid_0's auc: 0.839709\tvalid_0's binary_logloss: 0.102282\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 10\n","[5]\tvalid_0's auc: 0.840453\tvalid_0's binary_logloss: 0.101718\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 10\n","[6]\tvalid_0's auc: 0.843663\tvalid_0's binary_logloss: 0.10122\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 9\n","[7]\tvalid_0's auc: 0.845975\tvalid_0's binary_logloss: 0.100726\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[8]\tvalid_0's auc: 0.848459\tvalid_0's binary_logloss: 0.100375\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 13\n","[9]\tvalid_0's auc: 0.848478\tvalid_0's binary_logloss: 0.0998518\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 10\n","[10]\tvalid_0's auc: 0.849178\tvalid_0's binary_logloss: 0.0992995\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 11\n","[11]\tvalid_0's auc: 0.851656\tvalid_0's binary_logloss: 0.0986769\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[12]\tvalid_0's auc: 0.852579\tvalid_0's binary_logloss: 0.0980894\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[13]\tvalid_0's auc: 0.853591\tvalid_0's binary_logloss: 0.0975954\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 9\n","[14]\tvalid_0's auc: 0.854402\tvalid_0's binary_logloss: 0.0970741\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 11\n","[15]\tvalid_0's auc: 0.854346\tvalid_0's binary_logloss: 0.0966365\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[16]\tvalid_0's auc: 0.854571\tvalid_0's binary_logloss: 0.0962381\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 8\n","[17]\tvalid_0's auc: 0.855203\tvalid_0's binary_logloss: 0.0958376\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[18]\tvalid_0's auc: 0.855393\tvalid_0's binary_logloss: 0.0953464\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 8\n","[19]\tvalid_0's auc: 0.855529\tvalid_0's binary_logloss: 0.0949188\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[20]\tvalid_0's auc: 0.855654\tvalid_0's binary_logloss: 0.0945007\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 8\n","[21]\tvalid_0's auc: 0.85559\tvalid_0's binary_logloss: 0.0941511\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 9\n","[22]\tvalid_0's auc: 0.85601\tvalid_0's binary_logloss: 0.0937399\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 10\n","[23]\tvalid_0's auc: 0.856137\tvalid_0's binary_logloss: 0.0935033\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 13\n","[24]\tvalid_0's auc: 0.856356\tvalid_0's binary_logloss: 0.09329\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 9\n","[25]\tvalid_0's auc: 0.856717\tvalid_0's binary_logloss: 0.0929369\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 7\n","[26]\tvalid_0's auc: 0.857092\tvalid_0's binary_logloss: 0.0925533\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[27]\tvalid_0's auc: 0.857128\tvalid_0's binary_logloss: 0.0922281\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 11\n","[28]\tvalid_0's auc: 0.857299\tvalid_0's binary_logloss: 0.0920184\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[29]\tvalid_0's auc: 0.857507\tvalid_0's binary_logloss: 0.0917033\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 12\n","[30]\tvalid_0's auc: 0.85754\tvalid_0's binary_logloss: 0.0914359\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 8\n","[31]\tvalid_0's auc: 0.857839\tvalid_0's binary_logloss: 0.0911348\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[32]\tvalid_0's auc: 0.857933\tvalid_0's binary_logloss: 0.0908152\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[33]\tvalid_0's auc: 0.85809\tvalid_0's binary_logloss: 0.0906241\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[34]\tvalid_0's auc: 0.857899\tvalid_0's binary_logloss: 0.0903814\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 11\n","[35]\tvalid_0's auc: 0.858158\tvalid_0's binary_logloss: 0.0900881\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 10\n","[36]\tvalid_0's auc: 0.857982\tvalid_0's binary_logloss: 0.089826\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[37]\tvalid_0's auc: 0.857944\tvalid_0's binary_logloss: 0.0895926\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[38]\tvalid_0's auc: 0.858024\tvalid_0's binary_logloss: 0.0893238\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[39]\tvalid_0's auc: 0.858171\tvalid_0's binary_logloss: 0.0890961\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[40]\tvalid_0's auc: 0.858297\tvalid_0's binary_logloss: 0.0888373\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 8\n","[41]\tvalid_0's auc: 0.858446\tvalid_0's binary_logloss: 0.0886069\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 22 and max_depth = 7\n","[42]\tvalid_0's auc: 0.858519\tvalid_0's binary_logloss: 0.0883639\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[43]\tvalid_0's auc: 0.858752\tvalid_0's binary_logloss: 0.0881314\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[44]\tvalid_0's auc: 0.8589\tvalid_0's binary_logloss: 0.087913\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[45]\tvalid_0's auc: 0.858878\tvalid_0's binary_logloss: 0.0876989\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[46]\tvalid_0's auc: 0.859095\tvalid_0's binary_logloss: 0.087485\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 7\n","[47]\tvalid_0's auc: 0.859133\tvalid_0's binary_logloss: 0.0872736\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 8\n","[48]\tvalid_0's auc: 0.859348\tvalid_0's binary_logloss: 0.0870638\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[49]\tvalid_0's auc: 0.859585\tvalid_0's binary_logloss: 0.0869299\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 21 and max_depth = 8\n","[50]\tvalid_0's auc: 0.859444\tvalid_0's binary_logloss: 0.0867508\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 11\n","[51]\tvalid_0's auc: 0.859622\tvalid_0's binary_logloss: 0.0866217\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 8\n","[52]\tvalid_0's auc: 0.859654\tvalid_0's binary_logloss: 0.0864388\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 10\n","[53]\tvalid_0's auc: 0.859881\tvalid_0's binary_logloss: 0.0862581\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[54]\tvalid_0's auc: 0.859859\tvalid_0's binary_logloss: 0.0860869\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 8\n","[55]\tvalid_0's auc: 0.859985\tvalid_0's binary_logloss: 0.0859074\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[56]\tvalid_0's auc: 0.859996\tvalid_0's binary_logloss: 0.085737\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 11\n","[57]\tvalid_0's auc: 0.859868\tvalid_0's binary_logloss: 0.0855915\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 11\n","[58]\tvalid_0's auc: 0.859802\tvalid_0's binary_logloss: 0.0854604\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[59]\tvalid_0's auc: 0.8599\tvalid_0's binary_logloss: 0.0852921\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 9\n","[60]\tvalid_0's auc: 0.85997\tvalid_0's binary_logloss: 0.0851551\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[61]\tvalid_0's auc: 0.859971\tvalid_0's binary_logloss: 0.0849999\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 28 and max_depth = 12\n","[62]\tvalid_0's auc: 0.860091\tvalid_0's binary_logloss: 0.0848975\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 28 and max_depth = 9\n","[63]\tvalid_0's auc: 0.86004\tvalid_0's binary_logloss: 0.0847839\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 7\n","[64]\tvalid_0's auc: 0.860072\tvalid_0's binary_logloss: 0.0846348\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 28 and max_depth = 9\n","[65]\tvalid_0's auc: 0.860158\tvalid_0's binary_logloss: 0.0845056\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[66]\tvalid_0's auc: 0.860053\tvalid_0's binary_logloss: 0.0843781\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[67]\tvalid_0's auc: 0.860127\tvalid_0's binary_logloss: 0.084237\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[68]\tvalid_0's auc: 0.860184\tvalid_0's binary_logloss: 0.0841176\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 6\n","[69]\tvalid_0's auc: 0.86019\tvalid_0's binary_logloss: 0.0839882\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[70]\tvalid_0's auc: 0.860246\tvalid_0's binary_logloss: 0.0838558\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[71]\tvalid_0's auc: 0.860175\tvalid_0's binary_logloss: 0.0837235\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 11\n","[72]\tvalid_0's auc: 0.860093\tvalid_0's binary_logloss: 0.0836076\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[73]\tvalid_0's auc: 0.86032\tvalid_0's binary_logloss: 0.0835222\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[74]\tvalid_0's auc: 0.860223\tvalid_0's binary_logloss: 0.0834067\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[75]\tvalid_0's auc: 0.860297\tvalid_0's binary_logloss: 0.0833302\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 10\n","[76]\tvalid_0's auc: 0.860324\tvalid_0's binary_logloss: 0.0832339\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 9\n","[77]\tvalid_0's auc: 0.860306\tvalid_0's binary_logloss: 0.0831297\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 28 and max_depth = 9\n","[78]\tvalid_0's auc: 0.860356\tvalid_0's binary_logloss: 0.0830099\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[79]\tvalid_0's auc: 0.860351\tvalid_0's binary_logloss: 0.0829068\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[80]\tvalid_0's auc: 0.860498\tvalid_0's binary_logloss: 0.0827982\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 6\n","[81]\tvalid_0's auc: 0.860598\tvalid_0's binary_logloss: 0.0826898\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[82]\tvalid_0's auc: 0.860629\tvalid_0's binary_logloss: 0.0825792\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 10\n","[83]\tvalid_0's auc: 0.860674\tvalid_0's binary_logloss: 0.0824854\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[84]\tvalid_0's auc: 0.860646\tvalid_0's binary_logloss: 0.0823978\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[85]\tvalid_0's auc: 0.86063\tvalid_0's binary_logloss: 0.0823132\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 23 and max_depth = 7\n","[86]\tvalid_0's auc: 0.860693\tvalid_0's binary_logloss: 0.0822083\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 7\n","[87]\tvalid_0's auc: 0.860668\tvalid_0's binary_logloss: 0.0821158\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 10\n","[88]\tvalid_0's auc: 0.860559\tvalid_0's binary_logloss: 0.082034\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 7\n","[89]\tvalid_0's auc: 0.86064\tvalid_0's binary_logloss: 0.0819395\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[90]\tvalid_0's auc: 0.860741\tvalid_0's binary_logloss: 0.0818462\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 9\n","[91]\tvalid_0's auc: 0.860736\tvalid_0's binary_logloss: 0.0817621\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 8\n","[92]\tvalid_0's auc: 0.860837\tvalid_0's binary_logloss: 0.0816668\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[93]\tvalid_0's auc: 0.860958\tvalid_0's binary_logloss: 0.081587\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[94]\tvalid_0's auc: 0.860883\tvalid_0's binary_logloss: 0.0815195\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 8\n","[95]\tvalid_0's auc: 0.860905\tvalid_0's binary_logloss: 0.0814357\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 27 and max_depth = 7\n","[96]\tvalid_0's auc: 0.860963\tvalid_0's binary_logloss: 0.0813493\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 7\n","[97]\tvalid_0's auc: 0.860957\tvalid_0's binary_logloss: 0.0812737\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 26 and max_depth = 8\n","[98]\tvalid_0's auc: 0.860858\tvalid_0's binary_logloss: 0.0812115\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 25 and max_depth = 9\n","[99]\tvalid_0's auc: 0.860947\tvalid_0's binary_logloss: 0.0811317\n","[LightGBM] [Debug] Re-bagging, using 83882 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 24 and max_depth = 8\n","[100]\tvalid_0's auc: 0.861012\tvalid_0's binary_logloss: 0.081053\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's auc: 0.861012\tvalid_0's binary_logloss: 0.081053\n"]}]},{"cell_type":"code","source":["!python src/rank/GBDT_classifier.py --task usercf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLoHpfyp4LEw","executionInfo":{"status":"ok","timestamp":1639076386243,"user_tz":-480,"elapsed":34168,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"9ad9df7b-e4dc-408b-92b4-c0c78e7f2bbb"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 1200, number of negative: 114290\n","[LightGBM] [Info] Total Bins 3571\n","[LightGBM] [Info] Number of data: 115490, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010391 -> initscore=-4.556418\n","[LightGBM] [Info] Start training from score -4.556418\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[1]\tvalid_0's auc: 0.817442\tvalid_0's binary_logloss: 0.0577818\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[2]\tvalid_0's auc: 0.832749\tvalid_0's binary_logloss: 0.057539\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 9\n","[3]\tvalid_0's auc: 0.851659\tvalid_0's binary_logloss: 0.0572279\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[4]\tvalid_0's auc: 0.851413\tvalid_0's binary_logloss: 0.05694\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[5]\tvalid_0's auc: 0.856824\tvalid_0's binary_logloss: 0.0567293\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[6]\tvalid_0's auc: 0.85712\tvalid_0's binary_logloss: 0.05645\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[7]\tvalid_0's auc: 0.858793\tvalid_0's binary_logloss: 0.0561771\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[8]\tvalid_0's auc: 0.860551\tvalid_0's binary_logloss: 0.0559423\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 9\n","[9]\tvalid_0's auc: 0.860818\tvalid_0's binary_logloss: 0.0557228\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 9\n","[10]\tvalid_0's auc: 0.859956\tvalid_0's binary_logloss: 0.0554789\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[11]\tvalid_0's auc: 0.859808\tvalid_0's binary_logloss: 0.0552367\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[12]\tvalid_0's auc: 0.860828\tvalid_0's binary_logloss: 0.0550141\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[13]\tvalid_0's auc: 0.86065\tvalid_0's binary_logloss: 0.0548107\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[14]\tvalid_0's auc: 0.860596\tvalid_0's binary_logloss: 0.0546044\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[15]\tvalid_0's auc: 0.861393\tvalid_0's binary_logloss: 0.0544112\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[16]\tvalid_0's auc: 0.861883\tvalid_0's binary_logloss: 0.0542082\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[17]\tvalid_0's auc: 0.861816\tvalid_0's binary_logloss: 0.0540169\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 8\n","[18]\tvalid_0's auc: 0.862174\tvalid_0's binary_logloss: 0.0538422\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[19]\tvalid_0's auc: 0.862244\tvalid_0's binary_logloss: 0.0536802\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 9\n","[20]\tvalid_0's auc: 0.862073\tvalid_0's binary_logloss: 0.0535196\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 9\n","[21]\tvalid_0's auc: 0.862842\tvalid_0's binary_logloss: 0.0533623\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 9\n","[22]\tvalid_0's auc: 0.863118\tvalid_0's binary_logloss: 0.0531923\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[23]\tvalid_0's auc: 0.863469\tvalid_0's binary_logloss: 0.0530372\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[24]\tvalid_0's auc: 0.864333\tvalid_0's binary_logloss: 0.0528964\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[25]\tvalid_0's auc: 0.86446\tvalid_0's binary_logloss: 0.0527324\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[26]\tvalid_0's auc: 0.86479\tvalid_0's binary_logloss: 0.0525635\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[27]\tvalid_0's auc: 0.865394\tvalid_0's binary_logloss: 0.0524045\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[28]\tvalid_0's auc: 0.865523\tvalid_0's binary_logloss: 0.0522571\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[29]\tvalid_0's auc: 0.865507\tvalid_0's binary_logloss: 0.0521063\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[30]\tvalid_0's auc: 0.865425\tvalid_0's binary_logloss: 0.0519992\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[31]\tvalid_0's auc: 0.865861\tvalid_0's binary_logloss: 0.0517751\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[32]\tvalid_0's auc: 0.865813\tvalid_0's binary_logloss: 0.0515816\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[33]\tvalid_0's auc: 0.866565\tvalid_0's binary_logloss: 0.0514207\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[34]\tvalid_0's auc: 0.866745\tvalid_0's binary_logloss: 0.0512821\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 5\n","[35]\tvalid_0's auc: 0.867518\tvalid_0's binary_logloss: 0.0510651\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[36]\tvalid_0's auc: 0.868244\tvalid_0's binary_logloss: 0.0508732\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 9 and max_depth = 7\n","[37]\tvalid_0's auc: 0.868739\tvalid_0's binary_logloss: 0.0507708\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[38]\tvalid_0's auc: 0.868978\tvalid_0's binary_logloss: 0.0505889\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[39]\tvalid_0's auc: 0.869308\tvalid_0's binary_logloss: 0.0504068\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[40]\tvalid_0's auc: 0.869479\tvalid_0's binary_logloss: 0.0502819\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[41]\tvalid_0's auc: 0.869885\tvalid_0's binary_logloss: 0.0501428\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[42]\tvalid_0's auc: 0.86991\tvalid_0's binary_logloss: 0.0499821\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 6\n","[43]\tvalid_0's auc: 0.869907\tvalid_0's binary_logloss: 0.0498268\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 6\n","[44]\tvalid_0's auc: 0.870015\tvalid_0's binary_logloss: 0.0496671\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 5\n","[45]\tvalid_0's auc: 0.870318\tvalid_0's binary_logloss: 0.0494948\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[46]\tvalid_0's auc: 0.870523\tvalid_0's binary_logloss: 0.0493712\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[47]\tvalid_0's auc: 0.870751\tvalid_0's binary_logloss: 0.0492267\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[48]\tvalid_0's auc: 0.870829\tvalid_0's binary_logloss: 0.0491068\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[49]\tvalid_0's auc: 0.871221\tvalid_0's binary_logloss: 0.0489543\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[50]\tvalid_0's auc: 0.871353\tvalid_0's binary_logloss: 0.0488433\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[51]\tvalid_0's auc: 0.871493\tvalid_0's binary_logloss: 0.0487453\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[52]\tvalid_0's auc: 0.8715\tvalid_0's binary_logloss: 0.0486181\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[53]\tvalid_0's auc: 0.871571\tvalid_0's binary_logloss: 0.0484809\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[54]\tvalid_0's auc: 0.871556\tvalid_0's binary_logloss: 0.0483629\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 5\n","[55]\tvalid_0's auc: 0.871648\tvalid_0's binary_logloss: 0.04824\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 6\n","[56]\tvalid_0's auc: 0.871806\tvalid_0's binary_logloss: 0.048122\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[57]\tvalid_0's auc: 0.871894\tvalid_0's binary_logloss: 0.0480434\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[58]\tvalid_0's auc: 0.872277\tvalid_0's binary_logloss: 0.047924\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[59]\tvalid_0's auc: 0.872195\tvalid_0's binary_logloss: 0.0478164\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 9\n","[60]\tvalid_0's auc: 0.872682\tvalid_0's binary_logloss: 0.0476817\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 5\n","[61]\tvalid_0's auc: 0.872982\tvalid_0's binary_logloss: 0.0475558\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[62]\tvalid_0's auc: 0.873117\tvalid_0's binary_logloss: 0.0474714\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[63]\tvalid_0's auc: 0.873236\tvalid_0's binary_logloss: 0.0473628\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[64]\tvalid_0's auc: 0.873243\tvalid_0's binary_logloss: 0.0472657\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[65]\tvalid_0's auc: 0.87343\tvalid_0's binary_logloss: 0.0471774\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[66]\tvalid_0's auc: 0.873577\tvalid_0's binary_logloss: 0.0470706\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[67]\tvalid_0's auc: 0.873757\tvalid_0's binary_logloss: 0.0469662\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[68]\tvalid_0's auc: 0.874013\tvalid_0's binary_logloss: 0.0468686\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[69]\tvalid_0's auc: 0.87407\tvalid_0's binary_logloss: 0.0467869\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[70]\tvalid_0's auc: 0.874243\tvalid_0's binary_logloss: 0.0466726\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[71]\tvalid_0's auc: 0.87434\tvalid_0's binary_logloss: 0.0465967\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[72]\tvalid_0's auc: 0.874302\tvalid_0's binary_logloss: 0.0465021\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[73]\tvalid_0's auc: 0.874712\tvalid_0's binary_logloss: 0.0463928\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 7\n","[74]\tvalid_0's auc: 0.875046\tvalid_0's binary_logloss: 0.0462793\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[75]\tvalid_0's auc: 0.875114\tvalid_0's binary_logloss: 0.0461894\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[76]\tvalid_0's auc: 0.875346\tvalid_0's binary_logloss: 0.0461188\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[77]\tvalid_0's auc: 0.87556\tvalid_0's binary_logloss: 0.0460318\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 6\n","[78]\tvalid_0's auc: 0.87564\tvalid_0's binary_logloss: 0.0459252\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[79]\tvalid_0's auc: 0.875731\tvalid_0's binary_logloss: 0.0458406\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[80]\tvalid_0's auc: 0.875754\tvalid_0's binary_logloss: 0.0457402\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[81]\tvalid_0's auc: 0.876057\tvalid_0's binary_logloss: 0.0456524\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[82]\tvalid_0's auc: 0.876047\tvalid_0's binary_logloss: 0.045579\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[83]\tvalid_0's auc: 0.876131\tvalid_0's binary_logloss: 0.045505\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[84]\tvalid_0's auc: 0.876473\tvalid_0's binary_logloss: 0.0454171\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[85]\tvalid_0's auc: 0.876606\tvalid_0's binary_logloss: 0.045354\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 5\n","[86]\tvalid_0's auc: 0.876793\tvalid_0's binary_logloss: 0.0452574\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[87]\tvalid_0's auc: 0.876986\tvalid_0's binary_logloss: 0.0451892\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[88]\tvalid_0's auc: 0.87696\tvalid_0's binary_logloss: 0.0451249\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 7\n","[89]\tvalid_0's auc: 0.877108\tvalid_0's binary_logloss: 0.0450419\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[90]\tvalid_0's auc: 0.877302\tvalid_0's binary_logloss: 0.0449587\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[91]\tvalid_0's auc: 0.877399\tvalid_0's binary_logloss: 0.0448763\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 5\n","[92]\tvalid_0's auc: 0.877313\tvalid_0's binary_logloss: 0.0448079\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[93]\tvalid_0's auc: 0.87752\tvalid_0's binary_logloss: 0.0447277\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 5\n","[94]\tvalid_0's auc: 0.877856\tvalid_0's binary_logloss: 0.0446537\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 6\n","[95]\tvalid_0's auc: 0.877956\tvalid_0's binary_logloss: 0.0445815\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[96]\tvalid_0's auc: 0.878037\tvalid_0's binary_logloss: 0.0445122\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[97]\tvalid_0's auc: 0.878142\tvalid_0's binary_logloss: 0.0444342\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[98]\tvalid_0's auc: 0.878432\tvalid_0's binary_logloss: 0.0443809\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[99]\tvalid_0's auc: 0.878447\tvalid_0's binary_logloss: 0.0443196\n","[LightGBM] [Debug] Re-bagging, using 80838 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[100]\tvalid_0's auc: 0.878758\tvalid_0's binary_logloss: 0.0442396\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's auc: 0.878758\tvalid_0's binary_logloss: 0.0442396\n","[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 1200, number of negative: 114327\n","[LightGBM] [Info] Total Bins 3570\n","[LightGBM] [Info] Number of data: 115527, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010387 -> initscore=-4.556741\n","[LightGBM] [Info] Start training from score -4.556741\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 8\n","[1]\tvalid_0's auc: 0.801518\tvalid_0's binary_logloss: 0.0578487\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[2]\tvalid_0's auc: 0.824201\tvalid_0's binary_logloss: 0.0576076\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[3]\tvalid_0's auc: 0.84847\tvalid_0's binary_logloss: 0.0572755\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[4]\tvalid_0's auc: 0.850958\tvalid_0's binary_logloss: 0.0569617\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[5]\tvalid_0's auc: 0.856213\tvalid_0's binary_logloss: 0.0567626\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[6]\tvalid_0's auc: 0.85728\tvalid_0's binary_logloss: 0.0564559\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[7]\tvalid_0's auc: 0.857569\tvalid_0's binary_logloss: 0.0561573\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[8]\tvalid_0's auc: 0.860209\tvalid_0's binary_logloss: 0.0559305\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 9\n","[9]\tvalid_0's auc: 0.861387\tvalid_0's binary_logloss: 0.0557124\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 10\n","[10]\tvalid_0's auc: 0.860223\tvalid_0's binary_logloss: 0.0554508\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[11]\tvalid_0's auc: 0.860873\tvalid_0's binary_logloss: 0.0551941\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[12]\tvalid_0's auc: 0.8611\tvalid_0's binary_logloss: 0.05496\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[13]\tvalid_0's auc: 0.860942\tvalid_0's binary_logloss: 0.0547433\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[14]\tvalid_0's auc: 0.861047\tvalid_0's binary_logloss: 0.0545186\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[15]\tvalid_0's auc: 0.862915\tvalid_0's binary_logloss: 0.0543512\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[16]\tvalid_0's auc: 0.863561\tvalid_0's binary_logloss: 0.0541646\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[17]\tvalid_0's auc: 0.863209\tvalid_0's binary_logloss: 0.0539644\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[18]\tvalid_0's auc: 0.862864\tvalid_0's binary_logloss: 0.0537717\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[19]\tvalid_0's auc: 0.862888\tvalid_0's binary_logloss: 0.0535954\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[20]\tvalid_0's auc: 0.862717\tvalid_0's binary_logloss: 0.0534207\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[21]\tvalid_0's auc: 0.863371\tvalid_0's binary_logloss: 0.0532654\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[22]\tvalid_0's auc: 0.863738\tvalid_0's binary_logloss: 0.0530729\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[23]\tvalid_0's auc: 0.864217\tvalid_0's binary_logloss: 0.052923\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[24]\tvalid_0's auc: 0.86481\tvalid_0's binary_logloss: 0.0527936\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[25]\tvalid_0's auc: 0.865667\tvalid_0's binary_logloss: 0.0525457\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[26]\tvalid_0's auc: 0.866234\tvalid_0's binary_logloss: 0.0523444\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[27]\tvalid_0's auc: 0.866944\tvalid_0's binary_logloss: 0.0521655\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[28]\tvalid_0's auc: 0.866929\tvalid_0's binary_logloss: 0.0520332\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 9\n","[29]\tvalid_0's auc: 0.867291\tvalid_0's binary_logloss: 0.0518665\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[30]\tvalid_0's auc: 0.868053\tvalid_0's binary_logloss: 0.0517551\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[31]\tvalid_0's auc: 0.86834\tvalid_0's binary_logloss: 0.0515955\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[32]\tvalid_0's auc: 0.868619\tvalid_0's binary_logloss: 0.0513824\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[33]\tvalid_0's auc: 0.869164\tvalid_0's binary_logloss: 0.0512289\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 9\n","[34]\tvalid_0's auc: 0.869366\tvalid_0's binary_logloss: 0.0510973\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[35]\tvalid_0's auc: 0.869951\tvalid_0's binary_logloss: 0.0508652\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[36]\tvalid_0's auc: 0.870872\tvalid_0's binary_logloss: 0.0506384\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[37]\tvalid_0's auc: 0.870929\tvalid_0's binary_logloss: 0.0505347\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[38]\tvalid_0's auc: 0.871301\tvalid_0's binary_logloss: 0.0503285\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 5\n","[39]\tvalid_0's auc: 0.872066\tvalid_0's binary_logloss: 0.0501115\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[40]\tvalid_0's auc: 0.872463\tvalid_0's binary_logloss: 0.0499249\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[41]\tvalid_0's auc: 0.8726\tvalid_0's binary_logloss: 0.0497754\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[42]\tvalid_0's auc: 0.872789\tvalid_0's binary_logloss: 0.0495857\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[43]\tvalid_0's auc: 0.87313\tvalid_0's binary_logloss: 0.0494077\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[44]\tvalid_0's auc: 0.873647\tvalid_0's binary_logloss: 0.0492814\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[45]\tvalid_0's auc: 0.873733\tvalid_0's binary_logloss: 0.0491557\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 7\n","[46]\tvalid_0's auc: 0.873714\tvalid_0's binary_logloss: 0.0490235\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[47]\tvalid_0's auc: 0.873903\tvalid_0's binary_logloss: 0.0488513\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[48]\tvalid_0's auc: 0.874209\tvalid_0's binary_logloss: 0.0487057\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[49]\tvalid_0's auc: 0.874495\tvalid_0's binary_logloss: 0.048616\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[50]\tvalid_0's auc: 0.874674\tvalid_0's binary_logloss: 0.0485029\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[51]\tvalid_0's auc: 0.874561\tvalid_0's binary_logloss: 0.048414\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[52]\tvalid_0's auc: 0.874624\tvalid_0's binary_logloss: 0.0483165\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[53]\tvalid_0's auc: 0.874778\tvalid_0's binary_logloss: 0.0481492\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[54]\tvalid_0's auc: 0.874848\tvalid_0's binary_logloss: 0.047994\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[55]\tvalid_0's auc: 0.875032\tvalid_0's binary_logloss: 0.0478733\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 6\n","[56]\tvalid_0's auc: 0.875405\tvalid_0's binary_logloss: 0.0477233\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[57]\tvalid_0's auc: 0.87548\tvalid_0's binary_logloss: 0.0476387\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[58]\tvalid_0's auc: 0.875761\tvalid_0's binary_logloss: 0.0475281\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[59]\tvalid_0's auc: 0.875772\tvalid_0's binary_logloss: 0.0474052\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[60]\tvalid_0's auc: 0.875942\tvalid_0's binary_logloss: 0.0473245\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[61]\tvalid_0's auc: 0.876121\tvalid_0's binary_logloss: 0.0471909\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[62]\tvalid_0's auc: 0.876164\tvalid_0's binary_logloss: 0.047115\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[63]\tvalid_0's auc: 0.876661\tvalid_0's binary_logloss: 0.0469861\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[64]\tvalid_0's auc: 0.876643\tvalid_0's binary_logloss: 0.0468735\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[65]\tvalid_0's auc: 0.876827\tvalid_0's binary_logloss: 0.0467936\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[66]\tvalid_0's auc: 0.876902\tvalid_0's binary_logloss: 0.0466994\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[67]\tvalid_0's auc: 0.876938\tvalid_0's binary_logloss: 0.0465874\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[68]\tvalid_0's auc: 0.877049\tvalid_0's binary_logloss: 0.0464887\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[69]\tvalid_0's auc: 0.876927\tvalid_0's binary_logloss: 0.0464029\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[70]\tvalid_0's auc: 0.877041\tvalid_0's binary_logloss: 0.0463002\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[71]\tvalid_0's auc: 0.877048\tvalid_0's binary_logloss: 0.0462197\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[72]\tvalid_0's auc: 0.877327\tvalid_0's binary_logloss: 0.0461093\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[73]\tvalid_0's auc: 0.877465\tvalid_0's binary_logloss: 0.046042\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[74]\tvalid_0's auc: 0.877669\tvalid_0's binary_logloss: 0.0459241\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[75]\tvalid_0's auc: 0.877746\tvalid_0's binary_logloss: 0.0458376\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[76]\tvalid_0's auc: 0.877802\tvalid_0's binary_logloss: 0.0457667\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[77]\tvalid_0's auc: 0.87813\tvalid_0's binary_logloss: 0.0456794\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[78]\tvalid_0's auc: 0.878356\tvalid_0's binary_logloss: 0.0455628\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[79]\tvalid_0's auc: 0.878445\tvalid_0's binary_logloss: 0.0454943\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[80]\tvalid_0's auc: 0.878696\tvalid_0's binary_logloss: 0.0453885\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[81]\tvalid_0's auc: 0.879243\tvalid_0's binary_logloss: 0.0452849\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 5\n","[82]\tvalid_0's auc: 0.879493\tvalid_0's binary_logloss: 0.045177\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[83]\tvalid_0's auc: 0.879617\tvalid_0's binary_logloss: 0.0450913\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[84]\tvalid_0's auc: 0.87977\tvalid_0's binary_logloss: 0.0450054\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[85]\tvalid_0's auc: 0.879764\tvalid_0's binary_logloss: 0.0449519\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 5\n","[86]\tvalid_0's auc: 0.880029\tvalid_0's binary_logloss: 0.0448559\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[87]\tvalid_0's auc: 0.880191\tvalid_0's binary_logloss: 0.0447639\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[88]\tvalid_0's auc: 0.880178\tvalid_0's binary_logloss: 0.0447032\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[89]\tvalid_0's auc: 0.880446\tvalid_0's binary_logloss: 0.0446096\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[90]\tvalid_0's auc: 0.880531\tvalid_0's binary_logloss: 0.044521\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[91]\tvalid_0's auc: 0.880625\tvalid_0's binary_logloss: 0.0444447\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[92]\tvalid_0's auc: 0.88084\tvalid_0's binary_logloss: 0.0443543\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[93]\tvalid_0's auc: 0.88097\tvalid_0's binary_logloss: 0.0442791\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[94]\tvalid_0's auc: 0.881029\tvalid_0's binary_logloss: 0.0442245\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 6\n","[95]\tvalid_0's auc: 0.881326\tvalid_0's binary_logloss: 0.0441537\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[96]\tvalid_0's auc: 0.881401\tvalid_0's binary_logloss: 0.0440808\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[97]\tvalid_0's auc: 0.881363\tvalid_0's binary_logloss: 0.0440125\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[98]\tvalid_0's auc: 0.881363\tvalid_0's binary_logloss: 0.0439674\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[99]\tvalid_0's auc: 0.881386\tvalid_0's binary_logloss: 0.0438917\n","[LightGBM] [Debug] Re-bagging, using 80858 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[100]\tvalid_0's auc: 0.881582\tvalid_0's binary_logloss: 0.0438163\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's auc: 0.881582\tvalid_0's binary_logloss: 0.0438163\n","[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 1200, number of negative: 114241\n","[LightGBM] [Info] Total Bins 3569\n","[LightGBM] [Info] Number of data: 115441, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010395 -> initscore=-4.555989\n","[LightGBM] [Info] Start training from score -4.555989\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[1]\tvalid_0's auc: 0.822085\tvalid_0's binary_logloss: 0.0576898\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[2]\tvalid_0's auc: 0.842537\tvalid_0's binary_logloss: 0.0574576\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 9 and max_depth = 7\n","[3]\tvalid_0's auc: 0.860473\tvalid_0's binary_logloss: 0.0571338\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[4]\tvalid_0's auc: 0.858305\tvalid_0's binary_logloss: 0.0568352\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[5]\tvalid_0's auc: 0.862899\tvalid_0's binary_logloss: 0.0566211\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[6]\tvalid_0's auc: 0.862715\tvalid_0's binary_logloss: 0.0563109\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[7]\tvalid_0's auc: 0.861318\tvalid_0's binary_logloss: 0.0560405\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[8]\tvalid_0's auc: 0.861595\tvalid_0's binary_logloss: 0.0558238\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[9]\tvalid_0's auc: 0.861588\tvalid_0's binary_logloss: 0.0556121\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[10]\tvalid_0's auc: 0.861284\tvalid_0's binary_logloss: 0.0553514\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[11]\tvalid_0's auc: 0.860654\tvalid_0's binary_logloss: 0.0551045\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[12]\tvalid_0's auc: 0.861397\tvalid_0's binary_logloss: 0.0548766\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[13]\tvalid_0's auc: 0.862291\tvalid_0's binary_logloss: 0.0546656\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[14]\tvalid_0's auc: 0.862913\tvalid_0's binary_logloss: 0.0544497\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[15]\tvalid_0's auc: 0.86312\tvalid_0's binary_logloss: 0.0542876\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[16]\tvalid_0's auc: 0.862415\tvalid_0's binary_logloss: 0.054103\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[17]\tvalid_0's auc: 0.862531\tvalid_0's binary_logloss: 0.0539023\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 9\n","[18]\tvalid_0's auc: 0.862518\tvalid_0's binary_logloss: 0.0536897\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[19]\tvalid_0's auc: 0.862787\tvalid_0's binary_logloss: 0.0535066\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 6\n","[20]\tvalid_0's auc: 0.863417\tvalid_0's binary_logloss: 0.0533354\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[21]\tvalid_0's auc: 0.863512\tvalid_0's binary_logloss: 0.0531767\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[22]\tvalid_0's auc: 0.863917\tvalid_0's binary_logloss: 0.0529846\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[23]\tvalid_0's auc: 0.864305\tvalid_0's binary_logloss: 0.0528335\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[24]\tvalid_0's auc: 0.864737\tvalid_0's binary_logloss: 0.0527018\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[25]\tvalid_0's auc: 0.866113\tvalid_0's binary_logloss: 0.0524489\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[26]\tvalid_0's auc: 0.866699\tvalid_0's binary_logloss: 0.052262\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[27]\tvalid_0's auc: 0.867461\tvalid_0's binary_logloss: 0.0520956\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[28]\tvalid_0's auc: 0.86735\tvalid_0's binary_logloss: 0.0519536\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[29]\tvalid_0's auc: 0.867809\tvalid_0's binary_logloss: 0.0517691\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[30]\tvalid_0's auc: 0.867948\tvalid_0's binary_logloss: 0.0516509\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 6\n","[31]\tvalid_0's auc: 0.869437\tvalid_0's binary_logloss: 0.0513974\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[32]\tvalid_0's auc: 0.869828\tvalid_0's binary_logloss: 0.0511866\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[33]\tvalid_0's auc: 0.870221\tvalid_0's binary_logloss: 0.05103\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[34]\tvalid_0's auc: 0.87027\tvalid_0's binary_logloss: 0.050903\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[35]\tvalid_0's auc: 0.870868\tvalid_0's binary_logloss: 0.0506681\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 9 and max_depth = 6\n","[36]\tvalid_0's auc: 0.871169\tvalid_0's binary_logloss: 0.0504714\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[37]\tvalid_0's auc: 0.871548\tvalid_0's binary_logloss: 0.0503582\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[38]\tvalid_0's auc: 0.8721\tvalid_0's binary_logloss: 0.050154\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[39]\tvalid_0's auc: 0.872519\tvalid_0's binary_logloss: 0.0499546\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[40]\tvalid_0's auc: 0.872765\tvalid_0's binary_logloss: 0.0497714\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[41]\tvalid_0's auc: 0.872714\tvalid_0's binary_logloss: 0.0496463\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[42]\tvalid_0's auc: 0.872918\tvalid_0's binary_logloss: 0.0494614\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[43]\tvalid_0's auc: 0.873352\tvalid_0's binary_logloss: 0.0492791\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[44]\tvalid_0's auc: 0.873579\tvalid_0's binary_logloss: 0.0491588\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[45]\tvalid_0's auc: 0.873664\tvalid_0's binary_logloss: 0.049034\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[46]\tvalid_0's auc: 0.873897\tvalid_0's binary_logloss: 0.0488481\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[47]\tvalid_0's auc: 0.874195\tvalid_0's binary_logloss: 0.0486744\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[48]\tvalid_0's auc: 0.874368\tvalid_0's binary_logloss: 0.0485267\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[49]\tvalid_0's auc: 0.874382\tvalid_0's binary_logloss: 0.0484383\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[50]\tvalid_0's auc: 0.874458\tvalid_0's binary_logloss: 0.0483441\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[51]\tvalid_0's auc: 0.874623\tvalid_0's binary_logloss: 0.0482389\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[52]\tvalid_0's auc: 0.874533\tvalid_0's binary_logloss: 0.0481457\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[53]\tvalid_0's auc: 0.874903\tvalid_0's binary_logloss: 0.0479868\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[54]\tvalid_0's auc: 0.874894\tvalid_0's binary_logloss: 0.0478492\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[55]\tvalid_0's auc: 0.874896\tvalid_0's binary_logloss: 0.0477449\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[56]\tvalid_0's auc: 0.875047\tvalid_0's binary_logloss: 0.0476122\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[57]\tvalid_0's auc: 0.875043\tvalid_0's binary_logloss: 0.0475393\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[58]\tvalid_0's auc: 0.875816\tvalid_0's binary_logloss: 0.0474121\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[59]\tvalid_0's auc: 0.876033\tvalid_0's binary_logloss: 0.047282\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[60]\tvalid_0's auc: 0.876061\tvalid_0's binary_logloss: 0.0472018\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[61]\tvalid_0's auc: 0.876028\tvalid_0's binary_logloss: 0.0470783\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[62]\tvalid_0's auc: 0.876168\tvalid_0's binary_logloss: 0.0470077\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[63]\tvalid_0's auc: 0.876507\tvalid_0's binary_logloss: 0.0468814\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[64]\tvalid_0's auc: 0.876476\tvalid_0's binary_logloss: 0.0467733\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[65]\tvalid_0's auc: 0.876568\tvalid_0's binary_logloss: 0.0466854\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[66]\tvalid_0's auc: 0.876799\tvalid_0's binary_logloss: 0.0466025\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[67]\tvalid_0's auc: 0.876683\tvalid_0's binary_logloss: 0.0464994\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 7\n","[68]\tvalid_0's auc: 0.877071\tvalid_0's binary_logloss: 0.0463937\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[69]\tvalid_0's auc: 0.877104\tvalid_0's binary_logloss: 0.0463217\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[70]\tvalid_0's auc: 0.877397\tvalid_0's binary_logloss: 0.0462026\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 9\n","[71]\tvalid_0's auc: 0.877321\tvalid_0's binary_logloss: 0.0461325\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 7\n","[72]\tvalid_0's auc: 0.877648\tvalid_0's binary_logloss: 0.0460298\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[73]\tvalid_0's auc: 0.877857\tvalid_0's binary_logloss: 0.0459166\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 5\n","[74]\tvalid_0's auc: 0.877943\tvalid_0's binary_logloss: 0.0458127\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[75]\tvalid_0's auc: 0.878106\tvalid_0's binary_logloss: 0.0457188\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[76]\tvalid_0's auc: 0.878136\tvalid_0's binary_logloss: 0.0456375\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[77]\tvalid_0's auc: 0.878289\tvalid_0's binary_logloss: 0.0455318\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[78]\tvalid_0's auc: 0.878352\tvalid_0's binary_logloss: 0.0454275\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 9\n","[79]\tvalid_0's auc: 0.878401\tvalid_0's binary_logloss: 0.0453361\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[80]\tvalid_0's auc: 0.878439\tvalid_0's binary_logloss: 0.0452454\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[81]\tvalid_0's auc: 0.878545\tvalid_0's binary_logloss: 0.0451547\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 5\n","[82]\tvalid_0's auc: 0.878684\tvalid_0's binary_logloss: 0.0450568\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[83]\tvalid_0's auc: 0.878813\tvalid_0's binary_logloss: 0.0449713\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[84]\tvalid_0's auc: 0.879164\tvalid_0's binary_logloss: 0.044869\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[85]\tvalid_0's auc: 0.879194\tvalid_0's binary_logloss: 0.0448076\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 5\n","[86]\tvalid_0's auc: 0.879412\tvalid_0's binary_logloss: 0.0447197\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[87]\tvalid_0's auc: 0.879467\tvalid_0's binary_logloss: 0.0446373\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[88]\tvalid_0's auc: 0.879512\tvalid_0's binary_logloss: 0.0445666\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[89]\tvalid_0's auc: 0.879591\tvalid_0's binary_logloss: 0.0444872\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[90]\tvalid_0's auc: 0.879743\tvalid_0's binary_logloss: 0.0444057\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 5\n","[91]\tvalid_0's auc: 0.879797\tvalid_0's binary_logloss: 0.0443291\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[92]\tvalid_0's auc: 0.879905\tvalid_0's binary_logloss: 0.0442521\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[93]\tvalid_0's auc: 0.880055\tvalid_0's binary_logloss: 0.044172\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[94]\tvalid_0's auc: 0.880108\tvalid_0's binary_logloss: 0.0441135\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[95]\tvalid_0's auc: 0.88012\tvalid_0's binary_logloss: 0.0440468\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[96]\tvalid_0's auc: 0.880157\tvalid_0's binary_logloss: 0.0439814\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[97]\tvalid_0's auc: 0.880396\tvalid_0's binary_logloss: 0.0439147\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[98]\tvalid_0's auc: 0.880417\tvalid_0's binary_logloss: 0.0438581\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[99]\tvalid_0's auc: 0.88049\tvalid_0's binary_logloss: 0.043786\n","[LightGBM] [Debug] Re-bagging, using 80805 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[100]\tvalid_0's auc: 0.880655\tvalid_0's binary_logloss: 0.0437157\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's auc: 0.880655\tvalid_0's binary_logloss: 0.0437157\n","[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 1200, number of negative: 113975\n","[LightGBM] [Info] Total Bins 3570\n","[LightGBM] [Info] Number of data: 115175, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010419 -> initscore=-4.553658\n","[LightGBM] [Info] Start training from score -4.553658\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[1]\tvalid_0's auc: 0.799062\tvalid_0's binary_logloss: 0.0572705\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[2]\tvalid_0's auc: 0.813895\tvalid_0's binary_logloss: 0.0570561\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 9\n","[3]\tvalid_0's auc: 0.8397\tvalid_0's binary_logloss: 0.0567584\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[4]\tvalid_0's auc: 0.841387\tvalid_0's binary_logloss: 0.0564866\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[5]\tvalid_0's auc: 0.847077\tvalid_0's binary_logloss: 0.0562876\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[6]\tvalid_0's auc: 0.847939\tvalid_0's binary_logloss: 0.0559982\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[7]\tvalid_0's auc: 0.847947\tvalid_0's binary_logloss: 0.0557429\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[8]\tvalid_0's auc: 0.849249\tvalid_0's binary_logloss: 0.055533\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[9]\tvalid_0's auc: 0.849569\tvalid_0's binary_logloss: 0.0553304\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 9\n","[10]\tvalid_0's auc: 0.849589\tvalid_0's binary_logloss: 0.0550995\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[11]\tvalid_0's auc: 0.849537\tvalid_0's binary_logloss: 0.0548893\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[12]\tvalid_0's auc: 0.848734\tvalid_0's binary_logloss: 0.054682\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[13]\tvalid_0's auc: 0.849042\tvalid_0's binary_logloss: 0.0544862\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[14]\tvalid_0's auc: 0.849806\tvalid_0's binary_logloss: 0.054301\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[15]\tvalid_0's auc: 0.85073\tvalid_0's binary_logloss: 0.0541411\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[16]\tvalid_0's auc: 0.850565\tvalid_0's binary_logloss: 0.0539743\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[17]\tvalid_0's auc: 0.850406\tvalid_0's binary_logloss: 0.0537965\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 8\n","[18]\tvalid_0's auc: 0.850512\tvalid_0's binary_logloss: 0.0536073\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[19]\tvalid_0's auc: 0.850963\tvalid_0's binary_logloss: 0.0534278\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[20]\tvalid_0's auc: 0.851559\tvalid_0's binary_logloss: 0.0532489\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[21]\tvalid_0's auc: 0.852087\tvalid_0's binary_logloss: 0.0531004\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[22]\tvalid_0's auc: 0.852621\tvalid_0's binary_logloss: 0.0529156\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[23]\tvalid_0's auc: 0.853198\tvalid_0's binary_logloss: 0.0527713\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 8\n","[24]\tvalid_0's auc: 0.854922\tvalid_0's binary_logloss: 0.052657\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[25]\tvalid_0's auc: 0.855372\tvalid_0's binary_logloss: 0.0524283\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[26]\tvalid_0's auc: 0.855284\tvalid_0's binary_logloss: 0.0522783\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[27]\tvalid_0's auc: 0.854758\tvalid_0's binary_logloss: 0.0521347\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[28]\tvalid_0's auc: 0.854893\tvalid_0's binary_logloss: 0.0520099\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[29]\tvalid_0's auc: 0.855007\tvalid_0's binary_logloss: 0.0518442\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 10\n","[30]\tvalid_0's auc: 0.85484\tvalid_0's binary_logloss: 0.0517431\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[31]\tvalid_0's auc: 0.855053\tvalid_0's binary_logloss: 0.0515322\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[32]\tvalid_0's auc: 0.855716\tvalid_0's binary_logloss: 0.0513362\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[33]\tvalid_0's auc: 0.856942\tvalid_0's binary_logloss: 0.0511879\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[34]\tvalid_0's auc: 0.857758\tvalid_0's binary_logloss: 0.0510593\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[35]\tvalid_0's auc: 0.858428\tvalid_0's binary_logloss: 0.0508455\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[36]\tvalid_0's auc: 0.858777\tvalid_0's binary_logloss: 0.0506448\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[37]\tvalid_0's auc: 0.859005\tvalid_0's binary_logloss: 0.0505456\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[38]\tvalid_0's auc: 0.859413\tvalid_0's binary_logloss: 0.0503532\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[39]\tvalid_0's auc: 0.859825\tvalid_0's binary_logloss: 0.0501671\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[40]\tvalid_0's auc: 0.860035\tvalid_0's binary_logloss: 0.0499994\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[41]\tvalid_0's auc: 0.859878\tvalid_0's binary_logloss: 0.049876\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[42]\tvalid_0's auc: 0.86004\tvalid_0's binary_logloss: 0.0497065\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[43]\tvalid_0's auc: 0.860272\tvalid_0's binary_logloss: 0.0495399\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[44]\tvalid_0's auc: 0.860571\tvalid_0's binary_logloss: 0.0494311\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[45]\tvalid_0's auc: 0.860949\tvalid_0's binary_logloss: 0.0492761\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[46]\tvalid_0's auc: 0.861241\tvalid_0's binary_logloss: 0.0491134\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 7\n","[47]\tvalid_0's auc: 0.861369\tvalid_0's binary_logloss: 0.0489702\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 9\n","[48]\tvalid_0's auc: 0.861345\tvalid_0's binary_logloss: 0.0488797\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[49]\tvalid_0's auc: 0.86232\tvalid_0's binary_logloss: 0.0487927\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[50]\tvalid_0's auc: 0.862745\tvalid_0's binary_logloss: 0.0487132\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[51]\tvalid_0's auc: 0.863082\tvalid_0's binary_logloss: 0.0486242\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[52]\tvalid_0's auc: 0.863027\tvalid_0's binary_logloss: 0.0485462\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[53]\tvalid_0's auc: 0.863137\tvalid_0's binary_logloss: 0.0484155\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[54]\tvalid_0's auc: 0.863241\tvalid_0's binary_logloss: 0.0482945\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 5\n","[55]\tvalid_0's auc: 0.863516\tvalid_0's binary_logloss: 0.0482023\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 5\n","[56]\tvalid_0's auc: 0.863681\tvalid_0's binary_logloss: 0.048078\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 8\n","[57]\tvalid_0's auc: 0.863815\tvalid_0's binary_logloss: 0.0479804\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[58]\tvalid_0's auc: 0.864072\tvalid_0's binary_logloss: 0.0478755\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[59]\tvalid_0's auc: 0.863944\tvalid_0's binary_logloss: 0.047774\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[60]\tvalid_0's auc: 0.864554\tvalid_0's binary_logloss: 0.0476926\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[61]\tvalid_0's auc: 0.864739\tvalid_0's binary_logloss: 0.0475813\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[62]\tvalid_0's auc: 0.864761\tvalid_0's binary_logloss: 0.0474972\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[63]\tvalid_0's auc: 0.865125\tvalid_0's binary_logloss: 0.047387\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[64]\tvalid_0's auc: 0.865308\tvalid_0's binary_logloss: 0.047293\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[65]\tvalid_0's auc: 0.865312\tvalid_0's binary_logloss: 0.0471908\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 7\n","[66]\tvalid_0's auc: 0.865922\tvalid_0's binary_logloss: 0.047086\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[67]\tvalid_0's auc: 0.865943\tvalid_0's binary_logloss: 0.0469906\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[68]\tvalid_0's auc: 0.865803\tvalid_0's binary_logloss: 0.0469298\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 7\n","[69]\tvalid_0's auc: 0.865825\tvalid_0's binary_logloss: 0.0468655\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 5\n","[70]\tvalid_0's auc: 0.865849\tvalid_0's binary_logloss: 0.0467527\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[71]\tvalid_0's auc: 0.865647\tvalid_0's binary_logloss: 0.0466783\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[72]\tvalid_0's auc: 0.86604\tvalid_0's binary_logloss: 0.0465685\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 7\n","[73]\tvalid_0's auc: 0.866135\tvalid_0's binary_logloss: 0.0464701\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[74]\tvalid_0's auc: 0.866212\tvalid_0's binary_logloss: 0.0463704\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[75]\tvalid_0's auc: 0.866273\tvalid_0's binary_logloss: 0.0462872\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[76]\tvalid_0's auc: 0.866442\tvalid_0's binary_logloss: 0.0462001\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[77]\tvalid_0's auc: 0.866709\tvalid_0's binary_logloss: 0.0461153\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 5\n","[78]\tvalid_0's auc: 0.86671\tvalid_0's binary_logloss: 0.0460229\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[79]\tvalid_0's auc: 0.866683\tvalid_0's binary_logloss: 0.0459475\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 5\n","[80]\tvalid_0's auc: 0.866673\tvalid_0's binary_logloss: 0.04586\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 5\n","[81]\tvalid_0's auc: 0.866903\tvalid_0's binary_logloss: 0.0457746\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 5\n","[82]\tvalid_0's auc: 0.867011\tvalid_0's binary_logloss: 0.0456827\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[83]\tvalid_0's auc: 0.867309\tvalid_0's binary_logloss: 0.0456011\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[84]\tvalid_0's auc: 0.867699\tvalid_0's binary_logloss: 0.0455117\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[85]\tvalid_0's auc: 0.867538\tvalid_0's binary_logloss: 0.0454608\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[86]\tvalid_0's auc: 0.867503\tvalid_0's binary_logloss: 0.045386\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[87]\tvalid_0's auc: 0.867524\tvalid_0's binary_logloss: 0.045312\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 9\n","[88]\tvalid_0's auc: 0.867481\tvalid_0's binary_logloss: 0.0452563\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[89]\tvalid_0's auc: 0.86762\tvalid_0's binary_logloss: 0.0451827\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 7\n","[90]\tvalid_0's auc: 0.867768\tvalid_0's binary_logloss: 0.0451095\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 7\n","[91]\tvalid_0's auc: 0.86776\tvalid_0's binary_logloss: 0.0450391\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[92]\tvalid_0's auc: 0.867971\tvalid_0's binary_logloss: 0.0449646\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[93]\tvalid_0's auc: 0.868091\tvalid_0's binary_logloss: 0.044888\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[94]\tvalid_0's auc: 0.868179\tvalid_0's binary_logloss: 0.0448219\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 5\n","[95]\tvalid_0's auc: 0.868232\tvalid_0's binary_logloss: 0.0447674\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[96]\tvalid_0's auc: 0.868092\tvalid_0's binary_logloss: 0.0447082\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[97]\tvalid_0's auc: 0.868374\tvalid_0's binary_logloss: 0.0446473\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 9\n","[98]\tvalid_0's auc: 0.868321\tvalid_0's binary_logloss: 0.0446034\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 7\n","[99]\tvalid_0's auc: 0.868457\tvalid_0's binary_logloss: 0.0445364\n","[LightGBM] [Debug] Re-bagging, using 80618 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[100]\tvalid_0's auc: 0.868426\tvalid_0's binary_logloss: 0.0444882\n","Did not meet early stopping. Best iteration is:\n","[99]\tvalid_0's auc: 0.868457\tvalid_0's binary_logloss: 0.0445364\n","[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 1200, number of negative: 114131\n","[LightGBM] [Info] Total Bins 3570\n","[LightGBM] [Info] Number of data: 115331, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010405 -> initscore=-4.555025\n","[LightGBM] [Info] Start training from score -4.555025\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[1]\tvalid_0's auc: 0.804867\tvalid_0's binary_logloss: 0.0575247\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 9\n","[2]\tvalid_0's auc: 0.827092\tvalid_0's binary_logloss: 0.0572941\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 9\n","[3]\tvalid_0's auc: 0.849332\tvalid_0's binary_logloss: 0.0569631\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[4]\tvalid_0's auc: 0.854242\tvalid_0's binary_logloss: 0.0566548\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[5]\tvalid_0's auc: 0.858407\tvalid_0's binary_logloss: 0.0564482\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[6]\tvalid_0's auc: 0.85907\tvalid_0's binary_logloss: 0.0561329\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 6\n","[7]\tvalid_0's auc: 0.857414\tvalid_0's binary_logloss: 0.0558531\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 8\n","[8]\tvalid_0's auc: 0.858559\tvalid_0's binary_logloss: 0.0556461\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[9]\tvalid_0's auc: 0.859604\tvalid_0's binary_logloss: 0.0554301\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[10]\tvalid_0's auc: 0.859463\tvalid_0's binary_logloss: 0.0551733\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[11]\tvalid_0's auc: 0.859068\tvalid_0's binary_logloss: 0.0549359\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 10\n","[12]\tvalid_0's auc: 0.858134\tvalid_0's binary_logloss: 0.0547153\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[13]\tvalid_0's auc: 0.85719\tvalid_0's binary_logloss: 0.0545063\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[14]\tvalid_0's auc: 0.856192\tvalid_0's binary_logloss: 0.0542944\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[15]\tvalid_0's auc: 0.857131\tvalid_0's binary_logloss: 0.0541342\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[16]\tvalid_0's auc: 0.857209\tvalid_0's binary_logloss: 0.0539591\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[17]\tvalid_0's auc: 0.857939\tvalid_0's binary_logloss: 0.0537724\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[18]\tvalid_0's auc: 0.857389\tvalid_0's binary_logloss: 0.0535718\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[19]\tvalid_0's auc: 0.856827\tvalid_0's binary_logloss: 0.0533841\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 8\n","[20]\tvalid_0's auc: 0.857276\tvalid_0's binary_logloss: 0.0532119\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[21]\tvalid_0's auc: 0.858068\tvalid_0's binary_logloss: 0.0530448\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[22]\tvalid_0's auc: 0.859576\tvalid_0's binary_logloss: 0.0527734\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 6\n","[23]\tvalid_0's auc: 0.859548\tvalid_0's binary_logloss: 0.0526338\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[24]\tvalid_0's auc: 0.859792\tvalid_0's binary_logloss: 0.0525079\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[25]\tvalid_0's auc: 0.860468\tvalid_0's binary_logloss: 0.0522676\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 6\n","[26]\tvalid_0's auc: 0.860398\tvalid_0's binary_logloss: 0.0520964\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 9\n","[27]\tvalid_0's auc: 0.860645\tvalid_0's binary_logloss: 0.0519245\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[28]\tvalid_0's auc: 0.860512\tvalid_0's binary_logloss: 0.0517923\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[29]\tvalid_0's auc: 0.860578\tvalid_0's binary_logloss: 0.0516153\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[30]\tvalid_0's auc: 0.860324\tvalid_0's binary_logloss: 0.0515063\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[31]\tvalid_0's auc: 0.861157\tvalid_0's binary_logloss: 0.0512639\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[32]\tvalid_0's auc: 0.861508\tvalid_0's binary_logloss: 0.0510621\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[33]\tvalid_0's auc: 0.862491\tvalid_0's binary_logloss: 0.0509108\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[34]\tvalid_0's auc: 0.862899\tvalid_0's binary_logloss: 0.0507726\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[35]\tvalid_0's auc: 0.863338\tvalid_0's binary_logloss: 0.0505472\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[36]\tvalid_0's auc: 0.863372\tvalid_0's binary_logloss: 0.0503445\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[37]\tvalid_0's auc: 0.86354\tvalid_0's binary_logloss: 0.0502403\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[38]\tvalid_0's auc: 0.863678\tvalid_0's binary_logloss: 0.0500406\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[39]\tvalid_0's auc: 0.863866\tvalid_0's binary_logloss: 0.0498442\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 9\n","[40]\tvalid_0's auc: 0.863862\tvalid_0's binary_logloss: 0.0496698\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 7\n","[41]\tvalid_0's auc: 0.863997\tvalid_0's binary_logloss: 0.049538\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[42]\tvalid_0's auc: 0.864147\tvalid_0's binary_logloss: 0.0493643\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[43]\tvalid_0's auc: 0.864169\tvalid_0's binary_logloss: 0.0491986\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[44]\tvalid_0's auc: 0.864263\tvalid_0's binary_logloss: 0.0490852\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 8\n","[45]\tvalid_0's auc: 0.864236\tvalid_0's binary_logloss: 0.0489203\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 5\n","[46]\tvalid_0's auc: 0.864397\tvalid_0's binary_logloss: 0.0487487\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[47]\tvalid_0's auc: 0.864471\tvalid_0's binary_logloss: 0.0486029\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 10\n","[48]\tvalid_0's auc: 0.86458\tvalid_0's binary_logloss: 0.0485035\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[49]\tvalid_0's auc: 0.86426\tvalid_0's binary_logloss: 0.048423\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 8\n","[50]\tvalid_0's auc: 0.864215\tvalid_0's binary_logloss: 0.0483364\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[51]\tvalid_0's auc: 0.864581\tvalid_0's binary_logloss: 0.0482481\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[52]\tvalid_0's auc: 0.864501\tvalid_0's binary_logloss: 0.0481594\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 10 and max_depth = 6\n","[53]\tvalid_0's auc: 0.864448\tvalid_0's binary_logloss: 0.0480236\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[54]\tvalid_0's auc: 0.864365\tvalid_0's binary_logloss: 0.0478982\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[55]\tvalid_0's auc: 0.86448\tvalid_0's binary_logloss: 0.0477706\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[56]\tvalid_0's auc: 0.864523\tvalid_0's binary_logloss: 0.0476566\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[57]\tvalid_0's auc: 0.864434\tvalid_0's binary_logloss: 0.0475647\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[58]\tvalid_0's auc: 0.865082\tvalid_0's binary_logloss: 0.0474469\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[59]\tvalid_0's auc: 0.865274\tvalid_0's binary_logloss: 0.0473214\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[60]\tvalid_0's auc: 0.865629\tvalid_0's binary_logloss: 0.0471999\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[61]\tvalid_0's auc: 0.865561\tvalid_0's binary_logloss: 0.0470866\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[62]\tvalid_0's auc: 0.865759\tvalid_0's binary_logloss: 0.0470081\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[63]\tvalid_0's auc: 0.865677\tvalid_0's binary_logloss: 0.0469094\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[64]\tvalid_0's auc: 0.865586\tvalid_0's binary_logloss: 0.0468091\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[65]\tvalid_0's auc: 0.866035\tvalid_0's binary_logloss: 0.0466983\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[66]\tvalid_0's auc: 0.866152\tvalid_0's binary_logloss: 0.0465923\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[67]\tvalid_0's auc: 0.866147\tvalid_0's binary_logloss: 0.0465179\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[68]\tvalid_0's auc: 0.866359\tvalid_0's binary_logloss: 0.0464395\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[69]\tvalid_0's auc: 0.866728\tvalid_0's binary_logloss: 0.0463526\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[70]\tvalid_0's auc: 0.866651\tvalid_0's binary_logloss: 0.0462589\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[71]\tvalid_0's auc: 0.866793\tvalid_0's binary_logloss: 0.0461532\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 5\n","[72]\tvalid_0's auc: 0.867179\tvalid_0's binary_logloss: 0.0460353\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[73]\tvalid_0's auc: 0.86748\tvalid_0's binary_logloss: 0.0459278\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[74]\tvalid_0's auc: 0.867561\tvalid_0's binary_logloss: 0.0458229\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 6\n","[75]\tvalid_0's auc: 0.867902\tvalid_0's binary_logloss: 0.0457198\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[76]\tvalid_0's auc: 0.86815\tvalid_0's binary_logloss: 0.0456418\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 5\n","[77]\tvalid_0's auc: 0.868615\tvalid_0's binary_logloss: 0.0455351\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[78]\tvalid_0's auc: 0.868786\tvalid_0's binary_logloss: 0.045433\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 7\n","[79]\tvalid_0's auc: 0.869081\tvalid_0's binary_logloss: 0.0453324\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[80]\tvalid_0's auc: 0.869152\tvalid_0's binary_logloss: 0.0452441\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[81]\tvalid_0's auc: 0.869172\tvalid_0's binary_logloss: 0.0451605\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 5\n","[82]\tvalid_0's auc: 0.869362\tvalid_0's binary_logloss: 0.0450621\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 13 and max_depth = 6\n","[83]\tvalid_0's auc: 0.869467\tvalid_0's binary_logloss: 0.044981\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 7\n","[84]\tvalid_0's auc: 0.869717\tvalid_0's binary_logloss: 0.044901\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[85]\tvalid_0's auc: 0.869753\tvalid_0's binary_logloss: 0.0448369\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[86]\tvalid_0's auc: 0.87001\tvalid_0's binary_logloss: 0.0447528\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[87]\tvalid_0's auc: 0.869919\tvalid_0's binary_logloss: 0.0446788\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[88]\tvalid_0's auc: 0.870009\tvalid_0's binary_logloss: 0.0446096\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 5\n","[89]\tvalid_0's auc: 0.870202\tvalid_0's binary_logloss: 0.0445293\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 14 and max_depth = 5\n","[90]\tvalid_0's auc: 0.870268\tvalid_0's binary_logloss: 0.0444476\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[91]\tvalid_0's auc: 0.870286\tvalid_0's binary_logloss: 0.0443748\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 5\n","[92]\tvalid_0's auc: 0.87042\tvalid_0's binary_logloss: 0.0443008\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 6\n","[93]\tvalid_0's auc: 0.870656\tvalid_0's binary_logloss: 0.0442255\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[94]\tvalid_0's auc: 0.870871\tvalid_0's binary_logloss: 0.044147\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[95]\tvalid_0's auc: 0.870847\tvalid_0's binary_logloss: 0.0440772\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 11 and max_depth = 5\n","[96]\tvalid_0's auc: 0.870964\tvalid_0's binary_logloss: 0.0440109\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 7\n","[97]\tvalid_0's auc: 0.871193\tvalid_0's binary_logloss: 0.0439486\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 8\n","[98]\tvalid_0's auc: 0.871295\tvalid_0's binary_logloss: 0.0439005\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 5\n","[99]\tvalid_0's auc: 0.871332\tvalid_0's binary_logloss: 0.0438241\n","[LightGBM] [Debug] Re-bagging, using 80727 data to train\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Debug] Trained a tree with leaves = 12 and max_depth = 6\n","[100]\tvalid_0's auc: 0.871377\tvalid_0's binary_logloss: 0.0437604\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's auc: 0.871377\tvalid_0's binary_logloss: 0.0437604\n"]}]},{"cell_type":"code","source":["!python src/rank/GBDT_classifier.py --task srgnn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kNC9O5dw4MZk","executionInfo":{"status":"ok","timestamp":1639076429811,"user_tz":-480,"elapsed":43572,"user":{"displayName":"王乾潮","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14757222036545009683"}},"outputId":"566c7676-6dbb-4fab-9b9b-1bdbe3bd2b58"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 4895, number of negative: 106574\n","[LightGBM] [Info] Total Bins 3568\n","[LightGBM] [Info] Number of data: 111469, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.043914 -> initscore=-3.080625\n","[LightGBM] [Info] Start training from score -3.080625\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[1]\tvalid_0's binary_logloss: 0.177893\tvalid_0's auc: 0.816192\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[2]\tvalid_0's binary_logloss: 0.176605\tvalid_0's auc: 0.825645\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[3]\tvalid_0's binary_logloss: 0.175852\tvalid_0's auc: 0.8276\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n","[4]\tvalid_0's binary_logloss: 0.17474\tvalid_0's auc: 0.831911\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[5]\tvalid_0's binary_logloss: 0.173611\tvalid_0's auc: 0.833482\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[6]\tvalid_0's binary_logloss: 0.172967\tvalid_0's auc: 0.832946\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[7]\tvalid_0's binary_logloss: 0.172307\tvalid_0's auc: 0.834261\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[8]\tvalid_0's binary_logloss: 0.171746\tvalid_0's auc: 0.833899\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[9]\tvalid_0's binary_logloss: 0.170751\tvalid_0's auc: 0.834765\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[10]\tvalid_0's binary_logloss: 0.169848\tvalid_0's auc: 0.835757\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[11]\tvalid_0's binary_logloss: 0.169255\tvalid_0's auc: 0.834903\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[12]\tvalid_0's binary_logloss: 0.168432\tvalid_0's auc: 0.835726\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[13]\tvalid_0's binary_logloss: 0.167636\tvalid_0's auc: 0.836107\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[14]\tvalid_0's binary_logloss: 0.167144\tvalid_0's auc: 0.835654\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[15]\tvalid_0's binary_logloss: 0.16639\tvalid_0's auc: 0.835832\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[16]\tvalid_0's binary_logloss: 0.165671\tvalid_0's auc: 0.835895\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[17]\tvalid_0's binary_logloss: 0.165232\tvalid_0's auc: 0.835734\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[18]\tvalid_0's binary_logloss: 0.164752\tvalid_0's auc: 0.835595\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[19]\tvalid_0's binary_logloss: 0.164093\tvalid_0's auc: 0.836158\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[20]\tvalid_0's binary_logloss: 0.163444\tvalid_0's auc: 0.836478\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[21]\tvalid_0's binary_logloss: 0.162828\tvalid_0's auc: 0.836611\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[22]\tvalid_0's binary_logloss: 0.162237\tvalid_0's auc: 0.836885\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[23]\tvalid_0's binary_logloss: 0.161833\tvalid_0's auc: 0.83687\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[24]\tvalid_0's binary_logloss: 0.161464\tvalid_0's auc: 0.836654\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[25]\tvalid_0's binary_logloss: 0.161089\tvalid_0's auc: 0.836547\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[26]\tvalid_0's binary_logloss: 0.160568\tvalid_0's auc: 0.836681\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 6\n","[27]\tvalid_0's binary_logloss: 0.160223\tvalid_0's auc: 0.836585\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[28]\tvalid_0's binary_logloss: 0.159879\tvalid_0's auc: 0.83649\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n","[29]\tvalid_0's binary_logloss: 0.159389\tvalid_0's auc: 0.836846\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[30]\tvalid_0's binary_logloss: 0.158929\tvalid_0's auc: 0.837032\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[31]\tvalid_0's binary_logloss: 0.158466\tvalid_0's auc: 0.837145\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[32]\tvalid_0's binary_logloss: 0.157995\tvalid_0's auc: 0.837304\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[33]\tvalid_0's binary_logloss: 0.157704\tvalid_0's auc: 0.837236\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[34]\tvalid_0's binary_logloss: 0.157273\tvalid_0's auc: 0.837443\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[35]\tvalid_0's binary_logloss: 0.15697\tvalid_0's auc: 0.837291\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[36]\tvalid_0's binary_logloss: 0.156677\tvalid_0's auc: 0.837157\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[37]\tvalid_0's binary_logloss: 0.156279\tvalid_0's auc: 0.837172\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[38]\tvalid_0's binary_logloss: 0.155886\tvalid_0's auc: 0.837351\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[39]\tvalid_0's binary_logloss: 0.155613\tvalid_0's auc: 0.837347\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[40]\tvalid_0's binary_logloss: 0.155231\tvalid_0's auc: 0.837496\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[41]\tvalid_0's binary_logloss: 0.154866\tvalid_0's auc: 0.837641\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[42]\tvalid_0's binary_logloss: 0.154601\tvalid_0's auc: 0.837645\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[43]\tvalid_0's binary_logloss: 0.154256\tvalid_0's auc: 0.837642\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[44]\tvalid_0's binary_logloss: 0.153906\tvalid_0's auc: 0.837707\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[45]\tvalid_0's binary_logloss: 0.153651\tvalid_0's auc: 0.837635\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[46]\tvalid_0's binary_logloss: 0.153403\tvalid_0's auc: 0.837651\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[47]\tvalid_0's binary_logloss: 0.153078\tvalid_0's auc: 0.837769\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[48]\tvalid_0's binary_logloss: 0.152844\tvalid_0's auc: 0.837708\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[49]\tvalid_0's binary_logloss: 0.152627\tvalid_0's auc: 0.83765\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[50]\tvalid_0's binary_logloss: 0.15232\tvalid_0's auc: 0.83784\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[51]\tvalid_0's binary_logloss: 0.152125\tvalid_0's auc: 0.837752\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[52]\tvalid_0's binary_logloss: 0.151813\tvalid_0's auc: 0.837958\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[53]\tvalid_0's binary_logloss: 0.151594\tvalid_0's auc: 0.837893\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[54]\tvalid_0's binary_logloss: 0.151303\tvalid_0's auc: 0.837966\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[55]\tvalid_0's binary_logloss: 0.151095\tvalid_0's auc: 0.838033\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[56]\tvalid_0's binary_logloss: 0.150821\tvalid_0's auc: 0.838092\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[57]\tvalid_0's binary_logloss: 0.150537\tvalid_0's auc: 0.838228\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[58]\tvalid_0's binary_logloss: 0.150262\tvalid_0's auc: 0.838417\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[59]\tvalid_0's binary_logloss: 0.15\tvalid_0's auc: 0.838537\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[60]\tvalid_0's binary_logloss: 0.149744\tvalid_0's auc: 0.838561\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[61]\tvalid_0's binary_logloss: 0.149567\tvalid_0's auc: 0.838547\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[62]\tvalid_0's binary_logloss: 0.149398\tvalid_0's auc: 0.83856\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[63]\tvalid_0's binary_logloss: 0.149215\tvalid_0's auc: 0.838676\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[64]\tvalid_0's binary_logloss: 0.149042\tvalid_0's auc: 0.838674\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[65]\tvalid_0's binary_logloss: 0.148813\tvalid_0's auc: 0.838667\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[66]\tvalid_0's binary_logloss: 0.148584\tvalid_0's auc: 0.838752\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[67]\tvalid_0's binary_logloss: 0.148358\tvalid_0's auc: 0.838883\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[68]\tvalid_0's binary_logloss: 0.148133\tvalid_0's auc: 0.838972\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[69]\tvalid_0's binary_logloss: 0.147923\tvalid_0's auc: 0.839051\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[70]\tvalid_0's binary_logloss: 0.147763\tvalid_0's auc: 0.838955\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[71]\tvalid_0's binary_logloss: 0.14754\tvalid_0's auc: 0.839068\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[72]\tvalid_0's binary_logloss: 0.147323\tvalid_0's auc: 0.839207\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[73]\tvalid_0's binary_logloss: 0.147171\tvalid_0's auc: 0.839215\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[74]\tvalid_0's binary_logloss: 0.147015\tvalid_0's auc: 0.839287\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[75]\tvalid_0's binary_logloss: 0.146891\tvalid_0's auc: 0.839253\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[76]\tvalid_0's binary_logloss: 0.146698\tvalid_0's auc: 0.839331\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[77]\tvalid_0's binary_logloss: 0.146508\tvalid_0's auc: 0.839359\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[78]\tvalid_0's binary_logloss: 0.146325\tvalid_0's auc: 0.839397\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[79]\tvalid_0's binary_logloss: 0.146189\tvalid_0's auc: 0.839403\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[80]\tvalid_0's binary_logloss: 0.146017\tvalid_0's auc: 0.839442\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[81]\tvalid_0's binary_logloss: 0.14584\tvalid_0's auc: 0.839516\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[82]\tvalid_0's binary_logloss: 0.14571\tvalid_0's auc: 0.839503\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[83]\tvalid_0's binary_logloss: 0.145532\tvalid_0's auc: 0.839541\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[84]\tvalid_0's binary_logloss: 0.145358\tvalid_0's auc: 0.839605\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[85]\tvalid_0's binary_logloss: 0.145188\tvalid_0's auc: 0.839666\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[86]\tvalid_0's binary_logloss: 0.145073\tvalid_0's auc: 0.839529\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[87]\tvalid_0's binary_logloss: 0.144917\tvalid_0's auc: 0.839566\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[88]\tvalid_0's binary_logloss: 0.144757\tvalid_0's auc: 0.839629\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[89]\tvalid_0's binary_logloss: 0.144597\tvalid_0's auc: 0.839757\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[90]\tvalid_0's binary_logloss: 0.144446\tvalid_0's auc: 0.839762\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[91]\tvalid_0's binary_logloss: 0.144293\tvalid_0's auc: 0.839823\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[92]\tvalid_0's binary_logloss: 0.144176\tvalid_0's auc: 0.839781\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[93]\tvalid_0's binary_logloss: 0.144038\tvalid_0's auc: 0.839791\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[94]\tvalid_0's binary_logloss: 0.143897\tvalid_0's auc: 0.839886\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[95]\tvalid_0's binary_logloss: 0.143796\tvalid_0's auc: 0.839946\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[96]\tvalid_0's binary_logloss: 0.14368\tvalid_0's auc: 0.839988\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[97]\tvalid_0's binary_logloss: 0.143538\tvalid_0's auc: 0.840108\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[98]\tvalid_0's binary_logloss: 0.143405\tvalid_0's auc: 0.840178\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[99]\tvalid_0's binary_logloss: 0.143298\tvalid_0's auc: 0.840225\n","[LightGBM] [Debug] Re-bagging, using 78014 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[100]\tvalid_0's binary_logloss: 0.143161\tvalid_0's auc: 0.840337\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's binary_logloss: 0.143161\tvalid_0's auc: 0.840337\n","[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 4895, number of negative: 106328\n","[LightGBM] [Info] Total Bins 3566\n","[LightGBM] [Info] Number of data: 111223, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.044011 -> initscore=-3.078314\n","[LightGBM] [Info] Start training from score -3.078314\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n","[1]\tvalid_0's binary_logloss: 0.176699\tvalid_0's auc: 0.826005\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[2]\tvalid_0's binary_logloss: 0.175476\tvalid_0's auc: 0.831857\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n","[3]\tvalid_0's binary_logloss: 0.174846\tvalid_0's auc: 0.837287\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n","[4]\tvalid_0's binary_logloss: 0.173691\tvalid_0's auc: 0.838536\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[5]\tvalid_0's binary_logloss: 0.172565\tvalid_0's auc: 0.839182\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[6]\tvalid_0's binary_logloss: 0.171894\tvalid_0's auc: 0.839796\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[7]\tvalid_0's binary_logloss: 0.17121\tvalid_0's auc: 0.83995\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[8]\tvalid_0's binary_logloss: 0.170612\tvalid_0's auc: 0.840435\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n","[9]\tvalid_0's binary_logloss: 0.169603\tvalid_0's auc: 0.841124\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[10]\tvalid_0's binary_logloss: 0.168696\tvalid_0's auc: 0.841781\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[11]\tvalid_0's binary_logloss: 0.168113\tvalid_0's auc: 0.841732\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[12]\tvalid_0's binary_logloss: 0.167268\tvalid_0's auc: 0.842351\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[13]\tvalid_0's binary_logloss: 0.166446\tvalid_0's auc: 0.842649\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[14]\tvalid_0's binary_logloss: 0.165887\tvalid_0's auc: 0.843016\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[15]\tvalid_0's binary_logloss: 0.165135\tvalid_0's auc: 0.843243\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[16]\tvalid_0's binary_logloss: 0.164396\tvalid_0's auc: 0.8433\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[17]\tvalid_0's binary_logloss: 0.163904\tvalid_0's auc: 0.843251\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[18]\tvalid_0's binary_logloss: 0.163433\tvalid_0's auc: 0.843392\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n","[19]\tvalid_0's binary_logloss: 0.162763\tvalid_0's auc: 0.843754\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[20]\tvalid_0's binary_logloss: 0.162139\tvalid_0's auc: 0.843922\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[21]\tvalid_0's binary_logloss: 0.161504\tvalid_0's auc: 0.843987\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[22]\tvalid_0's binary_logloss: 0.16089\tvalid_0's auc: 0.844046\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[23]\tvalid_0's binary_logloss: 0.160481\tvalid_0's auc: 0.844106\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[24]\tvalid_0's binary_logloss: 0.160074\tvalid_0's auc: 0.844226\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[25]\tvalid_0's binary_logloss: 0.159672\tvalid_0's auc: 0.844631\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[26]\tvalid_0's binary_logloss: 0.159125\tvalid_0's auc: 0.844594\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[27]\tvalid_0's binary_logloss: 0.15875\tvalid_0's auc: 0.844914\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[28]\tvalid_0's binary_logloss: 0.158403\tvalid_0's auc: 0.845112\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[29]\tvalid_0's binary_logloss: 0.157908\tvalid_0's auc: 0.845419\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[30]\tvalid_0's binary_logloss: 0.157433\tvalid_0's auc: 0.845353\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[31]\tvalid_0's binary_logloss: 0.156952\tvalid_0's auc: 0.84545\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[32]\tvalid_0's binary_logloss: 0.156486\tvalid_0's auc: 0.845504\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[33]\tvalid_0's binary_logloss: 0.156182\tvalid_0's auc: 0.845475\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[34]\tvalid_0's binary_logloss: 0.155749\tvalid_0's auc: 0.845451\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[35]\tvalid_0's binary_logloss: 0.15544\tvalid_0's auc: 0.845389\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[36]\tvalid_0's binary_logloss: 0.155113\tvalid_0's auc: 0.845568\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[37]\tvalid_0's binary_logloss: 0.154687\tvalid_0's auc: 0.845657\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[38]\tvalid_0's binary_logloss: 0.154279\tvalid_0's auc: 0.845691\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[39]\tvalid_0's binary_logloss: 0.153991\tvalid_0's auc: 0.845575\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[40]\tvalid_0's binary_logloss: 0.153606\tvalid_0's auc: 0.845496\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[41]\tvalid_0's binary_logloss: 0.153234\tvalid_0's auc: 0.845636\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[42]\tvalid_0's binary_logloss: 0.15297\tvalid_0's auc: 0.84566\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[43]\tvalid_0's binary_logloss: 0.152602\tvalid_0's auc: 0.84578\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[44]\tvalid_0's binary_logloss: 0.15225\tvalid_0's auc: 0.845792\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[45]\tvalid_0's binary_logloss: 0.151984\tvalid_0's auc: 0.845777\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[46]\tvalid_0's binary_logloss: 0.151729\tvalid_0's auc: 0.845858\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[47]\tvalid_0's binary_logloss: 0.151389\tvalid_0's auc: 0.845915\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[48]\tvalid_0's binary_logloss: 0.151158\tvalid_0's auc: 0.845862\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[49]\tvalid_0's binary_logloss: 0.150935\tvalid_0's auc: 0.845813\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[50]\tvalid_0's binary_logloss: 0.150628\tvalid_0's auc: 0.845909\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[51]\tvalid_0's binary_logloss: 0.150417\tvalid_0's auc: 0.845978\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[52]\tvalid_0's binary_logloss: 0.150103\tvalid_0's auc: 0.846142\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[53]\tvalid_0's binary_logloss: 0.149873\tvalid_0's auc: 0.846237\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[54]\tvalid_0's binary_logloss: 0.149578\tvalid_0's auc: 0.846291\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[55]\tvalid_0's binary_logloss: 0.149368\tvalid_0's auc: 0.846362\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[56]\tvalid_0's binary_logloss: 0.149089\tvalid_0's auc: 0.846384\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[57]\tvalid_0's binary_logloss: 0.148812\tvalid_0's auc: 0.846405\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[58]\tvalid_0's binary_logloss: 0.148549\tvalid_0's auc: 0.846383\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[59]\tvalid_0's binary_logloss: 0.148282\tvalid_0's auc: 0.846424\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[60]\tvalid_0's binary_logloss: 0.148026\tvalid_0's auc: 0.846422\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[61]\tvalid_0's binary_logloss: 0.147831\tvalid_0's auc: 0.846503\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[62]\tvalid_0's binary_logloss: 0.147652\tvalid_0's auc: 0.846539\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[63]\tvalid_0's binary_logloss: 0.147474\tvalid_0's auc: 0.846576\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[64]\tvalid_0's binary_logloss: 0.147296\tvalid_0's auc: 0.846575\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[65]\tvalid_0's binary_logloss: 0.14706\tvalid_0's auc: 0.846592\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[66]\tvalid_0's binary_logloss: 0.146822\tvalid_0's auc: 0.846592\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[67]\tvalid_0's binary_logloss: 0.146596\tvalid_0's auc: 0.846621\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[68]\tvalid_0's binary_logloss: 0.146374\tvalid_0's auc: 0.846618\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[69]\tvalid_0's binary_logloss: 0.146153\tvalid_0's auc: 0.846687\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[70]\tvalid_0's binary_logloss: 0.145978\tvalid_0's auc: 0.846781\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n","[71]\tvalid_0's binary_logloss: 0.145767\tvalid_0's auc: 0.846804\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[72]\tvalid_0's binary_logloss: 0.145559\tvalid_0's auc: 0.846796\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[73]\tvalid_0's binary_logloss: 0.145406\tvalid_0's auc: 0.846733\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[74]\tvalid_0's binary_logloss: 0.145257\tvalid_0's auc: 0.846823\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[75]\tvalid_0's binary_logloss: 0.145126\tvalid_0's auc: 0.846803\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[76]\tvalid_0's binary_logloss: 0.144926\tvalid_0's auc: 0.846953\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[77]\tvalid_0's binary_logloss: 0.144731\tvalid_0's auc: 0.846956\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[78]\tvalid_0's binary_logloss: 0.14454\tvalid_0's auc: 0.847019\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 6\n","[79]\tvalid_0's binary_logloss: 0.144399\tvalid_0's auc: 0.847032\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[80]\tvalid_0's binary_logloss: 0.144219\tvalid_0's auc: 0.847011\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[81]\tvalid_0's binary_logloss: 0.144036\tvalid_0's auc: 0.847058\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[82]\tvalid_0's binary_logloss: 0.143898\tvalid_0's auc: 0.847132\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[83]\tvalid_0's binary_logloss: 0.143717\tvalid_0's auc: 0.847133\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[84]\tvalid_0's binary_logloss: 0.14355\tvalid_0's auc: 0.847102\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[85]\tvalid_0's binary_logloss: 0.143374\tvalid_0's auc: 0.847116\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[86]\tvalid_0's binary_logloss: 0.143232\tvalid_0's auc: 0.84721\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[87]\tvalid_0's binary_logloss: 0.143071\tvalid_0's auc: 0.847185\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[88]\tvalid_0's binary_logloss: 0.1429\tvalid_0's auc: 0.847222\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[89]\tvalid_0's binary_logloss: 0.142751\tvalid_0's auc: 0.847187\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[90]\tvalid_0's binary_logloss: 0.142595\tvalid_0's auc: 0.847179\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[91]\tvalid_0's binary_logloss: 0.142442\tvalid_0's auc: 0.847287\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[92]\tvalid_0's binary_logloss: 0.142328\tvalid_0's auc: 0.847298\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[93]\tvalid_0's binary_logloss: 0.142182\tvalid_0's auc: 0.847289\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[94]\tvalid_0's binary_logloss: 0.142043\tvalid_0's auc: 0.847282\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[95]\tvalid_0's binary_logloss: 0.141939\tvalid_0's auc: 0.847317\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[96]\tvalid_0's binary_logloss: 0.141827\tvalid_0's auc: 0.847357\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[97]\tvalid_0's binary_logloss: 0.141694\tvalid_0's auc: 0.847424\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[98]\tvalid_0's binary_logloss: 0.14156\tvalid_0's auc: 0.847451\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[99]\tvalid_0's binary_logloss: 0.141442\tvalid_0's auc: 0.847552\n","[LightGBM] [Debug] Re-bagging, using 77850 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[100]\tvalid_0's binary_logloss: 0.141308\tvalid_0's auc: 0.847546\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's binary_logloss: 0.141308\tvalid_0's auc: 0.847546\n","[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 4895, number of negative: 107008\n","[LightGBM] [Info] Total Bins 3568\n","[LightGBM] [Info] Number of data: 111903, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.043743 -> initscore=-3.084689\n","[LightGBM] [Info] Start training from score -3.084689\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[1]\tvalid_0's binary_logloss: 0.180011\tvalid_0's auc: 0.814269\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[2]\tvalid_0's binary_logloss: 0.178724\tvalid_0's auc: 0.817899\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[3]\tvalid_0's binary_logloss: 0.177976\tvalid_0's auc: 0.821328\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[4]\tvalid_0's binary_logloss: 0.176871\tvalid_0's auc: 0.823572\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n","[5]\tvalid_0's binary_logloss: 0.175802\tvalid_0's auc: 0.824357\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[6]\tvalid_0's binary_logloss: 0.175168\tvalid_0's auc: 0.824874\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[7]\tvalid_0's binary_logloss: 0.174458\tvalid_0's auc: 0.826012\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[8]\tvalid_0's binary_logloss: 0.173874\tvalid_0's auc: 0.827324\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[9]\tvalid_0's binary_logloss: 0.172891\tvalid_0's auc: 0.827732\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[10]\tvalid_0's binary_logloss: 0.172024\tvalid_0's auc: 0.828776\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[11]\tvalid_0's binary_logloss: 0.171414\tvalid_0's auc: 0.829\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[12]\tvalid_0's binary_logloss: 0.170595\tvalid_0's auc: 0.82935\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[13]\tvalid_0's binary_logloss: 0.169816\tvalid_0's auc: 0.829198\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[14]\tvalid_0's binary_logloss: 0.169263\tvalid_0's auc: 0.829424\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[15]\tvalid_0's binary_logloss: 0.168548\tvalid_0's auc: 0.829337\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[16]\tvalid_0's binary_logloss: 0.167852\tvalid_0's auc: 0.829233\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[17]\tvalid_0's binary_logloss: 0.16739\tvalid_0's auc: 0.829294\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[18]\tvalid_0's binary_logloss: 0.166928\tvalid_0's auc: 0.829203\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[19]\tvalid_0's binary_logloss: 0.16631\tvalid_0's auc: 0.829543\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[20]\tvalid_0's binary_logloss: 0.165694\tvalid_0's auc: 0.830047\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[21]\tvalid_0's binary_logloss: 0.165087\tvalid_0's auc: 0.829968\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[22]\tvalid_0's binary_logloss: 0.164515\tvalid_0's auc: 0.829959\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[23]\tvalid_0's binary_logloss: 0.164118\tvalid_0's auc: 0.830127\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[24]\tvalid_0's binary_logloss: 0.163749\tvalid_0's auc: 0.83023\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[25]\tvalid_0's binary_logloss: 0.163381\tvalid_0's auc: 0.830059\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[26]\tvalid_0's binary_logloss: 0.162869\tvalid_0's auc: 0.830095\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[27]\tvalid_0's binary_logloss: 0.162517\tvalid_0's auc: 0.829981\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[28]\tvalid_0's binary_logloss: 0.162176\tvalid_0's auc: 0.830222\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[29]\tvalid_0's binary_logloss: 0.161684\tvalid_0's auc: 0.830595\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[30]\tvalid_0's binary_logloss: 0.161213\tvalid_0's auc: 0.830783\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[31]\tvalid_0's binary_logloss: 0.160759\tvalid_0's auc: 0.830845\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[32]\tvalid_0's binary_logloss: 0.160302\tvalid_0's auc: 0.831172\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[33]\tvalid_0's binary_logloss: 0.16\tvalid_0's auc: 0.831267\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[34]\tvalid_0's binary_logloss: 0.159578\tvalid_0's auc: 0.831235\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[35]\tvalid_0's binary_logloss: 0.159278\tvalid_0's auc: 0.83114\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[36]\tvalid_0's binary_logloss: 0.15899\tvalid_0's auc: 0.831099\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[37]\tvalid_0's binary_logloss: 0.158584\tvalid_0's auc: 0.831218\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[38]\tvalid_0's binary_logloss: 0.15818\tvalid_0's auc: 0.831368\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 6\n","[39]\tvalid_0's binary_logloss: 0.157909\tvalid_0's auc: 0.831296\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[40]\tvalid_0's binary_logloss: 0.157537\tvalid_0's auc: 0.831341\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[41]\tvalid_0's binary_logloss: 0.157181\tvalid_0's auc: 0.831611\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[42]\tvalid_0's binary_logloss: 0.156917\tvalid_0's auc: 0.8315\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[43]\tvalid_0's binary_logloss: 0.156573\tvalid_0's auc: 0.831465\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[44]\tvalid_0's binary_logloss: 0.156241\tvalid_0's auc: 0.831414\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[45]\tvalid_0's binary_logloss: 0.155998\tvalid_0's auc: 0.831315\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[46]\tvalid_0's binary_logloss: 0.155758\tvalid_0's auc: 0.831341\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[47]\tvalid_0's binary_logloss: 0.155447\tvalid_0's auc: 0.831334\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[48]\tvalid_0's binary_logloss: 0.155219\tvalid_0's auc: 0.83136\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[49]\tvalid_0's binary_logloss: 0.154999\tvalid_0's auc: 0.831426\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[50]\tvalid_0's binary_logloss: 0.154705\tvalid_0's auc: 0.831541\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[51]\tvalid_0's binary_logloss: 0.154518\tvalid_0's auc: 0.831753\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[52]\tvalid_0's binary_logloss: 0.154225\tvalid_0's auc: 0.831844\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[53]\tvalid_0's binary_logloss: 0.153999\tvalid_0's auc: 0.831775\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[54]\tvalid_0's binary_logloss: 0.15372\tvalid_0's auc: 0.831804\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[55]\tvalid_0's binary_logloss: 0.153525\tvalid_0's auc: 0.83181\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[56]\tvalid_0's binary_logloss: 0.153246\tvalid_0's auc: 0.831838\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[57]\tvalid_0's binary_logloss: 0.152975\tvalid_0's auc: 0.831908\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[58]\tvalid_0's binary_logloss: 0.152717\tvalid_0's auc: 0.831975\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[59]\tvalid_0's binary_logloss: 0.15246\tvalid_0's auc: 0.832058\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[60]\tvalid_0's binary_logloss: 0.152218\tvalid_0's auc: 0.832049\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[61]\tvalid_0's binary_logloss: 0.152023\tvalid_0's auc: 0.832083\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[62]\tvalid_0's binary_logloss: 0.151858\tvalid_0's auc: 0.832217\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[63]\tvalid_0's binary_logloss: 0.151686\tvalid_0's auc: 0.832226\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[64]\tvalid_0's binary_logloss: 0.151525\tvalid_0's auc: 0.832167\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[65]\tvalid_0's binary_logloss: 0.151302\tvalid_0's auc: 0.832144\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[66]\tvalid_0's binary_logloss: 0.151065\tvalid_0's auc: 0.832129\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[67]\tvalid_0's binary_logloss: 0.150846\tvalid_0's auc: 0.83215\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[68]\tvalid_0's binary_logloss: 0.15063\tvalid_0's auc: 0.832117\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[69]\tvalid_0's binary_logloss: 0.150424\tvalid_0's auc: 0.832159\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[70]\tvalid_0's binary_logloss: 0.150273\tvalid_0's auc: 0.832158\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[71]\tvalid_0's binary_logloss: 0.150072\tvalid_0's auc: 0.83226\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[72]\tvalid_0's binary_logloss: 0.149863\tvalid_0's auc: 0.832357\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[73]\tvalid_0's binary_logloss: 0.149712\tvalid_0's auc: 0.832432\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[74]\tvalid_0's binary_logloss: 0.149568\tvalid_0's auc: 0.832462\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[75]\tvalid_0's binary_logloss: 0.149444\tvalid_0's auc: 0.832516\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[76]\tvalid_0's binary_logloss: 0.149256\tvalid_0's auc: 0.832648\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[77]\tvalid_0's binary_logloss: 0.149081\tvalid_0's auc: 0.83256\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[78]\tvalid_0's binary_logloss: 0.148901\tvalid_0's auc: 0.832576\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[79]\tvalid_0's binary_logloss: 0.148774\tvalid_0's auc: 0.832494\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[80]\tvalid_0's binary_logloss: 0.148606\tvalid_0's auc: 0.832491\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[81]\tvalid_0's binary_logloss: 0.148436\tvalid_0's auc: 0.83247\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[82]\tvalid_0's binary_logloss: 0.148302\tvalid_0's auc: 0.832511\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[83]\tvalid_0's binary_logloss: 0.148134\tvalid_0's auc: 0.832454\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[84]\tvalid_0's binary_logloss: 0.147958\tvalid_0's auc: 0.83252\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[85]\tvalid_0's binary_logloss: 0.147791\tvalid_0's auc: 0.832615\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[86]\tvalid_0's binary_logloss: 0.147667\tvalid_0's auc: 0.83261\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[87]\tvalid_0's binary_logloss: 0.14751\tvalid_0's auc: 0.832658\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[88]\tvalid_0's binary_logloss: 0.14735\tvalid_0's auc: 0.832711\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[89]\tvalid_0's binary_logloss: 0.147192\tvalid_0's auc: 0.832705\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[90]\tvalid_0's binary_logloss: 0.147038\tvalid_0's auc: 0.832761\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[91]\tvalid_0's binary_logloss: 0.146898\tvalid_0's auc: 0.832845\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[92]\tvalid_0's binary_logloss: 0.146785\tvalid_0's auc: 0.832894\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[93]\tvalid_0's binary_logloss: 0.146647\tvalid_0's auc: 0.832904\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[94]\tvalid_0's binary_logloss: 0.146514\tvalid_0's auc: 0.832899\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[95]\tvalid_0's binary_logloss: 0.146412\tvalid_0's auc: 0.832926\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[96]\tvalid_0's binary_logloss: 0.146308\tvalid_0's auc: 0.833011\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[97]\tvalid_0's binary_logloss: 0.146177\tvalid_0's auc: 0.833074\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[98]\tvalid_0's binary_logloss: 0.146057\tvalid_0's auc: 0.83309\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[99]\tvalid_0's binary_logloss: 0.145953\tvalid_0's auc: 0.833174\n","[LightGBM] [Debug] Re-bagging, using 78320 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[100]\tvalid_0's binary_logloss: 0.14583\tvalid_0's auc: 0.833261\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's binary_logloss: 0.14583\tvalid_0's auc: 0.833261\n","[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 4895, number of negative: 106764\n","[LightGBM] [Info] Total Bins 3567\n","[LightGBM] [Info] Number of data: 111659, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.043839 -> initscore=-3.082407\n","[LightGBM] [Info] Start training from score -3.082407\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[1]\tvalid_0's binary_logloss: 0.178754\tvalid_0's auc: 0.818947\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[2]\tvalid_0's binary_logloss: 0.177457\tvalid_0's auc: 0.822991\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[3]\tvalid_0's binary_logloss: 0.176694\tvalid_0's auc: 0.825062\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[4]\tvalid_0's binary_logloss: 0.175573\tvalid_0's auc: 0.825879\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n","[5]\tvalid_0's binary_logloss: 0.174491\tvalid_0's auc: 0.827566\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[6]\tvalid_0's binary_logloss: 0.173838\tvalid_0's auc: 0.826991\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[7]\tvalid_0's binary_logloss: 0.173191\tvalid_0's auc: 0.827142\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[8]\tvalid_0's binary_logloss: 0.172658\tvalid_0's auc: 0.826266\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n","[9]\tvalid_0's binary_logloss: 0.171641\tvalid_0's auc: 0.827282\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[10]\tvalid_0's binary_logloss: 0.170735\tvalid_0's auc: 0.828438\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[11]\tvalid_0's binary_logloss: 0.170138\tvalid_0's auc: 0.828021\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[12]\tvalid_0's binary_logloss: 0.169308\tvalid_0's auc: 0.828237\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[13]\tvalid_0's binary_logloss: 0.168479\tvalid_0's auc: 0.829256\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[14]\tvalid_0's binary_logloss: 0.167934\tvalid_0's auc: 0.829128\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[15]\tvalid_0's binary_logloss: 0.16719\tvalid_0's auc: 0.829607\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n","[16]\tvalid_0's binary_logloss: 0.166459\tvalid_0's auc: 0.829583\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[17]\tvalid_0's binary_logloss: 0.166004\tvalid_0's auc: 0.829357\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[18]\tvalid_0's binary_logloss: 0.165529\tvalid_0's auc: 0.829195\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[19]\tvalid_0's binary_logloss: 0.164876\tvalid_0's auc: 0.829304\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[20]\tvalid_0's binary_logloss: 0.164233\tvalid_0's auc: 0.829739\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[21]\tvalid_0's binary_logloss: 0.163603\tvalid_0's auc: 0.830106\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[22]\tvalid_0's binary_logloss: 0.163006\tvalid_0's auc: 0.830559\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[23]\tvalid_0's binary_logloss: 0.1626\tvalid_0's auc: 0.830237\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[24]\tvalid_0's binary_logloss: 0.162211\tvalid_0's auc: 0.830201\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[25]\tvalid_0's binary_logloss: 0.161841\tvalid_0's auc: 0.83005\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[26]\tvalid_0's binary_logloss: 0.161296\tvalid_0's auc: 0.83052\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[27]\tvalid_0's binary_logloss: 0.160917\tvalid_0's auc: 0.830678\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[28]\tvalid_0's binary_logloss: 0.160582\tvalid_0's auc: 0.830499\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[29]\tvalid_0's binary_logloss: 0.160084\tvalid_0's auc: 0.83079\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[30]\tvalid_0's binary_logloss: 0.159604\tvalid_0's auc: 0.830946\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[31]\tvalid_0's binary_logloss: 0.159108\tvalid_0's auc: 0.831307\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[32]\tvalid_0's binary_logloss: 0.158656\tvalid_0's auc: 0.831462\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[33]\tvalid_0's binary_logloss: 0.158369\tvalid_0's auc: 0.831232\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[34]\tvalid_0's binary_logloss: 0.157931\tvalid_0's auc: 0.831381\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[35]\tvalid_0's binary_logloss: 0.157607\tvalid_0's auc: 0.83157\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[36]\tvalid_0's binary_logloss: 0.157289\tvalid_0's auc: 0.831458\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[37]\tvalid_0's binary_logloss: 0.15686\tvalid_0's auc: 0.831568\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[38]\tvalid_0's binary_logloss: 0.156457\tvalid_0's auc: 0.831756\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 6\n","[39]\tvalid_0's binary_logloss: 0.156178\tvalid_0's auc: 0.831733\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[40]\tvalid_0's binary_logloss: 0.155794\tvalid_0's auc: 0.831831\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[41]\tvalid_0's binary_logloss: 0.155432\tvalid_0's auc: 0.832059\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[42]\tvalid_0's binary_logloss: 0.155178\tvalid_0's auc: 0.831952\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[43]\tvalid_0's binary_logloss: 0.154817\tvalid_0's auc: 0.832021\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[44]\tvalid_0's binary_logloss: 0.154472\tvalid_0's auc: 0.832135\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[45]\tvalid_0's binary_logloss: 0.154223\tvalid_0's auc: 0.832149\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[46]\tvalid_0's binary_logloss: 0.153989\tvalid_0's auc: 0.832089\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[47]\tvalid_0's binary_logloss: 0.153652\tvalid_0's auc: 0.832273\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[48]\tvalid_0's binary_logloss: 0.153424\tvalid_0's auc: 0.832246\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[49]\tvalid_0's binary_logloss: 0.153209\tvalid_0's auc: 0.832253\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[50]\tvalid_0's binary_logloss: 0.152894\tvalid_0's auc: 0.832475\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[51]\tvalid_0's binary_logloss: 0.152712\tvalid_0's auc: 0.832361\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[52]\tvalid_0's binary_logloss: 0.152411\tvalid_0's auc: 0.8325\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[53]\tvalid_0's binary_logloss: 0.152196\tvalid_0's auc: 0.832477\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[54]\tvalid_0's binary_logloss: 0.151901\tvalid_0's auc: 0.832549\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[55]\tvalid_0's binary_logloss: 0.151712\tvalid_0's auc: 0.832491\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[56]\tvalid_0's binary_logloss: 0.151424\tvalid_0's auc: 0.832637\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[57]\tvalid_0's binary_logloss: 0.151146\tvalid_0's auc: 0.832691\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[58]\tvalid_0's binary_logloss: 0.15088\tvalid_0's auc: 0.832734\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[59]\tvalid_0's binary_logloss: 0.150622\tvalid_0's auc: 0.832832\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[60]\tvalid_0's binary_logloss: 0.150358\tvalid_0's auc: 0.832997\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[61]\tvalid_0's binary_logloss: 0.150161\tvalid_0's auc: 0.833003\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n","[62]\tvalid_0's binary_logloss: 0.150016\tvalid_0's auc: 0.832902\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[63]\tvalid_0's binary_logloss: 0.149856\tvalid_0's auc: 0.832865\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[64]\tvalid_0's binary_logloss: 0.149678\tvalid_0's auc: 0.832848\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[65]\tvalid_0's binary_logloss: 0.149444\tvalid_0's auc: 0.832917\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[66]\tvalid_0's binary_logloss: 0.149198\tvalid_0's auc: 0.833035\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[67]\tvalid_0's binary_logloss: 0.148964\tvalid_0's auc: 0.833109\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[68]\tvalid_0's binary_logloss: 0.148737\tvalid_0's auc: 0.833211\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[69]\tvalid_0's binary_logloss: 0.148515\tvalid_0's auc: 0.833329\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[70]\tvalid_0's binary_logloss: 0.148355\tvalid_0's auc: 0.833334\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[71]\tvalid_0's binary_logloss: 0.14815\tvalid_0's auc: 0.833367\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[72]\tvalid_0's binary_logloss: 0.147938\tvalid_0's auc: 0.833426\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[73]\tvalid_0's binary_logloss: 0.147797\tvalid_0's auc: 0.833339\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[74]\tvalid_0's binary_logloss: 0.147654\tvalid_0's auc: 0.833237\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[75]\tvalid_0's binary_logloss: 0.147528\tvalid_0's auc: 0.833191\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[76]\tvalid_0's binary_logloss: 0.147339\tvalid_0's auc: 0.833309\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[77]\tvalid_0's binary_logloss: 0.147148\tvalid_0's auc: 0.833366\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[78]\tvalid_0's binary_logloss: 0.146968\tvalid_0's auc: 0.833446\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[79]\tvalid_0's binary_logloss: 0.14684\tvalid_0's auc: 0.833426\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[80]\tvalid_0's binary_logloss: 0.14666\tvalid_0's auc: 0.833424\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[81]\tvalid_0's binary_logloss: 0.146487\tvalid_0's auc: 0.833545\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[82]\tvalid_0's binary_logloss: 0.146365\tvalid_0's auc: 0.833483\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[83]\tvalid_0's binary_logloss: 0.146182\tvalid_0's auc: 0.833619\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[84]\tvalid_0's binary_logloss: 0.14601\tvalid_0's auc: 0.833643\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[85]\tvalid_0's binary_logloss: 0.145846\tvalid_0's auc: 0.833663\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[86]\tvalid_0's binary_logloss: 0.145719\tvalid_0's auc: 0.833721\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[87]\tvalid_0's binary_logloss: 0.145559\tvalid_0's auc: 0.833862\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[88]\tvalid_0's binary_logloss: 0.145388\tvalid_0's auc: 0.833966\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[89]\tvalid_0's binary_logloss: 0.14523\tvalid_0's auc: 0.834049\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[90]\tvalid_0's binary_logloss: 0.145077\tvalid_0's auc: 0.834062\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[91]\tvalid_0's binary_logloss: 0.14493\tvalid_0's auc: 0.834168\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[92]\tvalid_0's binary_logloss: 0.144823\tvalid_0's auc: 0.83413\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[93]\tvalid_0's binary_logloss: 0.144676\tvalid_0's auc: 0.83413\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[94]\tvalid_0's binary_logloss: 0.144537\tvalid_0's auc: 0.834149\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[95]\tvalid_0's binary_logloss: 0.144432\tvalid_0's auc: 0.834199\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[96]\tvalid_0's binary_logloss: 0.144323\tvalid_0's auc: 0.834185\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[97]\tvalid_0's binary_logloss: 0.144178\tvalid_0's auc: 0.834326\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[98]\tvalid_0's binary_logloss: 0.144047\tvalid_0's auc: 0.834407\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[99]\tvalid_0's binary_logloss: 0.143947\tvalid_0's auc: 0.834409\n","[LightGBM] [Debug] Re-bagging, using 78156 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[100]\tvalid_0's binary_logloss: 0.143816\tvalid_0's auc: 0.834431\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's binary_logloss: 0.143816\tvalid_0's auc: 0.834431\n","[LightGBM] [Warning] Starting from the 2.1.2 version, default value for the \"boost_from_average\" parameter in \"binary\" objective is true.\n","This may cause significantly different results comparing to the previous versions of LightGBM.\n","Try to set boost_from_average=false, if your old models produce bad results\n","[LightGBM] [Info] Number of positive: 4896, number of negative: 106998\n","[LightGBM] [Info] Total Bins 3566\n","[LightGBM] [Info] Number of data: 111894, number of used features: 14\n","[LightGBM] [Warning] Cannot change bin_construct_sample_cnt after constructed Dataset handle.\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.043756 -> initscore=-3.084392\n","[LightGBM] [Info] Start training from score -3.084392\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n","[1]\tvalid_0's binary_logloss: 0.179993\tvalid_0's auc: 0.79976\n","Training until validation scores don't improve for 50 rounds.\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n","[2]\tvalid_0's binary_logloss: 0.178734\tvalid_0's auc: 0.810493\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[3]\tvalid_0's binary_logloss: 0.177986\tvalid_0's auc: 0.810921\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[4]\tvalid_0's binary_logloss: 0.176884\tvalid_0's auc: 0.813378\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[5]\tvalid_0's binary_logloss: 0.175798\tvalid_0's auc: 0.815403\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[6]\tvalid_0's binary_logloss: 0.175186\tvalid_0's auc: 0.815534\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[7]\tvalid_0's binary_logloss: 0.174495\tvalid_0's auc: 0.816563\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[8]\tvalid_0's binary_logloss: 0.173916\tvalid_0's auc: 0.816331\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[9]\tvalid_0's binary_logloss: 0.172936\tvalid_0's auc: 0.817858\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[10]\tvalid_0's binary_logloss: 0.172061\tvalid_0's auc: 0.819321\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[11]\tvalid_0's binary_logloss: 0.171463\tvalid_0's auc: 0.818608\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[12]\tvalid_0's binary_logloss: 0.170655\tvalid_0's auc: 0.819409\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[13]\tvalid_0's binary_logloss: 0.169855\tvalid_0's auc: 0.819651\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[14]\tvalid_0's binary_logloss: 0.169335\tvalid_0's auc: 0.819384\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[15]\tvalid_0's binary_logloss: 0.168617\tvalid_0's auc: 0.819817\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n","[16]\tvalid_0's binary_logloss: 0.167895\tvalid_0's auc: 0.820075\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[17]\tvalid_0's binary_logloss: 0.167451\tvalid_0's auc: 0.820401\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[18]\tvalid_0's binary_logloss: 0.166993\tvalid_0's auc: 0.820344\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[19]\tvalid_0's binary_logloss: 0.166353\tvalid_0's auc: 0.820967\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 11\n","[20]\tvalid_0's binary_logloss: 0.165735\tvalid_0's auc: 0.821169\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[21]\tvalid_0's binary_logloss: 0.165114\tvalid_0's auc: 0.821479\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[22]\tvalid_0's binary_logloss: 0.164536\tvalid_0's auc: 0.821766\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[23]\tvalid_0's binary_logloss: 0.164162\tvalid_0's auc: 0.821418\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[24]\tvalid_0's binary_logloss: 0.163788\tvalid_0's auc: 0.821557\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[25]\tvalid_0's binary_logloss: 0.163414\tvalid_0's auc: 0.821263\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[26]\tvalid_0's binary_logloss: 0.1629\tvalid_0's auc: 0.821371\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[27]\tvalid_0's binary_logloss: 0.162544\tvalid_0's auc: 0.821338\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[28]\tvalid_0's binary_logloss: 0.162218\tvalid_0's auc: 0.820957\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[29]\tvalid_0's binary_logloss: 0.161729\tvalid_0's auc: 0.821495\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[30]\tvalid_0's binary_logloss: 0.161256\tvalid_0's auc: 0.822016\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[31]\tvalid_0's binary_logloss: 0.160795\tvalid_0's auc: 0.822247\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[32]\tvalid_0's binary_logloss: 0.16035\tvalid_0's auc: 0.822308\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 6\n","[33]\tvalid_0's binary_logloss: 0.160066\tvalid_0's auc: 0.822043\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[34]\tvalid_0's binary_logloss: 0.159647\tvalid_0's auc: 0.822279\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[35]\tvalid_0's binary_logloss: 0.159356\tvalid_0's auc: 0.821975\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[36]\tvalid_0's binary_logloss: 0.159078\tvalid_0's auc: 0.821854\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[37]\tvalid_0's binary_logloss: 0.158665\tvalid_0's auc: 0.822\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 10\n","[38]\tvalid_0's binary_logloss: 0.158259\tvalid_0's auc: 0.822231\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 6\n","[39]\tvalid_0's binary_logloss: 0.15798\tvalid_0's auc: 0.822286\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[40]\tvalid_0's binary_logloss: 0.157621\tvalid_0's auc: 0.822327\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[41]\tvalid_0's binary_logloss: 0.157268\tvalid_0's auc: 0.822648\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[42]\tvalid_0's binary_logloss: 0.157025\tvalid_0's auc: 0.82258\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[43]\tvalid_0's binary_logloss: 0.156671\tvalid_0's auc: 0.822642\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[44]\tvalid_0's binary_logloss: 0.156336\tvalid_0's auc: 0.82274\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[45]\tvalid_0's binary_logloss: 0.156095\tvalid_0's auc: 0.822656\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[46]\tvalid_0's binary_logloss: 0.15586\tvalid_0's auc: 0.82267\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[47]\tvalid_0's binary_logloss: 0.155547\tvalid_0's auc: 0.822784\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[48]\tvalid_0's binary_logloss: 0.15532\tvalid_0's auc: 0.822729\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[49]\tvalid_0's binary_logloss: 0.15511\tvalid_0's auc: 0.822702\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[50]\tvalid_0's binary_logloss: 0.154806\tvalid_0's auc: 0.82293\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[51]\tvalid_0's binary_logloss: 0.154627\tvalid_0's auc: 0.822911\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[52]\tvalid_0's binary_logloss: 0.154333\tvalid_0's auc: 0.822962\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[53]\tvalid_0's binary_logloss: 0.154128\tvalid_0's auc: 0.822876\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[54]\tvalid_0's binary_logloss: 0.153854\tvalid_0's auc: 0.822887\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[55]\tvalid_0's binary_logloss: 0.153661\tvalid_0's auc: 0.823008\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[56]\tvalid_0's binary_logloss: 0.153393\tvalid_0's auc: 0.82314\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[57]\tvalid_0's binary_logloss: 0.153135\tvalid_0's auc: 0.823189\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[58]\tvalid_0's binary_logloss: 0.152882\tvalid_0's auc: 0.823339\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[59]\tvalid_0's binary_logloss: 0.152629\tvalid_0's auc: 0.823502\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[60]\tvalid_0's binary_logloss: 0.152391\tvalid_0's auc: 0.823585\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[61]\tvalid_0's binary_logloss: 0.152226\tvalid_0's auc: 0.823369\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[62]\tvalid_0's binary_logloss: 0.152055\tvalid_0's auc: 0.823502\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[63]\tvalid_0's binary_logloss: 0.151894\tvalid_0's auc: 0.8234\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[64]\tvalid_0's binary_logloss: 0.151732\tvalid_0's auc: 0.823513\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[65]\tvalid_0's binary_logloss: 0.151505\tvalid_0's auc: 0.823622\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[66]\tvalid_0's binary_logloss: 0.151286\tvalid_0's auc: 0.823712\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[67]\tvalid_0's binary_logloss: 0.15107\tvalid_0's auc: 0.823775\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[68]\tvalid_0's binary_logloss: 0.150855\tvalid_0's auc: 0.823804\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[69]\tvalid_0's binary_logloss: 0.150661\tvalid_0's auc: 0.82389\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[70]\tvalid_0's binary_logloss: 0.150511\tvalid_0's auc: 0.823752\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[71]\tvalid_0's binary_logloss: 0.150303\tvalid_0's auc: 0.82386\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[72]\tvalid_0's binary_logloss: 0.1501\tvalid_0's auc: 0.823917\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[73]\tvalid_0's binary_logloss: 0.149951\tvalid_0's auc: 0.824035\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[74]\tvalid_0's binary_logloss: 0.149816\tvalid_0's auc: 0.8241\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[75]\tvalid_0's binary_logloss: 0.149692\tvalid_0's auc: 0.824194\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[76]\tvalid_0's binary_logloss: 0.149509\tvalid_0's auc: 0.824316\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[77]\tvalid_0's binary_logloss: 0.14933\tvalid_0's auc: 0.824383\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[78]\tvalid_0's binary_logloss: 0.149163\tvalid_0's auc: 0.824435\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[79]\tvalid_0's binary_logloss: 0.149038\tvalid_0's auc: 0.82446\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[80]\tvalid_0's binary_logloss: 0.148855\tvalid_0's auc: 0.824592\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[81]\tvalid_0's binary_logloss: 0.148689\tvalid_0's auc: 0.824636\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[82]\tvalid_0's binary_logloss: 0.148563\tvalid_0's auc: 0.824563\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[83]\tvalid_0's binary_logloss: 0.148396\tvalid_0's auc: 0.824527\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[84]\tvalid_0's binary_logloss: 0.148231\tvalid_0's auc: 0.824562\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[85]\tvalid_0's binary_logloss: 0.148078\tvalid_0's auc: 0.824642\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[86]\tvalid_0's binary_logloss: 0.147959\tvalid_0's auc: 0.824602\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[87]\tvalid_0's binary_logloss: 0.147802\tvalid_0's auc: 0.824695\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[88]\tvalid_0's binary_logloss: 0.147636\tvalid_0's auc: 0.824806\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[89]\tvalid_0's binary_logloss: 0.147495\tvalid_0's auc: 0.824856\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[90]\tvalid_0's binary_logloss: 0.147354\tvalid_0's auc: 0.824881\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[91]\tvalid_0's binary_logloss: 0.147211\tvalid_0's auc: 0.825049\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[92]\tvalid_0's binary_logloss: 0.147098\tvalid_0's auc: 0.824995\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[93]\tvalid_0's binary_logloss: 0.146966\tvalid_0's auc: 0.825024\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[94]\tvalid_0's binary_logloss: 0.146833\tvalid_0's auc: 0.825053\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 7\n","[95]\tvalid_0's binary_logloss: 0.14674\tvalid_0's auc: 0.8251\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[96]\tvalid_0's binary_logloss: 0.146636\tvalid_0's auc: 0.825153\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 6\n","[97]\tvalid_0's binary_logloss: 0.146509\tvalid_0's auc: 0.825171\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[98]\tvalid_0's binary_logloss: 0.146377\tvalid_0's auc: 0.825261\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 8\n","[99]\tvalid_0's binary_logloss: 0.146275\tvalid_0's auc: 0.825223\n","[LightGBM] [Debug] Re-bagging, using 78313 data to train\n","[LightGBM] [Debug] Trained a tree with leaves = 31 and max_depth = 9\n","[100]\tvalid_0's binary_logloss: 0.146143\tvalid_0's auc: 0.8253\n","Did not meet early stopping. Best iteration is:\n","[100]\tvalid_0's binary_logloss: 0.146143\tvalid_0's auc: 0.8253\n"]}]}]}